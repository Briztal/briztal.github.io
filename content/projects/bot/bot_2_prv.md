---
title: "Trading bot : data provider."
summary: "Explaining the design of the data provider."
series: ["Trading bot"]
series_order: 2
categories: ["Trading bot"]
#externalUrl: ""
showSummary: true
date: 2025-06-19
showTableOfContents : true
draft: true
---

This chapter will describe the design choices made when implementing the data provision system and justify them by covering in a simplified, but functional way, how disk access, memory translation, kernel buffer management and memory mapping work.

## Participants

The data provider system will feature the following participants : 
- multiple remote providers, each providing data for a specific set of instruments in a specific time range. It is assumed that those remote providers provide the exact same information and that when multiple providers can provide the same data elements, any can be choosed.
- multiple data consumers, which want to use data provided by remote providers. Consumers can be strategies, or simulation brokers.

## Consumer needs

The data consumers need to do the two following types of request : 
- range request : takes a start minute index (stt) and an end minute index (end) and for the time interval [stt, end[, generate : 
  - a volumes array with one volume per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0.
  - a values array with one value per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0.
- value request : takes `tgt` a target minute index (tgt), and determines `mid` the greatest minute index inferior or equal to `tgt` with a non-zero volume, then returns : 
  - `mid`.
  - `vol` the volume at `mid`.  
  - `val` the value at `mid`.  

In practice, the range request is used when we want to cache data, or compute statistics like moving averages, or correlations, and the value request is used to find the most recent volume and price at a given moment in time.

## Remote provider format

Remote providers like polygonio provide rest-ful APIs to fetch data. Those APIs will likely transfer data in plain text formats like JSon, that the local provider will have to parse and convert into floats for the rest of the system to use them.

Rest APIs are thread-safe by design, so there will be no need to coordinate the possibly multiple active trading cores to fetch data.

## Constraints

First, as stated in the previous chapters, it is essential to avoid downloading anything from remote providers if it is not strictly necessary. That gives us our first two constraints :
- data downloaded from the provider must be stored to disk and used in place of the remote provider when possible.
- local providers running on the same machine must collaboratively use the same storage.

Then, as stated in the 'Consumer neers' section, data is ultimately to be either read by value or stored in contiguous memory arrays. This gives us our third constraint : 
- disk storage must be efficient for both value and range read.

In practice, this means that our disk storage will store data in contiguous arrays which will follow the same padding rules as the arrays generated by range requests : if at a given minute ID there has been no transactions, then both volume and value array alements will be 0.

My proposal for the design answering those three constraints uses the file system and its memory mapping capabilities as storage medium. To understand why, let's cover how modern operating systems manage disk storage and file memory mapping in a simplified way.

## Architectural and kernel concepts.

The content of this section originally covered the following things : 
- how modern OSes manage storage devices like SATA disks.
- how the MMU works and how VA -> PA translation is done.
- what happens when you call mmap.

After writing it, I decided to move it in a dedicated article that can be found here, as it may be interesting beyond the scope of the trading bot.

Please take the time to read it to get familiar with the concepts that the current design choice of the provider is based on.

## File storage design

### File structure, values, volumes.

Data downloaded from the remote provider is stored in the file system, and mmap is used to efficiently access it.

Since mmap prevents us to resize a file once mapped in memory : 
- we need to store data in fixed-sized files.
- we need to extend the files to their largest possible size at creation before the first mmap is done.

My decision was to use a dedicated file per yer per instrument. One could use a different time base but one ultimately has to pick one.

Each file contains data starting at Jan 1st 00:00:00 for the file's year, and contains all zero-padded data until the end of the year, one data element per minute :
- if at a given minute no transaction happened, the data element is : 
  - volume = 0;
  - value = 0;
- if at a given minute at least one transaction happened, the data element is : 
  - volume = sum of all transactions volumes;
  - value = average of all transaction values weighted by their volumes;

Data is written in time order, starting at Jan 1st 00:00:00. An element at mid A cannot be written before all elements with mids B < A have been written.
If an instrument has no data in a time range, volumes and values for this range are set to 0. 

Each file is thus be divided into the following sections, which are be independenly mmap-ed by every provider needing to access the data that it contains : 
- descriptor : identifies the file using the instrument name and the year.
- sync : data used by multiple provides using this file to synchronize their writes.
- volumes : the array of all volumes of transactions for each minute of this year. Expressed with f64s.
- values : the array of all values of transactions for each minute of this year. Expressed with f64s.
- prv_ids : another array containing an array index for each minute of this year. Expressed with u32s. Has a special purpose which is not needed to understand the structure of the provider. Described in a dedicated section at the end of the article if the reader is curious. 

### Mapping attributes.

As stated above, each file section is mmap-ed by every provider who needs to use its data. Let's recap how it is mapped and accessed.
- descriptor : only written by the provider that creates the file. Mappable as read-only by every other provider. Could be mapped as private once we're sure that the provider that created the file has initialized the descriptor and called msync.
- sync : is actively read and written using atomics by the different providers who use the file's data. Needs to be mapped as shareable read-write by everyone. 
- volumes, values, prv_ids : is written in order, which causes those sections to be incrementally written. Only one page at a time is written and all others could be read-only. In practice to avoid the cost of TLB updates, all pages are writeable. All pages >= the currently written pages must be shared to observe writes from other providers. All pages < the currently written page could be remapped as private but this brings no benefit in practice since no one is supposed to write to those (or it is a bug). In practice, all those pages are mapped as shareable read-write. 

### Sync 

Since providers may use the same data at the same time, they may also need the same non-downloaded data at the same time.

The shared section contains data that providers may need to read and write to coordinate themselves in accessing the file's data. In practice it contains : 
- u8 `lck` : a lock which protectes the access to the two fields below.
- u8 `wrt` : a flag reporting that someone is actively downloading writing data.
- u64 `siz` : the number of elements that have been downloaded and written and can be feely read by anyone.

One could say that since we assumed that all remote providers provide identical data, all local providers could then _just_ all download data independently and write them to the file sections that they all mapped as shared.

This would work in practice but : 
- is dirty (personal opinion).
- would lead to unnecessary cache coherency latency for writes to the same locations. Those writes would ultimately write the same data, but the cache coherency system would probably not be aware of that and the perf hit could be noticeable.  
- we would still need to synchronize the `siz` field of the sync data to ensure that it is monotonically increasing. Otherwise, a provider A could write a lot of data and increase it and another provider B, who started to write at the same time but downloded a smaller segment (but in a slower manner) could complete after, and update `siz` to a lesser value. This would force other providers to re-download and re-write data already written by A.  

Hence, we need the sync section.

Now, let's desribe a write sequence example that will show how multiple provides collaborate to download the same data and how they all end up seeing what other write :
- Storage file only contains data for [Jan 1st 00:00:00, Jan 3rd 00:00:00[ (<= note the open bracket, we have no data for the end time).
- Providers A and B want to read data for [Jan 10th 00:00:00, Jan 20th 00:00:00[
- They both attempt to lock `lck` using atomics. The cache coherency system will ensure that exactly one acquires the lock.
- Provider A succeeds. Provider B is blocked until A releases the lock. 
- Provider A reads `siz` and sees that the data that it wants is not here. It will either have to download it or wait for another provider to do so.
- Provider A sees that `wrt` is clear : no one is actively downloading data so provider A must do it itself.
- Provider A sets `wrt` and releases `lck`.
- Provider B is unblocked, acquires `lck`.
- Provider B reads `siz` and sees that the data that it wants is not here. It will either have to download it or wait for another provider to do so.
- Provider B sees that `wrt` is set : someone is actively downloading data so provider B must wait for it to complete.
- Provider B releases `lck` and goes to sleep for some time. It will preiodically re-lock `lck` and check `siz` and `wrt` again to check if data is being downloaded or if it needs to do it itself.
- Provider A downloads data, processes it and writes it to its locally mmap-ed sections. Updates are propagated to B by the cache coherency system.
- Provider A attempts and succeeds in acquiring `lck`. It then clears `wrt`, updates `siz` to reflect its writes, and then releases `lck`.
- Provider A then proceeds to read the data range that it was originally interested in.
- Provider B wakes up and acquires `lck`. It then checks `siz` and notices that the data that it wants is now here. It releases `lck`.
- Provider b then proceeds to read the data range that it was originally interested in.


## Design

Let's take a look at what a system with two bots attempting to process years 2020 and 2021 of NVDA would look like. 

{{< figure
	src="images/bot/bot_prv_syn.svg"
	caption="Structure of the data provider."
	alt="prv sync"
	>}}

On the top, we see the remote provider, used by bots to download data. 

In the middle, we see our two bots. Each ultimately wants to create a buffer containing all volumes and values data for NVDA [2020, [2021[.

Both boths have their own local provider, and they have opened the storage files containing NVDA data for 2020 and 2021. 

On the bottom, we see a representation of those two files. 

## Storage file layout

```
  Offset        Description    Data
┌────────────┬──────────────┬───────────────────────────┐
│            │              │                           │
│            │              │  Marketplace              │
│  0         │  Descriptor  │  Symbol                   │
│            │              │  Total number of minutes  │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │  Sync data access lock    │
│ PG_SIZ     │  Sync data   │  Write flag               │
│            │              │  size counter             │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│            │              │  Raw array of values      │
│ 2 * PG_SIZ │  Values      │  in mid order             │
│            │              │  (f64)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │  Raw array of volumes     │
│ + VA_SIZ   │  Volumes     │  in mid order             │
│            │              │  (f64)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │  Raw array of indices     │
│ + VA_SIZ   │  Prev ids    │  in mid order             │
│ + VO_SIZ   │              │  (u32)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │                           │
│ + VA_SIZ   │   End        │  N/A                      │
│ + VO_SIZ   │              │                           │
│ + PR_SIZ   │              │                           │
│            │              │                           │
└────────────┴──────────────┴───────────────────────────┘
```

First, let's remind that each section (except END...) will be mmap-ed by the trading bot's local provider in order to read and write data. Hence, each section needs to be placed at an offset which allows its mapping. 

As stated above, the MMU only allows us to map entire pages, which means that our section must reside at an offset which is a multiple of a page.

Since our trading bot may run on multiple system, we must only ensure that our file layout is compatible with the systems that we want to run on, by choosing a very large page size. PG_SIZ = 64KiB is enough for modern system.

Since a year has a maximal number of minutes, equal to 366 * 24 * 60 = 383040, each array (volumes, values and prevs IDs) have at most 383040 elements. Hence their maximal size is : 
- VA_SIZ, VO_SIZ : 383040 * sizeof(f64) = 383040 * 8 = 3064320 bytes. 
- PR_SIZ : 3064320 * sizeof(u32) = 3064320 * 4 = 1532160 bytes.

Since we must respect page alignment, the effective size of those arrays can be determined by : 
- VA_SIZ, VO_SIZ : (3064320 + (PG_SIZ - 1)) & ~(PG_SIZ - 1) = 0x2f0000 = 3080192
- PR_SIZ : (1532160 + (PG_SIZ - 1)) & ~(PG_SIZ - 1) = 0x180000 = 1572864 

(This uses the fact that PG_SIZ is a power of 2, so rounding down to it is just masking some bits, and that rounding a number N up to X is equivalent to rounding N + (X - 1) down to X.

## Going further : mapping, shareability, cache coherency and performance.

As we covered in the file system section, whenever a local provider wants to read data for a given year of a given instrument, it will create if needed then open the file corresponding to this (year, instrument) and map its different sections in memory. 

If multiple local providers want to use the same data, they will end up mapping the same data pages in their respective virtual address space. This allows them to use simple atomic operations to coordinate themselves when writing to those buffers.

When doing so, the processor's cache coherency system will take care of :
- actually ensuring the atomicity of those instructions (like Compare And Swap) even in a multi-processing context.
- propagating the writes that one CPU makes to other CPUs, with the assumption that the correct memory barriers are used at the correct places. 

Cache coherency operations have an inherent cost.
When CPU A writes at a given location which is in CPU B's cache, both CPUs need to coordinate so that they all see the same value. If the architecture has implicit ordering guaranteed, they also need to ensure that all previous writes made by A will be visible to B if it tries to read at the related locations.

This is why one of the first rules of performance is : "avoid mutable shared data". Memory accesses are slow, and shared memory accesses is even slower. 

In our design, things are not dramatic as : 
- the descriptor section will never be written, except by the local provider which takes care of creating the file and initializing it. Other local provider will never map this section.
- the synchronization page will be the page whose accesses will be the slowest, as it is inherently here to take advantage of the CPU's cache coherency mechanism. We _want_ accesses to this page to be slow, as most of them will be atomic operations that are here for correctness rather than perf.   
- data pages should not cause us too much latency, as the data that they contain is written by only one CPU before being read by the others. The situation where multiple local providers wait for the same data element should be relatively rare, but in this case, there will be some latency involved, as cache coherency is made at the cache line (64 bytes = 8 64-bits floats) level. What could happen is that :
  - CPU A gets write rights, downloads data, writes it in memory. The write finishes with filling half (4 elements) of a cache line L.
  - CPU B reads the newly written data until the end -> cache line L must be migrated from CPU A to CPU B so that B sees A's updates.
  - CPU B gets write rights for the next data, downloads it, and writes it in memory. The write starts by filling the remaining half of L.
  - CPU A reads the newly written data until the end -> cache line L must be migrated back from CPU B to CPU A so that A sees B's updates.

Since our system never assumes that writes made by a local provider to a location should not be visible to other local providers, we can map all our pages as shareable, and avoid the expensive copy-on-write kernel mechanism.

We can map the pages that we do not intend to modify (descriptor page + full data pages) as read-only, in order to actually prevent anyone from writing to them and ensure that our code is bug-free. Though it may not be a good performance idea, as remap operations are expensive in term of :
- code path, since they involve taking a syscall.
- kernel processing, as they involve updating the kernel's bookkeeping structures to reflect our new attributes.
- HW cost, as they involve updating our MMU's TLB which could take long.

Though, as writing to a full page of data (64KiB / 8 = 8192 minutes = 136 hours = 5 days worth of data) will be relatively rare, one could disregard this perf hit.

## going further : prv array
