---
title: "Trading bot : data provider"
summary: "Data provider"
series: ["Trading bot"]
series_order: 2
categories: ["Trading bot"]
#externalUrl: ""
showSummary: true
date: 2025-06-13
showTableOfContents : true
draft: true
---

This chapter will describe the design choices of the data provision system.

## Participants

The data provider system will feature the following participants : 
- multiple remote providers, each providing data for a specific set of instruments in a specific time range. It is assumed that those remote providers provide the exact same information and that when multiple providers can provide the same data elements, any can be choosed.
- multiple data consumers, which want to use data provided by remote providers. Consumers can be strategies, or simulation brokers.

## Consumer needs

The data consumers need to do the two following types of request : 
- range request : takes a start minute index (stt) and an end minute index (end) and for the time interval [stt, end[, generate : 
  - a volumes array with one volume per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0.
  - a values array with one value per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0.
- value request : takes `tgt` a target minute index (tgt), and determines `mid` the greatest minute index inferior or equal to `tgt` with a non-zero volume, then returns : 
  - `mid`.
  - `vol` the volume at `mid`.  
  - `val` the value at `mid`.  

In practice, the range request is used when we want to cache data, or compute statistics like moving averages, or correlations, and the value request is used to find the most recent volume and price at a given moment in time.

## Remote provider format

Remote providers like polygonio provide rest-ful APIs to fetch data. Those APIs will likely transfer data in plain text formats like JSon, that the local provider will have to parse and convert into floats for the rest of the system to use them.

Rest APIs are thread-safe by design, so there will be no need to coordinate the possibly multiple active trading cores to fetch data.

## Constraints

First, as stated in the previous chapters, it is essential to avoid downloading anything from remote providers if it is not strictly necessary. That gives us our first two constraints :
- data downloaded from the provider must be stored to disk and used in place of the remote provider when possible.
- local providers running on the same machine must collaboratively use the same storage.

Then, as stated in the 'Consumer neers' section, data is ultimately to be either read by value or stored in contiguous memory arrays. This gives us our third constraint : 
- disk storage must be efficient for both value and range read.

In practice, this means that our disk storage will store data in contiguous arrays which will follow the same padding rules as the arrays generated by range requests : if at a given minute ID there has been no transactions, then both volume and value array alements will be 0.

My proposal for the design answering those three constraints uses the file system and its memory mapping capabilities as storage medium. To understand why, let's cover how modern operating systems manage disk storage and file memory mapping in a simplified way.

## File system and buffer mapping in userspace. 

### File system, disk and kernel buffers.

TODO

### MMU, virtual address space

TODO

### MMAP 

TODO rework this.

The file system offers us multiple ways to configure how pages are mapped in one's virtual address space : 
- mapping shareability : in our case it defines if we want to see the updates made by other entities to our page (map as shareable) or if we do not want to see them (map as private). Our system never involves pages where one entity will write, and where other entities do not want to see those writes. As such, we can avoid mwill avoid the expensive copy-on-write.
- mapping permissions : in our case, it defines if one is able to write data in a page. Mapping a page as read-only will make write operations fault (which will lead to the killing of our process by the kernel, or at least to the generation of a SIGSEGV). It will also have an impact on the precautions employed by the kernel to manage the different mappings to this page. For example, multiple processes mapping the same page as private read-only will reduce the kernel's bookkeeping, as the kernel knows that it will never have to do copy-on-write maintenance for this page, as no one will write to it.  


TODO

## Design

{{< figure
	src="images/bot/bot_prv_syn.svg"
	caption="Data provider synchronization example."
	alt="prv sync"
	>}}

TODO

## Storage file layout

```
  Offset        Description    Data
┌────────────┬──────────────┬───────────────────────────┐
│            │              │                           │
│            │              │  Marketplace              │
│  0         │  Descriptor  │  Symbol                   │
│            │              │  Total number of minutes  │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │  Sync data access lock    │
│ PG_SIZ     │  Sync data   │  Write flag               │
│            │              │  *Fill counter            │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│            │              │  Raw array of values      │
│ 2 * PG_SIZ │  Values      │  in mid order             │
│            │              │  (f64)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │  Raw array of volumes     │
│ + VA_SIZ   │  Volumes     │  in mid order             │
│            │              │  (f64)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │  Raw array of indices     │
│ + VA_SIZ   │  Prev ids    │  in mid order             │
│ + VO_SIZ   │              │  (u32)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │                           │
│ + VA_SIZ   │   End        │  N/A                      │
│ + VO_SIZ   │              │                           │
│ + PR_SIZ   │              │                           │
│            │              │                           │
└────────────┴──────────────┴───────────────────────────┘
```

First, let's remind that each section (except END...) will be mmap-ed by the trading bot's local provider in order to read and write data. Hence, each section needs to be placed at an offset which allows its mapping. 

As stated above, the MMU only allows us to map entire pages, which means that our section must reside at an offset which is a multiple of a page.

Since our trading bot may run on multiple system, we must only ensure that our file layout is compatible with the systems that we want to run on, by choosing a very large page size. PG_SIZ = 64KiB is enough for modern system.

Since a year has a maximal number of minutes, equal to 366 * 24 * 60 = 383040, each array (volumes, values and prevs IDs) have at most 383040 elements. Hence their maximal size is : 
- VA_SIZ, VO_SIZ : 383040 * sizeof(f64) = 383040 * 8 = 3064320 bytes. 
- PR_SIZ : 3064320 * sizeof(u32) = 3064320 * 4 = 1532160 bytes.

Since we must respect page alignment, the effective size of those arrays can be determined by : 
- VA_SIZ, VO_SIZ : (3064320 + (PG_SIZ - 1)) & ~(PG_SIZ - 1) = 0x2f0000 = 3080192
- PR_SIZ : (1532160 + (PG_SIZ - 1)) & ~(PG_SIZ - 1) = 0x180000 = 1572864 

(This uses the fact that PG_SIZ is a power of 2, so rounding down to it is just masking some bits, and that rounding a number N up to X is equivalent to rounding N + (X - 1) down to X.

## Going further ; mapping, shareability, cache coherency and performance.

As we covered in the file system section, whenever a local provider wants to read data for a given year of a given instrument, it will create if needed then open the file corresponding to this (year, instrument) and map its different sections in memory. 

If multiple local providers want to use the same data, they will end up mapping the same data pages in their respective virtual address space. This allows them to use simple atomic operations to coordinate themselves when writing to those buffers.

When doing so, the processor's cache coherency system will take care of :
- actually ensuring the atomicity of those instructions (like Compare And Swap) even in a multi-processing context.
- propagating the writes that one CPU makes to other CPUs, with the assumption that the correct memory barriers are used at the correct places. 

Cache coherency operations have an inherent cost.
When CPU A writes at a given location which is in CPU B's cache, both CPUs need to coordinate so that they all see the same value. If the architecture has implicit ordering guaranteed, they also need to ensure that all previous writes made by A will be visible to B if it tries to read at the related locations.

This is why one of the first rules of performance is : "avoid mutable shared data". Memory accesses are slow, and shared memory accesses is even slower. 

In our design, things are not dramatic as : 
- the descriptor section will never be written, except by the local provider which takes care of creating the file and initializing it. Other local provider will never map this section.
- the synchronization page will be the page whose accesses will be the slowest, as it is inherently here to take advantage of the CPU's cache coherency mechanism. We _want_ accesses to this page to be slow, as most of them will be atomic operations that are here for correctness rather than perf.   
- data pages should not cause us too much latency, as the data that they contain is written by only one CPU before being read by the others. The situation where multiple local providers wait for the same data element should be relatively rare, but in this case, there will be some latency involved, as cache coherency is made at the cache line (64 bytes = 8 64-bits floats) level. What could happen is that :
  - CPU A gets write rights, downloads data, writes it in memory. The write finishes with filling half (4 elements) of a cache line L.
  - CPU B reads the newly written data until the end -> cache line L must be migrated from CPU A to CPU B so that B sees A's updates.
  - CPU B gets write rights for the next data, downloads it, and writes it in memory. The write starts by filling the remaining half of L.
  - CPU A reads the newly written data until the end -> cache line L must be migrated back from CPU B to CPU A so that A sees B's updates.

Since our system never assumes that writes made by a local provider to a location should not be visible to other local providers, we can map all our pages as shareable, and avoid the expensive copy-on-write kernel mechanism.

We can map the pages that we do not intend to modify (descriptor page + full data pages) as read-only, in order to actually prevent anyone from writing to them and ensure that our code is bug-free. Though it may not be a good performance idea, as remap operations are expensive in term of :
- code path, since they involve taking a syscall.
- kernel processing, as they involve updating the kernel's bookkeeping structures to reflect our new attributes.
- HW cost, as they involve updating our MMU's TLB which could take long.

Though, as writing to a full page of data (64KiB / 8 = 8192 minutes = 136 hours = 5 days worth of data) will be relatively rare, one could disregard this perf hit.

## going further : prv array
