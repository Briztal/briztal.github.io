---
title: "Trading bot : data provider"
summary: "Data provider"
series: ["Trading bot"]
series_order: 2
categories: ["Trading bot"]
#externalUrl: ""
showSummary: true
date: 2025-06-13
showTableOfContents : true
draft: true
---

This chapter will describe the design choices of the data provision system.

## Participants

The data provider system will feature the following participants : 
- multiple remote providers, each providing data for a specific set of instruments in a specific time range. It is assumed that those remote providers provide the exact same information and that when multiple providers can provide the same data elements, any can be choosed.
- multiple data consumers, which want to use data provided by remote providers. Consumers can be strategies, or simulation brokers.

## Consumer needs

The data consumers need to do the two following types of request : 
- range request : takes a start minute index (stt) and an end minute index (end) and for the time interval [stt, end[, generate : 
  - a volumes array with one volume per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0.
  - a values array with one value per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0.
- value request : takes `tgt` a target minute index (tgt), and determines `mid` the greatest minute index inferior or equal to `tgt` with a non-zero volume, then returns : 
  - `mid`.
  - `vol` the volume at `mid`.  
  - `val` the value at `mid`.  

In practice, the range request is used when we want to cache data, or compute statistics like moving averages, or correlations, and the value request is used to find the most recent volume and price at a given moment in time.

## Remote provider format

Remote providers like polygonio provide rest-ful APIs to fetch data. Those APIs will likely transfer data in plain text formats like JSon, that the local provider will have to parse and convert into floats for the rest of the system to use them.

Rest APIs are thread-safe by design, so there will be no need to coordinate the possibly multiple active trading cores to fetch data.

## Constraints

First, as stated in the previous chapters, it is essential to avoid downloading anything from remote providers if it is not strictly necessary. That gives us our first two constraints :
- data downloaded from the provider must be stored to disk and used in place of the remote provider when possible.
- local providers running on the same machine must collaboratively use the same storage.

Then, as stated in the 'Consumer neers' section, data is ultimately to be either read by value or stored in contiguous memory arrays. This gives us our third constraint : 
- disk storage must be efficient for both value and range read.

In practice, this means that our disk storage will store data in contiguous arrays which will follow the same padding rules as the arrays generated by range requests : if at a given minute ID there has been no transactions, then both volume and value array alements will be 0.

My proposal for the design answering those three constraints uses the file system and its memory mapping capabilities as storage medium. To understand why, let's cover how modern operating systems manage disk storage and file memory mapping in a simplified way.

## Operating systems concepts.

### File system, disk and kernel buffers.

In the rest of this article :
- PA will be used as an accronym for "Physical Address", aka the addresses used in actual memory transactions on the bus (see below).
- VA will be used as an accronym for "Virtual Address", aka the addresses used by code running in CPUs.

The translation between the two and its impact will be covered in the next section. 

First, let's cover how the kernel manages storage devices (disks).

Some facts first.

Processors are distributes system with many components, among them :
- multiple CPUs, which need to access DRAM.
- multiple peripherals (Ethernet interface, USB interface, PCIE interface) that (in broad terms) are meant to be programmed by CPUs and that in some cases must access DRAM. 
- multiple DRAM slots.

The solution to make those entities interact with each others in an unified way is to introduce another entity, the interconnect, abusively called "memory bus" in all this chapter, which connect them all.
In particular : 
- it defines the notion of physical address.
- connected clients can be masters (can initiate transactions), slaves (can process transactions) or both. 
- it allows its masters to initiate implementation-defined transactions which target a given physical address. It is a functional simplification to consider that memory read and writes are particular cases of such transactions.
- it allows CPUs to configure and control peripherals by initiating transactions at PA ranges that peripherals are programmed to respond to.
- it allows CPUs and peripherals to read or write to DRAM by initiating transactions at PA ranges that DRAM controllers are programmed to respond to.

Storage devices like your average SATA or SSD drive :
- are meant to be pluggable and hence have their dedicated connectors and HW communication protocols.
- are not meant to interface with processor interconnects directly. One cannot just make an access at a given PA and expect a storage device to just process this access.
- cannot be accessed at a random granularity (byte, cache line, etc...). The same way the cache system cannot perform accesses at a granule inferior to the cache line, storage devices perform reads and writes at a minimal granularity, the sector size, which is a HW characteristic.
- consequence : accessing the data contained in a storage device (ex : a SATA disk) is made via a dedicated peripheral connected to the interconnect, providing a HW interface to talk to your disk, and requiring a kernel driver to control with this peripheral.

Here is a more complex but still extremely simplified view of how we can make code running in CPUs access (DRAM copy of) data located in a disk.

{{< figure
	src="images/bot/prv_fs_dsk.svg"
	caption="Interacting with a hard drive."
	alt="prv fs dsk"
	>}}

Participants : 
- interconnect : allowing all its clients to communicate with each other.
- CPU : executes user code. Interconnect master only for this example's purpose.
- DRAM : the actual memory of your system. Interconnect slave, processes transactions, by converting them to actual DRAM read and writes. 
- Disk : a storage device, that is controlled via an implementation-defined protocol only handled by the storage interface. Not directly connected to the interconnect.
- Storage interface : interfaces the interconnect and the disks. Must be programmed by an external entity to do so. Interconnect slave : processes memory transactions targetting its adress range as configuration requests, which allows external entites to control its behavior, for example to power up the disk, initiate a sector read or write (etc...). Interconnect master : forwards disk data to DRAM via write interconnect transactions.

Now, let's describe the different sequences involved to allow CPUs to read data at two different locations on the disk. We will here suppose that CPUs can read at arbitrary PAs, and the next section will complexify this model. 

Step A : storage interface setup.

The kernel running on CPU 0 does the following things : 
- it allocates two buffers of DRAM using its primary memory allocator, in a zone that allows the storage interface to perform memory transactions (DMA16 or more likely DMA32). 
- it sends control transactions to the storage interface via the memory bus, to instruct it to perform two disk reads and to store the resulting data at the PAs of the buffers that it allocated. 

Step B : disk reads.

The storage interface has understood the instructions sent by the CPU and starts the disk read sequences. The disk will respond with two consecutive data stream, that the storage interface will then transmit to DRAM using memory bus transactions (DMA).  

Once the reads are done, the storage interface notifies the kernel of their completion by sending an IRQ targetting one of the CPUs.

Step C : CPU access.

Now that DRAM buffers have been initialized with a copy of the data at the required disk locations, CPU can read those buffers by simply performing memory reads at PAs withing buffers A and B (provided that they have invalidated their caches for the memory ranges of A and B).

Now, an immediate consequence of what we covered here is that CPUs _cannot_ modify the data on disk themselves. All they can do is to modify the DRAM copy.

If updates need to be propagated to disk, the kernel will need to initiate another transaction (a disk write this time), instructing the storage interface to read from DRAM and write data to disk. 

From userspace, this is typically initiated via fsync or msync.

### MMU, virtual address space

In the previous section, we assumed that the different CPUs in the system could write to arbitrary PAs.

The reality is more nuanced, as PA access is critical.
First, allowing any userspace process to access any PA basically allows it to read any other process's data. Just for security reasons, we can't let that happen.
Then, we saw in the previous sections that some PAs correspond to HW configuration ranges, and if mis-used, can cause HW faults which often just make the system immediately panic. For practicality, we also cannot let that happen.
Finally, most programs are not meant to deal with PA itself. Physical memory management is a critical task of the kernel, and physical pages are usually allocated in a non-contiguous manner, on a page-per-page basis. Program often need to access large contiguous memory regions (like their whole 4MiB wide executable), and hence, have needs that are incompatible with how physical memory is intrinsicly managed by the kernel.

For all those reasons and others, code running on CPUs (whether it be kernel code or userspace code) does not operate with physical addresses directly.

Rather, it operates using virtual addresses, which translate (or not) into physical addresses using an in-memory structure that SW can configure but whose format is dictated by the HW, called a page table. Every CPU in the system is equipped with a Memory Management Unit or MMU, which translates any VA used by the CPU to the corresponding PA. 

This article will not dive into the format of the pagetable, but here are a few facts worth mentioning :
- VA->PA translation is done at the aligned page granule. Virtual memory is divided into pages of usually 4, 16 or 64 KiBs and each virtual page can map to at most one physical page. When I say map to, understand 'set by the pagetable content to be translated to'
- multiple virtual pages can map to the same physical page.
- the pagetable contains the translation data and also attributes like translation permissions (can data be read, written, executed, and by who ?) and cacheability.
- the pagetable is dynamically writeable, but usually by privileged software (kernel) only.

Now let's add some details on how the CPUs will need to map memory to perform the various transactions described in the previous section :
- to program the storage interface, CPUs will need to map the PA region corresponding to the peripheral's control range into a range in their virtual address space (in reality, into the kernel's virtual address space which all CPUs use in kernel mode), and perform memory accessed from this virtual range. The mapping will be RWnE (read/write/non-execute) and non-cacheable, as we want the control transaction not to be affected by the CPU's cache system.
- to read from and write to DRAM, CPUs will again need to map the PA regions corresponding to the buffer A and B into the kernel's virtual address space and perform their accesses from this virtual range. The mappings will be cacheable, but a few precautions will need to be taken to ensure that reads see the data written by the storage interface, and that reads by the storage interface (during disk writes) see the writes made by the CPU : 
  - before reading data written by the storage interface, CPUs will need to invalidate the ranges for buffers A and B, so that when they read at those addresses, data is re-fetched from memory.
  - before instructing the storage interface to write to disk, CPUs will need to flush the ranges for buffers A and B, so that the writes made by CPUs is propagated to memory and observed by the storage interface. 

### MMAP 

TODO rework this.

The file system offers us multiple ways to configure how pages are mapped in one's virtual address space : 
- mapping shareability : in our case it defines if we want to see the updates made by other entities to our page (map as shareable) or if we do not want to see them (map as private). Our system never involves pages where one entity will write, and where other entities do not want to see those writes. As such, we can avoid mwill avoid the expensive copy-on-write.
- mapping permissions : in our case, it defines if one is able to write data in a page. Mapping a page as read-only will make write operations fault (which will lead to the killing of our process by the kernel, or at least to the generation of a SIGSEGV). It will also have an impact on the precautions employed by the kernel to manage the different mappings to this page. For example, multiple processes mapping the same page as private read-only will reduce the kernel's bookkeeping, as the kernel knows that it will never have to do copy-on-write maintenance for this page, as no one will write to it.  


TODO

## Design

{{< figure
	src="images/bot/bot_prv_syn.svg"
	caption="Data provider synchronization example."
	alt="prv sync"
	>}}

TODO

## Storage file layout

```
  Offset        Description    Data
┌────────────┬──────────────┬───────────────────────────┐
│            │              │                           │
│            │              │  Marketplace              │
│  0         │  Descriptor  │  Symbol                   │
│            │              │  Total number of minutes  │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │  Sync data access lock    │
│ PG_SIZ     │  Sync data   │  Write flag               │
│            │              │  *Fill counter            │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│            │              │  Raw array of values      │
│ 2 * PG_SIZ │  Values      │  in mid order             │
│            │              │  (f64)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │  Raw array of volumes     │
│ + VA_SIZ   │  Volumes     │  in mid order             │
│            │              │  (f64)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │  Raw array of indices     │
│ + VA_SIZ   │  Prev ids    │  in mid order             │
│ + VO_SIZ   │              │  (u32)                    │
│            │              │                           │
├────────────┼──────────────┼───────────────────────────┤
│            │              │                           │
│ 2 * PG_SIZ │              │                           │
│ + VA_SIZ   │   End        │  N/A                      │
│ + VO_SIZ   │              │                           │
│ + PR_SIZ   │              │                           │
│            │              │                           │
└────────────┴──────────────┴───────────────────────────┘
```

First, let's remind that each section (except END...) will be mmap-ed by the trading bot's local provider in order to read and write data. Hence, each section needs to be placed at an offset which allows its mapping. 

As stated above, the MMU only allows us to map entire pages, which means that our section must reside at an offset which is a multiple of a page.

Since our trading bot may run on multiple system, we must only ensure that our file layout is compatible with the systems that we want to run on, by choosing a very large page size. PG_SIZ = 64KiB is enough for modern system.

Since a year has a maximal number of minutes, equal to 366 * 24 * 60 = 383040, each array (volumes, values and prevs IDs) have at most 383040 elements. Hence their maximal size is : 
- VA_SIZ, VO_SIZ : 383040 * sizeof(f64) = 383040 * 8 = 3064320 bytes. 
- PR_SIZ : 3064320 * sizeof(u32) = 3064320 * 4 = 1532160 bytes.

Since we must respect page alignment, the effective size of those arrays can be determined by : 
- VA_SIZ, VO_SIZ : (3064320 + (PG_SIZ - 1)) & ~(PG_SIZ - 1) = 0x2f0000 = 3080192
- PR_SIZ : (1532160 + (PG_SIZ - 1)) & ~(PG_SIZ - 1) = 0x180000 = 1572864 

(This uses the fact that PG_SIZ is a power of 2, so rounding down to it is just masking some bits, and that rounding a number N up to X is equivalent to rounding N + (X - 1) down to X.

## Going further ; mapping, shareability, cache coherency and performance.

As we covered in the file system section, whenever a local provider wants to read data for a given year of a given instrument, it will create if needed then open the file corresponding to this (year, instrument) and map its different sections in memory. 

If multiple local providers want to use the same data, they will end up mapping the same data pages in their respective virtual address space. This allows them to use simple atomic operations to coordinate themselves when writing to those buffers.

When doing so, the processor's cache coherency system will take care of :
- actually ensuring the atomicity of those instructions (like Compare And Swap) even in a multi-processing context.
- propagating the writes that one CPU makes to other CPUs, with the assumption that the correct memory barriers are used at the correct places. 

Cache coherency operations have an inherent cost.
When CPU A writes at a given location which is in CPU B's cache, both CPUs need to coordinate so that they all see the same value. If the architecture has implicit ordering guaranteed, they also need to ensure that all previous writes made by A will be visible to B if it tries to read at the related locations.

This is why one of the first rules of performance is : "avoid mutable shared data". Memory accesses are slow, and shared memory accesses is even slower. 

In our design, things are not dramatic as : 
- the descriptor section will never be written, except by the local provider which takes care of creating the file and initializing it. Other local provider will never map this section.
- the synchronization page will be the page whose accesses will be the slowest, as it is inherently here to take advantage of the CPU's cache coherency mechanism. We _want_ accesses to this page to be slow, as most of them will be atomic operations that are here for correctness rather than perf.   
- data pages should not cause us too much latency, as the data that they contain is written by only one CPU before being read by the others. The situation where multiple local providers wait for the same data element should be relatively rare, but in this case, there will be some latency involved, as cache coherency is made at the cache line (64 bytes = 8 64-bits floats) level. What could happen is that :
  - CPU A gets write rights, downloads data, writes it in memory. The write finishes with filling half (4 elements) of a cache line L.
  - CPU B reads the newly written data until the end -> cache line L must be migrated from CPU A to CPU B so that B sees A's updates.
  - CPU B gets write rights for the next data, downloads it, and writes it in memory. The write starts by filling the remaining half of L.
  - CPU A reads the newly written data until the end -> cache line L must be migrated back from CPU B to CPU A so that A sees B's updates.

Since our system never assumes that writes made by a local provider to a location should not be visible to other local providers, we can map all our pages as shareable, and avoid the expensive copy-on-write kernel mechanism.

We can map the pages that we do not intend to modify (descriptor page + full data pages) as read-only, in order to actually prevent anyone from writing to them and ensure that our code is bug-free. Though it may not be a good performance idea, as remap operations are expensive in term of :
- code path, since they involve taking a syscall.
- kernel processing, as they involve updating the kernel's bookkeeping structures to reflect our new attributes.
- HW cost, as they involve updating our MMU's TLB which could take long.

Though, as writing to a full page of data (64KiB / 8 = 8192 minutes = 136 hours = 5 days worth of data) will be relatively rare, one could disregard this perf hit.

## going further : prv array
