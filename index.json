[{"content":"Hi, I\u0026rsquo;m Raphael and I build stuff.\n","date":"27 September 2023","permalink":"briztal.github.io/about/","section":"Embedded Garage","summary":"Hi, I\u0026rsquo;m Raphael and I build stuff.","title":"About"},{"content":"As stated in the previous sections, to verify our accesses, we will use the MPU to blacklist all the RAM minus the stacks, which will make any access to these instructions trap.\nThen, the memory checker will use the context saved data to retrieve the PC of the instruction that caused the memory access, then decode it, then emulate it, checking the various accesses performed by the instruction in the meantime.\nOur emulator will be composed of two base blocks : First, a set of instruction emulation functions that will, for each instructions that we support, emulate the instruction. That involves :\nread the content of the context-saved registers. write the content of the context-saved registers. read memory. write memory. perform KASAN check whenever a memory access is performed. Second, a decoder, that will, given the binary encoding of an instruction, extract the instruction opcode, extract the relevant fields in the instruction, then call the C emulator function associated with that opcode.\nDecoder # The objective of the decoder will be to decide which instruction emulation function to call and with which argument, given the binary encoding of an instruction that caused a memory access.\nThis is where things get tricky.\nIndeed, an ARMV7M CPU like the cortex M7 executes ARM-tumb assembly. Sadly for us, the ARM Thumb assembly is NOT a fixed length instruction.\nAn ARM Thumb assembly instruction has one of two forms :\n16 bits instructions. 32 bits instructions, composed of two 16 bits words. No field crosses the two 16 bits words. This historically comes from the various additions to the THUMB instruction set, that forced the adoption of 32 bits instructions, as 16 bits were not enough to represent all required instructions, like VFP.\nAs we will see, writing a decoder is a real pain, but can be done really fast and automatically if we have the right tools. Though, decoding a variable-length instruction set is a REAL problem, and is much more complex than writing a fixed-length instruction set decoder.\nThink about it : in order for our decoder to process the instruction, it must read the actual value of the instruction. But in the case of a variable-length instruction, how can it know the length to read ? It should read byte by byte only whenever needed, which will greatly complexify the code, compared to a fixed length instruction set, where it only has to read once and (in a couple of words) bisect depending on the values of certain bits.\nLuckily, ARM makes our life easier by allowing us to detect if a word known to be the start of an instruction is the start of a 16 bits instruction or a 32 bits instruction :\nhttps://developer.arm.com/documentation/ddi0403/d/Application-Level-Architecture/The-Thumb-Instruction-Set-Encoding/Thumb-instruction-set-encoding#:~:text=The%20Thumb%20instruction%20stream%20is,consecutive%20halfwords%20in%20that%20stream.\nIf bits [15:11] of the halfword being decoded take any of the following values, the halfword is the first halfword of a 32-bit instruction: 0b11101 0b11110 0b11111.\nThis will allow us to divide the tough problem of decoding the variable length THUMB ISA into two much simpler problems, being decoding either the fixed length THUMB32 ISA or the THUMB16 ISA.\nThe first task of our decoder will be to determine if the instruction that faulted is a 16 bits instruction or a 32 bits instruction. Then, the decoder will call the relevant sub-decoder.\nBasic structure of a fixed-length instruction decoder # An instruction is composed of a fixed number of bits, each one having a particular meaning.\nWe can group these bits into three groups, for a given instruction :\nopcode bits : those bits will determine the sequence operation performed by the instruction, aka in our case, the emulation function to call to execute this instruction. data bits : those bits will provide configs and arguments like register ids to the sequence of operations, aka in our case, arguments to our emulation functions. unused bits : bits whose value do not influence the behavior of a given instruction. TODO PROVIDE AN EXAMPLE\nThe objective of our decoder will be, given a particular instruction represented by its binary encoding :\nto read the opcode bits and to determine which An instruction is decoded in hardware by the CPU (by a block called, oh surpruse, the decoder), and as such, the structure of the instruction is made to allow a fast decoding.\nIn real life :\ndata bits representing the same entity (an immediate, a register ID) will almost always be contiguous. We will call that entity a field. opcode bits may or may not be contiguous. the location of the different fields and opcode parts of the various instructions are often similar accross instructions of the same category. Let\u0026rsquo;s take a look at the structure of the THUMB16 ISA encoding to illustrate that point\nAs you can see, we definitly can see a pattern here in the placement of the opcodes and registers.\nThere is one exception to the rules stated before, which will kind of complefify the generation of the decoer : there are cases where two different instructions will have the exact same opcode bits, but one should be executed if a field (ex : register ID) has a specific value, and the other should be executed otherwise.\nEven if we can\u0026rsquo;t thank ARM for this rather unpleasant corner case, let\u0026rsquo;s thank them for not getting completely out of their minds and supporting the same situation but with a possible SET of values on each side rather than one value VS all others. That would have been a nightmare to manage. Thanks ARM.\nAll jokes aside, this will add a non-outrageous level of complexity to the decoder.\nGiven what I have described until now, you should start to have an idea of the structure of our decoder. It will be composed of :\na HUGE nested ifelse section (rather an ifelse tree) where we will bisect on the value of one single bit at a time; in the leaf parts of the ifelse tree either : a jump to handle an UNDEFINED instruction (aka an instruction not supported by the decoder); or a section to extract all data fields from the instruction encoding, and to call the related emulator function. If you are intrested by writing such piece of code, be assured that I\u0026rsquo;m not, as it is a nightmare to read, to write and to debug : any incorrect bit index can radically change the behavior of the decoder and cause some undebuggable issue.\nARM exploration tools # Luckily for US, ARM is in an even worse situation than us.\nIndeed, we only have to generate a decoder for the memory instructions. They have to generate decoders for ALL instructions, both in HW and in their SW tools. Due to that, it is highly unlikely that they didn\u0026rsquo;t come up with a solution to have those pieces of code easily generated.\nWhen releasing the ARMV8, ARM also released what they call the ARM Exploration tools, aka HUGE xml/html files that describe the structure of instructions, and in particular, THEIR ENCODINGS.\nThe reader could think, \u0026ldquo;Yeah, but they only released it for ARMV8, right ?\u0026rdquo;, which is true but kind of false in practice, as ARM ISAs are backwards compatible : the A32/T32 xml doc contains the encoding for the legacy ARM32 instruction, and all releases of the Thumb ISA.\nThis is \u0026hellip; PERFECT. I can\u0026rsquo;t thank ARM enough for doing so, as it will prevent us from needing to manually read the spec and translate the text-based encoding into C code.\nThanks to that, it is easy to come up with a python script that will transform those XML files into an easily processable text representation like the following :\nTODO EXAMPLE + EXPLANATION.\nThis code is not open-source.\nThe spec has downsides though :\nit doesn\u0026rsquo;t really gives the encoding in a direct manner. Rather, it gives the indices and lengths of the various fields that compose an instruction. Bit values within fields are represented using multiple notations (0, 1, \u0026lsquo;\u0026rsquo;, \u0026lsquo;Z\u0026rsquo;, \u0026lsquo;N\u0026rsquo;) and lack a bit of structure. When writing your parser, you will discover most of these corner cases the hard way. some instructions use values derived from MULTIPLE fields (ex : imm3:imm8). I searched a lot and I couldn\u0026rsquo;t find any machine-readable information that would allow us to automatically generate code to do those combinations automatically. For now, the emulation functions will have to take care of that. Going crazy : generating our decoder automatically. # Using the instruction representation provided by the python script, it was easy and fun to write a small program that generates a C decoder using this representation. The program proceeds the following way :\nit reads the description of each instruction to decode. then it generates a bisection tree by recursively dividing the instruction set in two based on the value of a particular bit know (either 0 or 1 but not x) for all bits. then, it generates the ifelse tree based on that, and the field extraction code based on each instruction description. This code is again close-source, but if the reader needs to be convinced that what I describe is possible, they may find a procedurally generated decoder for a part of the AArch32 ISA (with minor changes), through this link.\nThis code is not public domain and you should not use it as it will not work as it : the minor changes that I just mentioned are actually me intentionally inverting the encoding of some frequently used instructions.\nIf you need the real decoder, you can get in touch with me !\nTODO\n","date":"27 September 2023","permalink":"briztal.github.io/projects/kasan/kasan_6_emulator/","section":"Personal Projects","summary":"How to execute instructions in software.","title":"ARM Thumb Emulator."},{"content":"","date":"27 September 2023","permalink":"briztal.github.io/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"27 September 2023","permalink":"briztal.github.io/","section":"Embedded Garage","summary":"","title":"Embedded Garage"},{"content":"This article will give technical details on the hardware capabilities used to implement our KASAN.\nI\u0026rsquo;d like to point out that the KASAN implementation is not specific to this chip. The only features needed for the KASAN to theoretically work are :\nan exception handling mechanism. an MPU. The chip # TODO PICTURE.\nLet\u0026rsquo;s introduce the development board on which I built the KASAN.\nThe devboard that my dear wife made for me has :\nSTM32H750 processor; featuring a CortexM7 CPU implementing the ARMV7M architecture; a Memory Protection Unit (MPU) conforming to the ARMV7M architecture. a Floating Point Unit. 128KiB of Flash; TODO Around 512KiB of RAM. The MPU is the main component on which our KASAN implementation will be based. It is a needed feature.\nExecution privilleges # A procesor implementing the ARMV7M architecture has two levels of privileges :\nunprivilleged mode : code running in this privilege level has limited access to the processor system registers. privilleged mode : code running in this privilege level has unlimited access to all processor features. In particular, the CPU starts in privileged mode at reset. Then, it is up to the firmware (kernel) to run threads in unprivileged mode.\nCode running in unprivileged mode can do a system call to execute a system handler. This handler will execute in privileged mode. The actual function that will be executed can\u0026rsquo;t be programmed by unprivileged software.\nStacks # A processor implementing the ARMV7M architecture supports two different stacks :\nthe Main Stack, used in handler mode. Stack pointer : MSP the Process Stack (should have been called Thread Stack), used in thread mode. Stack pointer : PSP. The MSP / PSP are aliased to SP. Accessing SP actually accesses the stack pointer active in the current mode (thread or stack).\nExceptions # A processor implementing the ARMV7M architecture supports the notion of exception : code running at a given time can be temporarily stopped, for another piece of code (a handler) to be executed instead. When the handler is done (understand : when the handler function returns), the stopped code resumes its execution transparenly. This mechanism is purely hardware, and serves among other things, to handle system calls and interrupts. The piece of code that is executed is called a handler; when the processor handles an exception, it is said to be in handler mode. When it doesn\u0026rsquo;t, it is said to be thead mode. Thread mode can be privileged or not. Handler mode is always privileged.\nAn exception has an ID, which is used to determine :\nif this exception is enabled (i.e. if the handler is allowed to be called at all when the exception is triggered). the priority of this exception (i.e if the handler is allowed to be called given the currently excecuted exception, if any). the handler to be executed.. The ARMV7M architecture supports at most 512 exceptions.\nThe ARMV7M architecture defines the 16 first exception IDs as system exception\nTODO TABLE.\nThe (at most) 496 remaining exceptions are interrupts.\nWhen an exception A is triggered, the processor determines the priority of A, so as the priority of B, the exception currently handled by the processor, if any. If A is enabled and is more prioritary than B (or if the processor is not currently executing any exception), the current execution context is saved in the local stack (either Main Stack or Process Stack) by the hardware, and the handler of A executed. When the handler of A is complete (when it returns), the processor detects it, and restores the execution context saved on exception entry. Then, the execution of B (or thread code) continues transparently.\nContext. # The ARMV7M context is composed of\ngeneral purpose registers : R0 -\u0026gt; R3 : Caller-saved. R4 -\u0026gt; R11 : Callee-saved. R12 : Caller saved, scratch register. R13 : Caller saved, Stack pointer, alias : SP. R14 : Caller saved, Link register, alias LR. R15 : Caller saved, Program counter, alias PC. Floating point registers (When FPU is implemented and active) : S0 -\u0026gt; S15 : Caller saved. S16 -\u0026gt; S31 : Callee saved. Status registers : xPSR : composite of all status registers, see the ARMV7M ARM for more details. Context saving and exception. # Saving the execution context means atomically storing a subset of the context registers to the currently used stack (Main / Process). Restoring the execution context means atomically loading a subset of the context registers from the stack, and as such, updating the state of the processor. (The loaded subsets and the stored subsets are the same).\nIn the ARMV7M architecture, This subset of saved/restored registers is composed of all the caller-saved registers and of the program status registers. This makes the exception entry / return procedures follow the standard ARMV7M calling conventions, and allows us to implement exception handlers using C code only when required (not here lol).\nTODO context for both WITH FPU and WITHOUT FPU.\nNote : if after a context save, if the stored value of a register is modified, the modified value will be loaded in the register during context restore. This will be the base trick on top of which we will build our emulator.\nMPU # The ARMV7M features an optional MPU, always implemented in the CortexM7.\nThe MPU for Memory Protection Unit, allows the kernel to define up to 8 or 16 (depending on the implementation) MPU regions, and to define access permissions for these MPU regions.\nThese MPU regions must be :\nof a size that is a power of 2 and that is greater or equal than 32 bytes. aligned on their size (start address is a multiple on the size). I will not elaborate a lot on the various access permissions since that would require a chapter of its own. What is important to us is that the MPU allows us to :\nblacklist a memory block, i.e. prevent the CPU to access this memory block while the MPU is active. whitelist a memory block, i.e. allow the CPU to access this memory block while the MPU is active. The MPU regions are indexed from 0 to N (N = nb_mpu_regions - 1), and have a fixed priority : when determining the access permissions for a byte at address X, if multiple MPU regions (let\u0026rsquo;s say A and B) cover X, then the MPU region with the highest index will be selected to decide which permissions use to access X.\nThat allows us, for example, to blacklist a large memory block using a MPU region with low priority, and to whitelist smaller portions of this memory block using MPU regions with a higher priority : when an access to one of the smaller blocks will be made, both the whitelist and the blacklist MPU regions will be considered, but since the whitelist MPU region has a higher priority, it will be selected and access will be allowed. This will be the base trick on top of which we will build the memory access trap system.\n","date":"27 September 2023","permalink":"briztal.github.io/projects/kasan/kasan_2_hw/","section":"Personal Projects","summary":"What our KASAN will run on.","title":"Hardware."},{"content":"","date":"27 September 2023","permalink":"briztal.github.io/series/kasan/","section":"Series","summary":"","title":"KASAN"},{"content":"","date":"27 September 2023","permalink":"briztal.github.io/categories/kasan/","section":"Categories","summary":"","title":"KASAN"},{"content":" Foreword # This series of article will focus on how to implement a KASAN (or a memory access checker) on a microcontroller kernel.\nThe introduction is a reflection of my personal opinions and of the motivations that lead me to implement my KASAN. The reader may find it cleaving, but they should find rest of the article more technically oriented.\nThe need for a proper debug infrastructure. # Recently my wonderful wife made me the greatest present a kernel developper could immagine : a custom development board with a builtin SWD interface.\nDevelopment board. I had been looking for a devbvoard to get back to microcontroller programming for a while and my only need was a proper SWD interface to attach a debugger.\nSeems like a pretty unrestrictive criterion, right ? False.\nMainstream development boards provide a dedicated bootloader chip connected to the main chip\u0026rsquo;s SWD/JTAG interface to handle the flashing for you. Though it may seem like a good idea, as it avoids you the pain of buying your own debug tools, it prevents you from being able to connect your own debugger.\nUnless you are willing to trash those bootloader chips\nA debugger I needed.\nI rely a lot on debug tools to verify my code. My two most used tools are GDB to live debug, and valgrind to check for memory leaks / improper memory accesses. I consider valgrind to be the most essential correctness test, after \u0026ldquo;my code seems to do what it is supposed to do\u0026rdquo;.\nUntil recently, my strategy to debug code that aims to work on a microcontroller has been to code it in pure-C (no assembly) and to make it somehow work in a userspace process. That allowed me to debug it using gdb and valgrind.\nThough this strategy works for abstract pieces of the kernel (not dependent of the actual hardware, ex : file-system, scheduler, resource tracking, etc\u0026hellip;), this is hardly achievable for hardware-dependent ones, like drivers. One could think that we could just write an emulator of some sort and again run the driver in an emulated way. This works but will only give the coverage on the behavior that the emulator supports. Failure to emulate an HW feature will lead to lack of actual coverage on that feature when ran in real HW.\nThe best coverage we can get is to test the code under real operation conditions.\nTo achieve that, we need our favorite debug tools to work in embedded platforms.\nLet\u0026rsquo;s make enemies # The opinion in the embedded world seems to be\nYour code just has to be correct, and if it's not, you can just debug with printf.\nThis is not a satisfactory answer. This barely works for simple software, at the cost of bisecting the code, but becomes a nightmare when you deploy complex software that does memory allocation.\nMoreover, this relies on the fact that you have a kernel that handles your printfs correctly, to the UART that serves as log output. This implies either a booted kernel with device management initialized, or a very small kernel with embedded UART management. But what if we must debug the kernel itself ? What if the UART isn\u0026rsquo;t even available at that stage ? What if we are doing the bringup of the chip, i.e. testing boot / reset ? You won\u0026rsquo;t get any printf at that stage.\nStating that you can always debug using printf is absurd to the extent that, if you have access to a form of printf, you are potentially running your code in an environment like the Arduino framework. I can guarantee you that the people that developped this framework had some form of probe to debug the chip. When their programming and verification was done, they designed a board without this interface and sold it to you.\nComming back to the previous statement.\nThis barely works for simple software, at the cost of bisecting the code, but becomes a nightmare when you deploy complex software that, for example, does memory allocation.\nIf the answer to that last statement is :\nCode on a microcontroller should only be running simple software with statically allocated memory\nthis is not a satisfactory answer either.\nModern microcontrollers (whose name should likely be reviewed) operate at the megahertz scale, and can have around one MiB of flash or RAM. This is the very definiton of \u0026ldquo;potential to run complex software\u0026rdquo;.\nRather, that answer is merely a consequence of the lack of proper debugging tools widely available in the mainstream embedded world, and of the lack of knowledge around it : developpers took the lack of proper debugging infrastructure as an immutable truth and their practices evolved in that direction, validating that assumption.\nBut is is not an immutable truth. To be more precise, it is completely and utterly false.\nThis series of article aims to prove that running your code in a microcontroller environment gives you a leverage to actually perform memory checking of the same or even better quality than one available in a typicall userspace process.\nDisclaimer # The KASAN implementation presented in this series is not open-source nor free, and this series of article is not a tutorial on how to use such code. This is mostly due to two factors :\na KASAN is tightly coupled to the kernel that it is implemented in. My kernel being closed source, such is my KASAN. I\u0026rsquo;m not a very nice guy :). Though, to illustrate my points and give implementation details, I\u0026rsquo;ll sometimes provide code samples. For all intents and purposes, I declare this code as part of the public domain.\nRather, it aims to give a solid base for whoever wants to add a KASAN to their kernel, so that when executing the following piece of code :\nu8 faulty_function( void ) { u32 *mem = kernel_malloc(sizeof(u32)); kernel_free(mem); kernel_free(mem); } the kernel goes from an ugly crash to :\nKr0 started. Welcome. Kr0 initialized. Kr1 started, core1 0x0 online. core1 : 0x30044900. Kr1 initialized. Kr2 started. Kr2 initialized. Initializing pre-smp kernel modules. Double free. Unexpected kasan attrs found during attrs set at address 0x20014038. Expected type : USR_ANY (exact match). NO ACCESS_ALLOCATOR | ACCESS_USER Read type : ALL_RWI (exact match). ACCESS_ALLOCATOR | NO ACCESS_USER | READ_WRITE | INITIALIZED Incompatible attributes are : ACCESS_ALLOCATOR | NO ACCESS_USER ","date":"27 September 2023","permalink":"briztal.github.io/projects/kasan/kasan_0_intro/","section":"Personal Projects","summary":"Valgrind on a microcontroller.","title":"KASAN Introduction."},{"content":"Now that we know how the kernel manages memory from a high level point of view, let\u0026rsquo;s elaborate on how the KASAN will fit in this system.\nMetadata # The kasan will have to track the status of each byte of memory in the system.\nFor this, it will need metadata, that it will store in dedicated memory : each byte of memory managed by the memory system will have attributes, that will describe :\nif the byte is accessible (as defined in Regions manager section) ? who can access the byte, either user, allocator or both ? is the byte writeable ? is the byte initialized ? This can be achieved by storing 4 bits of attributes per byte of memory.\nAttributes location # The attributes must be located somewhere in memory. To place them, we will use the same strategy as the Regions allocator.\nWhen registering a memory regions in the regions manager, before the region manager allocates all its metadata, the kasan will split the region in two parts :\nkernel-accessible memory : this part will be forwarded to the region manager, that will divide it in two, pages, and pages metadata. kasan attributes : this part will contain 4 bits of attributes for each byte of memory in the kernel-accessible section. Attributes lifecycle # The attributes are the base information that the kasan uses to verify a memory access. A memory access of N bytes starting at A will cause the kasan to fetch the attributes for the address range [A, A + N[, and verify that they allow the access to occur. If so, the kasan will let the access happen. Otherwise, it will trigger an error.\nHere are some accesses that should cause an error to occur :\nreading from an uninitialized byte. writing to a non-writeable byte. user accessing a byte not accessible to the user. allocator accessing a byte not accessible to the allocator. access to a nonn accessible byte. The following diagram describes the lifecycles of attributes, and the various events (accesses) that can occur, so as the errors generated by the kasan checker if any.\nCPU state # To make our attributes model work, we need an additional per-cpu variable that will report whether the said CPU is running allocator code or user code.\nThis will be used as a base to verify the accesses :\na CPU running in user mode accessing a memory block accessible to allocator only will trigger a kasan error (use after free) a CPU running in allocator mode accessing a memory block accessible to user only will trigger a kasan error (use after alloc) Tuning the allocators # The following diagram relies on the accuracy of the kasan attributes.\nThose attributes are not only modified by a direct access like the \u0026lsquo;initialized\u0026rsquo; field.\nIn particular, any allocation or free in both the Regions manager or the secondary allocators must be reflected in the attributes.\nThis required the said memory managers to be modified, so that :\nwhen the region manager allocates pages, all the related bytes go from \u0026rsquo;not accessible\u0026rsquo; to \u0026lsquo;accessible to allocator only\u0026rsquo;. when the region manager allocates pages, all the related bytes go from \u0026lsquo;accessible to allocato only\u0026rsquo;. when the secondary allocators allocates memory to the user, all the related bytes go from \u0026lsquo;accessible to allocator only\u0026rsquo; to \u0026lsquo;accessible to user only\u0026rsquo;. when the secondary allocators frees memory, all the related bytes go from \u0026lsquo;accessible to uswer only\u0026rsquo; to \u0026lsquo;accessible to allocator only\u0026rsquo;. When those operations are executed, we must verify that the related bytes are actually on the expected state.\nThe allocators must also be updated to report their entry / exit in the cpu state.\nCorner cases # The rules stated in the two previous sections are true in the general case. The FSM described is a high level representation of the reality, but there are some corner cases that make the actual implementation look less like an actual FSM.\nThe main corner case here is that allocators themselves (the actual allocator structs), so as CPU states, and some part of the kernel main blocks must be accessible both in allocator and user state.\nThat causes us to need a specific attribute state \u0026ldquo;accessible to anyone\u0026rdquo; and dedicated KASAN entrypoints to report that a specified memory block falls in this category.\nI will not provide a detailed explanation on how to actually handle attributes, as it is very implementation-specific.\nAnother corner case that we have to handle is the state of global variables.\nAs much as we would like to avoid them, global variables are a necessary evil. Take the memory system data structure for example : it must be initialized during the early stages of boot, when we have not yet registered any memory region, thus, when dynamic allocation is not available. In any case, memory allocation goes through it, so we litteraly can\u0026rsquo;t have it dynamically allocated.\nThere are a few cases like this one, where we theoretically could avoid using globals, but where there is no real benefit of doing so.\nOur kernel executable will have the two regular \u0026lsquo;.data\u0026rsquo; and \u0026lsquo;.bss\u0026rsquo; sections in which globals / static variables are located.\nThose variables are initialized as part of the very early bootstrap sequence, by copying the content of the initializer of .data (likely located in flash) directly in .data, and by zeroing-out the content of .bss.\nThose sections must be considered initialized by the kasan, as they indeed are initilized during very early boot.\nThough, those sections are located in RAM, and will ultimately be included in a particular memory region.\nThis will add two tasks to the memory management system when registering a memory region :\nthe regions manager will have to tag all pages in these sections as not allocatable, as those pages are implicitly allocated to the kernel. the kasan will have to report any byte in such section as accessible by anyone, and initialized, as opposed to other bytes whose initial state is accessible by no one. ","date":"27 September 2023","permalink":"briztal.github.io/projects/kasan/kasan_5_kasanmem/","section":"Personal Projects","summary":"How KASAN manages memory.","title":"KASAN memory management."},{"content":"This chapter will describe the basic principles of operation of the KASAN.\nStructure # The previous chapters laid the foundations on which we will build the kasan :\nwe cannot use any transpiling-oriented method to wire our kasan due to potential code size increase. we have an MPU that can trigger a memory fault whenever a specific portion of memory is accessed. the handler of this memory fault can modify the interrupted code\u0026rsquo;s stack context to modify the register values, and to alter its control flow. the memory fault handler gives us the location of the fault, so as the address of the instruction that generated the fault. To implement our kasan, we will first use the MPU to disable access to the whole RAM region.\nThat will cause any access to any RAM to fault, and the memory management fault handler to be executed.\nThe memfault handler will save the part of the context that hasn\u0026rsquo;t been saved by the HW somewhere in memory for the emulator (defined below) to retrieve it later.\nThe memfault handler will then read the PC of the instruction that caused the fault, read the instruction and decode it.\nIt will then check that the related instruction is a memory access. If it is not one, it will just execute the classic memory fault exception.\nIf it is a memory access, it will emulate the instruction and perform its checking in the meantime.\nThis is a complex task and will require a chapter of its own, and this will be the place where the memory checking is done. It involves :\ndecoding the instruction. verifying that every access performed by the instruction is valid. updating the kasan memory metadata to reflect the new state caused by the memory access (ex : write to an uninitialized location causes the location to be treated as initialized in subsequent accesses to that location). emulating the instruction, by performing the underlying accesses : register reads will cause the emulator to read the context-saved values. register writes will cause the emulator to write to the context-saved values. memory reads will cause the emulator to actually perform the read now that the MPU is disabled. memory writes will cause the emulator to actually perform the write if the instruction emulation is successfull, modifying the PC that will be restored when the exception will return, so that the instruction after the one that just was emulated is executed at that time, and not the one that we emulated again. When this is done, it will re-enable the KASAN MPU regions, reload the SW-saved context (possibly updated by the emulator since save), and return from exception.\nThen, the processor will restore the HW saved context (possibly updated by the emulator since save), and give control back to the interrupted thread, but at the instruction after the one that just trapped and got emulated.\nThe following diagram summarizes the high level behavior or the kasan.\nTODO : summarize initialization sequence.\nTODO : summarize validation operation.\nWhitelisting the stacks. # In order for our MPU-based trap sytem to work, we need to add two other MPU regions, more prioritary than the RAM blacklist, to allow access to the user and exception stack (PSP, MSP).\nIf we didn\u0026rsquo;t do this, any code making a memory access in the RAM would cause an exception.\nThe processor, in the handling of this exception, would push the context to the stack in use at that moment (either Main or Process stack), which, if those regions were blacklisted by the MPU, would cause the memfault to be escalated in a hardfault. This is not what we want, as it has different privileges, and less recoverability possibility.\nThis increses to 3 the number of MPU regions necessary to implement our kasan.\nDisabling MPU regions in the memfault handler. # The kasan memory access checker will run in the memfault hanler.\nThis checker will likely access variables not located in the stack.\nThis is a potential problem, as the MPU may still be active when this handler is executed.\nEven if we had the option to disable the MPU in handler mode, this would not be what we would want, as that would prevent the kasan to check accesses in handler mode, under which a large part of the kernel operates (syscall, scheduling, interrupts, etc\u0026hellip;).\nTo handle this situation, we will need to reprogram the MPU when we enter and exit the MemFault handler.\nOn entry, we will disable the regions related to KASAN, and on exit, we will re-enable them.\nKASAN regular entry. # The next sections we will talk about how to manage the kasan memory attributes.\nTo do that, we will need the allocators to call into the kasan when they allocate or free memory.\nIn reality, there are some issues when trying to enter kasan in a function-like manner.\nAs described in the memory access trap section, the basic working principle of our kasan is to blacklist the whole RAM minus the stacks, and have any access to these regions.\nThe issue, is that the kasan is meant to check all accesses made by the kernel, interrupts included.\nAs such, when enering kasan \u0026lsquo;The software way\u0026rsquo;, we must prevent any interrupt to occur.\nThis can be done via disabling interrupts, and re-enabling them on exit. Though, we must be carefull here : interrupts may have been already disabled before entering kasan, so we must not re-enable them in this case.\nEntering kasan \u0026lsquo;The Software Way\u0026rsquo; requires us to be able to disable MPU regions, which requires us to have read / write access to registers of the system control space.\nARMV7M ARM for CortexM3 : User access prevents:\nuse of some instructions such as CPS to set FAULTMASK and PRIMASK access to most registers in System Control Space (SCS). As such, if we plan to have memory management code that runs at User level (some secondary allocator), or if we plan to support user code that reports memory as read-only to debug something, we need to escalate to Privileged mode.\nWe could have a dedicated syscall in the kernel to handle those cases. But then we would have to have two paths, for code that is already privilleged, and for code that is not.\nThere is a more clever way.\nWhat we will do, is to have a dedicated entry function that receives a pointer to a function to execute, and its arguments. This entry function will be located at a fixed address in the executable. This function will manually trigger a read at address 0 on purpose. This will cause a memory management exception to be triggered, and will cause the execution of the generic kasan handler, which will setup the environment for kasan execution. The generic kasan handler will then compare the fault PC to the location where the entry function causes the read at access 0. If the values are equal, then it will treat the memory fault as a kasan entry, and will read the function and its arguments from the saved context, execute the function and return. I If the values are not equal, then it will know that the memory fault was not made on purpose by the entry function, and will proceed with the regular kasan access checking.\nLet\u0026rsquo;s note that this techniques required address at address 0 to trigger a memory fault.\nUnluckily for us, in microcontroler-land, address 0 is often accessible. In my board, this is the start of the TCM-D RAM region.\nWe have two choices :\naccess a known non-mapped address that we know will trigger a fault. use the MPU to blacklist the first 32B starting at address 0. This has the double benefit of causing the program to fault on any access to address 0 (nullptr), which is a typical symptom of a buggy code. If we don\u0026rsquo;t do that, the buggy code will successfully access address 0 and continue until something else breaks. ","date":"27 September 2023","permalink":"briztal.github.io/projects/kasan/kasan_3_blocks/","section":"Personal Projects","summary":"KASAN in details.","title":"KASAN structure."},{"content":"Before elaborating on how the memory checker will actually check memory accesses, we need to elaborate on how memory is managed by the kernel in a microcontroller, or more generally, in a systen without an MMU.\nMemory region # The term region here refers to a memory region and not an MPU region.\nA memory region is a portion of the address space :\nat which memory is mapped. around which memort is not mapped. In particular, if an address range R is a memory region :\nall bytes in this range are accessible and are mapped to memory. the two bytes before and after R are not mapped to memory. A microcontroller has multiple memory regions. For example, the STM32H750 has the following memory regions available : TODO\nRegions manager # One of the most fundamental job of the kernel is to manage memory, that is, to allow itself and its users to allocate and free memory.\nThe precise architecture, and the reason that guide the architecture of such a memory management system will not be discussed here, as it requires a dedicated chapter. I may or may not write about that in the future. Rather, I will focus on the high level blocks and features, with few justifications.\nManaging memory is not easy, particularly when dealing with small memory blocks, as it often requires metadata.\nVarious allocation techniques exist and each have their tradeoff.\nThe memory management\u0026rsquo;s fundamental block is the regions manager. It has to :\nmanage memory regions, as in tracking the allocation state of every byte of every region. allow dynamic allocation of blocks of arbitrary sizes. allow dynamic free of allocated blocks. Page # To do that in a manner that is not too catastrophic for the perf, the regions manager will define arbitrarily the notion of page, as a block of size 2 ^ PAGE_ORDER, aligned on its size, with PAGE_ORDER the order of a page.\nThe region allocator will then manage memory on a per-page basis : rather than keeping track of the allocation status of each and every byte in the system, it will keep track of the status of every page in the system, and make allocations on a per-page basis.\nThe order of the page is arbitrary, but the reader can have the following orders in mind :\nmodern systems : size = 64KiB, order = 16. (or higher). older systems (and windows lol) size = 4KiB,order = 12. embedded systems : size = 1KiB, order = 10. (or lower). The order of a page may be dictated by the hardware, in a system with an MMU, as it must match the translation granule of the said MMU.\nHere, we do not have that constraint. The page order will be selected by the kernel developper depending on the use case, using the following criteria :\na larger memory order will mean less pages in the system, which will mean more fragmentation in the allocators that will use those pages, but less metadata per page, so more a potentially better use of the memory. May be better for specialized applications where large blocks are required. a smaller memory order will mean more pages in the system, while will mean less fragmentation in the allocators that will use those pages, but more metadata per page, so a potentially underuse of the memory. May be better for generic applications that allocate a lot of blocks of different sizes in a non-predictible manner. The way to allocate and free pages is implementation-defined and depends on the use case :\nif allocation of blocks of contiguous pages is required, a buddy allocator can be used. This method only supports allocating block of 2 ^ k pages which proves to be enough in practice. if the system only needs pages and not particularly blocks of pages, a simple linked list can be used. Region per-page metadata # To manage the state of the pages, the regions manager will need per-page metadata.\nThese per-page metadata are a net cost, as in they can\u0026rsquo;t be repurposed when the page is free, or allocated.\nThe regions manager is the fundamental block of the memory allocator. Other allocators are built on top of it. A consequence to that is that it cannot rely on dynamic memory allocation to allocate its metadata.\nThink about it : when the system boots, all allocators are empty. During bootstrap, when the regions manager will register its first memory region, how could it allocate metadata, since calling some form of malloc would ultimately cause its own page allocation functions to be called, yielding a failure, since no memory is yet registered. This is a classical memory chicken and egg problem.\nThe regions manager solves this problem by carving out a block of static (non-reusable, not provided by another allocator) per-page metadata in the actual memory block that will contain the pages. In other words, it will use some of the (theoretical) pages to contain the metadata of the remaining pages. The pages containing metadata will obviously not be available for allocation.\nFor the regions manager, a page will be either :\nnot accessible : the page hasn\u0026rsquo;t been allocated to any user of the regions manager and shoud never be accessed by anyone. accessible : the page has been allocated to a secondary allocator, who may access it, and provide access to portions of it to users. Secondary allocators. # As described in the previous section, the regions manager\u0026rsquo;s job is to manage memory regions.\nIt can only allocate memory on a per-page basis, and cannot allocate smaller blocks.\nThough, it is kind of rare for a user software to need an actual page of memory. A rough estimation is that most allocations require around 1 to 4 cache lines of memory (64B -\u0026gt; 256B) which is way smaller than a page, even in a microcontroller environment.\nThe secondary allocators handle this use case.\nTheir behavior is simple : they act as an interface between the user and the regions manager, by allocating large blocks of memory (blocks of 2 ^ k pages), and by dividing them into smaller blocks in an implementation-defined manner, and by then allocating these blocks to the user.\nThere are many secondary allocators, that each have their own axions and tradeoffs. We can mention :\nslab : supports both allocation and free of blocks of a fixed size. Works by dividing a page into block of the same size. Block state is either stored in a bitmap, or directly in the data of the free blocks, to form a linked list of free blocks. slab array : supports both allocation and free of blocks of a set of sizes. Uses multiple slabs, one for each supported size. stack : supports allocation of blocks of arbitrary size and mass-free only. heap : supports allocation and feee of blocks of arbitrary size, allocating blocks of different sizes in the same pages. Inefficient. Any memory block that the allocators manage is provided by the regions manager. As such, such block is always accessible.\nFrom the allocator\u0026rsquo;s perspective, a block of memory is :\nfree : the block is not accessible by any user. It can be used by the allocator to store metadata. allocated : the block is accessible to all users. It should never be accessed by the allocator. Summary # The following diagram summarizes the architecture of the memory system.\nTODO.\n","date":"27 September 2023","permalink":"briztal.github.io/projects/kasan/kasan_4_kernmem/","section":"Personal Projects","summary":"How the kernel manages primary memory.","title":"Kernel memory management."},{"content":"This article will give a theoretical base on memory checking.\nA memory checker is a system that checks a subset of the memory operation made by a program.\nTerminology : # Memory operations : Direct memory accesses and allocation/free operations. Memory access : read or write to a memory location. Memory location : location in CPU memory accessible by address.\n-\u0026gt; A register is not a memory location. -\u0026gt; Accessing a register is not a memory access.\nHere the \u0026lsquo;subset\u0026rsquo; can include all memory operation or an actual subset. We will elaborate more on that in a few moments.\nDisclaimer : I have no prior experience with the internals of existing memory checkers like valgrind. The rough explanation that I\u0026rsquo;m giving here may be partially / completely false. It just reflects my high level understanding. This should have no impact on the validiti of the further sections.\nChecking a memory operation means intercepting that memory operation before it occurs in \u0026lsquo;some way\u0026rsquo;, then verifying that the said memory operation is legitimate, then if so, allowing the operation to complete.\nLegitimate memory operation : an operation that philosophically makes sense, given the paradigms of the program. Examples :\nread to an initialized local variable. write to a writeable local variable. read to a memory region allocated by the program, not freed since allocation. Illegitimate memory operation : an operation that philosophically doesn\u0026rsquo;t make sense, given the paradigms of the program. Examples :\ndouble free. read of uninitialized memory. use by the user after free. double allocation (by the allocator). use by the allocator after the allocation. write to non-writeable memory. write to a memory location that philosophically doesn\u0026rsquo;t belong to the part of the program that does the access (mem corruption ?) The strategy to check a memory operation depends on the type of the operation.\nFor allocation/free operations, there are usually fixed entrypoints for those operations (C : malloc/free et.al. ). The memory checker just has to override these entrypoints so that its own implementations of (ex) malloc or free are called. Then it has the opportunity to verify the validity of operations, and call (potentially) call the real malloc / free if they are legit (or just manage memory itself, it doesn\u0026rsquo;t matter).\nMemory accesses are trickier to check. Those accesses are not made using a function code, they are made using actual assembly operations, and there is no easy way to just \u0026lsquo;override\u0026rsquo; those instructions.\nChecking memory accesses : transpiling and why we cannot. # A potential strategy is to transpile the program : the memory checker will disassemble the assembly code and replace all memory access operations with assembly stubs to instead call the memory checker\u0026rsquo;s entrypoints, or just do the checking in place. In all cases :\nthis is a very tricky operation, as for example, the jump addresses / offsets have to be carefully updated. this has a significant impact on : the code size, as a single memory access assembly operation is converted to a sequence of assembly operations. the code performance, as every access will have to be checked. This strategy could theoretically work for a microcontroller code, but we have to consider the impact on code size very seriously.\nComputers have a large virtual address space and kernel-powered swapping capabilities that allows enlarging the size of the executable way beyond what we could ever need. Taking a 10x hit on the executable size won\u0026rsquo;t affect the possibility of executing the code.\nMicrocontrollers are not in this situation.\nMicrocontrollers have flash that range from a few kilobytes to a few megabytes.\nIn my case, my code was already taking 63% of the chip\u0026rsquo;s flash (128KiB large).\nEven taking a 2x hit wasn\u0026rsquo;t acceptable in my case, and the transpiling method only gives a worst case guarantee of Nx (N being the number of instructions required to emulate a single memory access instruction, N \u0026gt;= 2).\nThis strategy was not applicable in our case.\nTo implement a KASAN on such device, we need to have a solution that is taking advantage of the processor hardware.\nSubset and limitations. # In the introduction I mentioned that the memory checker was checking a subset of the memory operations.\nIdeally, we would like our memory checker to check all memory operations.\nThough, depending on the implementation of the memory checker, some operations will be more difficult to handle than others.\nA trivial example is memory access operations vs allocation / free operations. It is very easy for a debugger to hook up to memory allocation primitives like malloc and free and to verify the validity of those operations, since those (with their realloc* counterparts) support the entire lifecycle memory (you\u0026rsquo;re not supposed to free a block not acquired via free and freeing a block means that you shouldn\u0026rsquo;t free it again).\nThough, as we said earlier, checking actual memory accesses is non-trivial as involves directly working with the assembly, either directly as described in the transpiling section, or indirectly, as it will be described later for our implementation.\nAnother example showing the variation in complexity between check operation arises when we think of local variables vs allocated variables.\nLet\u0026rsquo;s assume in this example that regisers and compiler optimization do not exist and that when we access any type of variable, a memory access instruction is generated by the compiler.\nWhen compiling this fuction :\nvoid f(void) { volatile uint64_t i = 0; } the comiler will detect that a local variable (@i) is used, and needs a location. It will place this variable in the execution stack allocated to @f, and will generate :\na function prologue to reserve space on the stack. a memory access to store 0 at the location of @i in the stack. a function epilogue to restore the stack to the state where it was before the execution of the prologue. This hardly requires subtracting 8 to the stack pointer, storing 0 at the new top of stack and adding 8 to the stack pointer.\nNow, let\u0026rsquo;s note that the actual address of i (that \u0026amp;i evaluates to) is a stack address, and will become invalid as soon as the function returns. After that, it will potentially be allocated to another stack variable, or never used again, depending on the executed code.\nDue to this, if we want to check accesses to local variables (to detect a read from an uninitialized location), we also need to modify the function prologue and epilogue to report stack adjustments to the memory checker.\nNow, when compiling this function :\nvoid f(void) { uint64_t *ptr = malloc(8); *ptr = 0; free(ptr); } the compiler will still allocate space on the stack for a local variable, but this time this local variable will be a pointer that will contain the address of the memory block to write 0 to.\nIn this case, it will be a lot easier for the memory checker to verify accesses to *ptr (not ptr), since this memory block has a clearly defined life cycle : its life starts after allocation by @malloc, after which it can be considered uninitilized, and ends before free by @free.\nThe conclusion to this point is that it is more difficult to check accesses to local variables than it is to check accesses to dynamically allocated variables.\nLuckily for us, the compilers nowadays feature decent function-local analysers that will warn against such usages at compile time. Added to that, the main bugs that we statistically encounter in real life are due to dynamically allocated memory. Checking local variable is a nice to have feature, but not a must have feature.\n","date":"27 September 2023","permalink":"briztal.github.io/projects/kasan/kasan_1_memcheck/","section":"Personal Projects","summary":"General memory checking considerations.","title":"Memory checking."},{"content":"","date":"27 September 2023","permalink":"briztal.github.io/projects/","section":"Personal Projects","summary":"","title":"Personal Projects"},{"content":"","date":"27 September 2023","permalink":"briztal.github.io/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/c/","section":"Tags","summary":"","title":"C"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/categories/c/","section":"Categories","summary":"","title":"C"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/emscripten/","section":"Tags","summary":"","title":"Emscripten"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/gnu/","section":"Tags","summary":"","title":"gnu"},{"content":"In the previous part, I introduced the base components of what I consider to be a minimalistic kernel, and spent some time defining static modules, and why their correct initialization raises some issues.\nIn this chapter, I will be going into more details on WebAssembly and describe the issues I faced when porting my kernel to this environment.\nWebAssembly C Toolchain # Though the work of porting a whole codebase to a browser environment may seem like a hellish amount of work, the Emscripten toolchain greatly eases the workload, by providing a C environment resembling (relatively speaking…) to the posix environment.\nThe C compiler used for emscripten is LLVM (abstracted by the emcc program), which helps keep the C developer in a world he is familiar with.\nBut the pleasure of enjoying compatibility stops here, due to the fact that WebAssembly chose to reimplement its own object format, instead of using a familiar standard for its executable format like ELF (Executable and Linkable Format).\nNow, I am not claiming the authority to judge whether this was a bad idea, nor question their objectives, and I am certain that they had their reasons for making such a choice. I could, for example, imagine them desiring to get rid of the executable side of the ELF format (program headers etc…) to avoid wasteful memory consumption.\nIn the end, the resulting format shares many similar features with the linkable side of the ELF format. But as we will observe in the next sections, they also chose not to support some of the ELF formats useful features that a C developer may be expecting.\nWebAssembly Object format # A quick look at the official documentation shows that the WebAssembly object format (the official documentation uses the term “module” but I will rather use the term “object” to avoid any ambiguity with the concept of module defined in the previous chapter) is similar to what a C developer expects of a linkable format, namely: the definition of sections referring to binary data. Each section having a attributes that define the format and content of this binary data :\n“ The binary encoding of modules is organized into sections. Most sections correspond to one component of a module record, except that function definitions are split into two sections, separating their type declarations in the function section from their bodies in the code section.”\nsource : https://webassembly.github.io/spec/core/binary/modules.html\nThe format defines standard section types, similarly to the ELF format, and assigns a particular meaning to them, such as code, data, import, etc…\nThey even provide support for ,what they referred to as custom sections, which may lead one to think that the feature evocated in the previous chapter (defining a custom section where a variable should be placed) may be supportable.\nBut all our hopes crumble, after observing that the compilation of the following code doesn’t result in the definition of any custom section :\n#include \u0026lt;stdio.h\u0026gt; __attribute__((section(“.xxx”))) const char *str = “MYSTRING”; int main() {printf(str);} Rather, the ‘str’ variable will be placed in the data section, meaning that the __attribute__((section(“xxx”))) is basically ignored :\nwasm-objdump -x main.wasm main.wasm: file format wasm 0x1 Section Details: Type[22]: … Import[3]: … Function[51]: … Table[1]: … Memory[1]: … Global[3]: … Export[13]: … Elem[1]: … Code[51]: … Data[3]: - segment[0] memory=0 size=565 — init i32=1024 - 0000400: 4d59 5354 5249 4e47 0000 0000 3806 0000 MYSTRING….8… - 0000410: 2d2b 2020 2030 5830 7800 286e 756c 6c29 -+ 0X0x.(null) For reference, the same file compiled with clang (C frontend for llvm) produces the expected result of placing the MYSTRING data in the custom xxx section. From this we can deduce that there isn’t any issue with the code itself but rather that the problem lies in the lack of support for this kind of operation in the emscripten toolchain.\nThis lack of support is in itself, not an issue for standard-C-compliant libraries. As the notion of sections resides outside of the C standard’s jurisdiction, and the __attribute__ keyword is an extension that just so happens to be supported by both gcc and clang. But, for any type of software that relies on the use of sections, WebAssembly will be incompatible.\nThough I may seem critical in my conclusion, I perfectly understand the reasons for this lack of support and their validity.\nIndeed the support for programmer-defined custom sections presupposes the existence of a more complex tool for defining the behaviour of the linker when merging multiple files containing custom sections.\nIn order to fill this need, standard toolchains rely on the provision by the programmer of linker scripts (.ld files), that define these behaviours.\nSupporting linker script represents a hellish workload, as such I perfectly understand why WebAssembly designers chose not to invest time on it, after all the usage of programmer-defined custom sections is actually very infrequent.\nMore details on the gnu linker scripts syntax : Command Language\nConclusion # This article served two purposes.\nFirst, introducing the requirements that may lead a developer to implementing or using a kernel, and describing the base software blocks that compose a minimalistic kernel.\nThen, showing that the concept of heterogeneous environments (bare-metal, OS-hosted, Web-browser-hosted) is not simply a concept but a reality, that implies an heterogeneity of toolchains, which force the developer aiming for portability to strictly rely on common toolchain capabilities, and that the lack of support for a toolchain feature may directly prevent particular programs to be ported to the considered environment.\nIn my example, the lack of support for user-defined per-variable custom sections forced me to rewrite parts of my kernel that depended on this functionality.\n","date":"8 October 2021","permalink":"briztal.github.io/projects/kernel_to_wasm_p2/","section":"Personal Projects","summary":"WebAssembly, or why the toolchain matters.","title":"Joys of porting a kernel to WebAssembly, Part 2"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/categories/kernel/","section":"Categories","summary":"","title":"kernel"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/kernel/","section":"Tags","summary":"","title":"Kernel"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/wasm/","section":"Tags","summary":"","title":"wasm"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/categories/wasm/","section":"Categories","summary":"","title":"wasm"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/web-development/","section":"Tags","summary":"","title":"Web Development"},{"content":"","date":"8 October 2021","permalink":"briztal.github.io/tags/webassembly/","section":"Tags","summary":"","title":"Webassembly"},{"content":" The back story # For as long as I can remember, understanding how kernels were designed and written was something that has interested me.\nIn the beginning, a newcomer to the world of programming, I saw them as mysterious, incredibly complex and barely comprehensible software blocks, handling equally mysterious tasks in the execution of a program.\nA few years later, after having perfected my skills as a programmer, I dived into the concept with a new perspective, and discovered the real issues that required the use of kernels and the possible implementations choices that solved these problems.\nAfterwards, I did what every normal programmer would do in my situation and spent more than the next half decade implementing my own one.\nLast year, I discovered WebAssembly, an Intermediate Representation allowing the deployment of C software in a web-browser. In order to make this port possible I had to update a part of my code base to support WebAssembly, and it was within that context that I set upon myself the challenge of :\nporting the kernel to the wasm environment. WebAssembly and its related toolchain, Emscripten, have done a very respectable work in providing support for C99 with gnu extensions (ex : constructs like ({statements})), allowing regular C libraries to be ported without any issue.\nYet, I encountered my fair share of interesting issues when porting my kernel to this environment. After all, kernel’s are quite far from the simple standard-compliant C code, and as such forced me to push to the boundaries of the emscripten wasm toolchain and get familiar with it’s nooks and crannies.\nThis article’s aim is to describe these difficulties in more detail and hopes to, in the process, give you glimpses of how kernel’s on one hand and the emscripten toolchain, on the other, work under the hood.\nI will start this article by briefly introducing Webassembly, and I will then tell you more about the main needs that pushed me to implement my own kernel, and in doing so, I will give a short description of its main blocks. Following this, I will provide a more detailed look at the Emscripten toolchain, to expose a corner case that forced me to reimplement a part of the kernel’s init procedure to support WebAssembly.\nWeb-browsers and Javascript # Historically designed to render statically defined text-based pages, web-browsers have drastically evolved to the point we know, being nowadays having more in common with actual Operating Systems than simple page renderers.\nWeb-browsers getting more and more complex and varying in their implementations, and web-designers desiring to avoid as much as possible taking the execution browser brand (firefox, chrome, …) into account during development, it was central to provide a stable common API. Javascript rapidly took this place, the Javascript VM now being one of the central components of a web-browser.\nJavascript being an interpreted complex language, its execution performances rapidly showed their limits, even with JIT features enabled. In a world where Moore’s law applies, this may not seem like a real issue, as the assumption that the next generation of computer will provide enough performance increase to compensate for this slowness. Though, in our years where this law clearly started to present its validity limits, this issue had to be addressed.\nWebAssembly # Webassembly is a relatively new (2017) trending IR (Intermediate Representation) code format designed to be loaded and interpreted, translated or JITed in, among others, web-browsers.\nWasm logo An intermediate representation is, as the name states, a manner to represent a program that doesn’t aim to be directly executed, but rather :\nto serve as storage or exchange format.\nto be interpreted efficiently by a wide range of program processors (compilers, VMs etc…), to be either translated into other formats (ex : actual assembly) or directly executed.\nWebAssembly is, as a consequence, not a programming language (though the textual version may be used this way), but rather an object format that compilers may use to store the result of their compilation, and that a virtual machine may use as executable source.\nThis allows a possible wide range of programming languages to be compiled into it, for example C or Rust for the most widely known.\nThough its reputation may be tainted due to its strong use by web hackers to inject (among others) crypto-mining payloads in client side, webassembly remains a good option for porting C libraries in the Web world.\nKernels, or the need for environment abstraction layers # As any software engineer I aim to write “good” code. Naturally, pretty fast, the problem of defining what “good code” is arises. After all, given a choice between multiple possible implementations, it would be best to have some criterion.\nThough I recognize that these criterion are not universal, in my implementations at least I now consider 4 performance indicators that I have listed them below in decreasing importance :\nportability : the code must be compatible with the widest scope of environments.\nfunctionalities : the code must offer an answer to the most general problem.\nmemory consumption : the code must use the smallest amount of memory, considering the previous rules.\nexecution time : the code must run as fast as possible, considering the previous rules.\nI wish to once more remind the reader that these criterion are completely subjective, and are only the reflection of my experience and my objectives.\nIt is certain that real-time software engineers would have radically different guidelines, and would, for example, place execution time invariability, or avoidance of linked lists traversals inside critical sections, on top of their list.\nBut let us go back to my criterion.\nGenerally, I tend to place code reusability across use-cases and environments among every other requirement.\nWhen I say environment, I mean the platforms (x8664, x8632, arm32, aarch64), host OS (freestanding, linux, windows, mbedOS) and toolchain (gcc, llvm).\nThis means that the code I write must be as independent as possible of the host API its executable version will access.\nHaving this constraint implies defining standard custom APIs for the most basic host requirements of a program :\nhost memory access : ex for stdlib : malloc, free.\nhost memory mapping : ex for linux : mmap, mprotect, munmap.\nhost file access : ex : printf, for linux : open, close, read, write, seek.\nhost directory access : ex for linux : readdir, closedir, seekdir.\nthreading : ex for linux : pthread environment.\nThe need for freestanding implementation # To achieve the aforementioned code reusability objective, in a classic hosted environment defining a simple function indirections (static inline functions or macro aliases in a shared header) will do the job.\nThat being said, as an embedded software engineer, my primary environment targets are freestanding (without OS, bare-metal) systems, not hosted ones, which complicates the problem.\nThough, it is still possible to use frameworks for those freestanding systems, that would provide some semblance of a standard environment. The Arduino framework, being one example, offers a default standard for low-performance embedded systems. In this case the previous simple solution would still be viable.\nBut, after a more thorough examination and more experience with these frameworks I have started to have several objections regarding their use :\ncode quality : a simple look in an Arduino framework implementation is enough to leave one, at best doubtful, at worst frightened by the code quality and the apparent absence of debugging efforts.\ncode performance : for example, the quality of the memory management system may be weak, which could lead to memory allocation poor performances, which could impact the whole program.\nmissing features : platforms like the teensy35 (a platform nevertheless close to my heart as it was my entry point into embedded systems and my hardware of choice for a few years) is based on a cortex-m4f based on the ARMV7M architecture. As such, it features all of the architectural components required to implement a context-switch based mono-core threading API. Yet, the Arduino framework is primarily made to provide an uniform API for extremely low-power cores like AVR8 chips. As such it doesn’t provide a satisfying threading API, and so, if an implementation is required, it must be implemented manually.\nlanguage constraints : the Arduino framework is written in C++, which doesn’t ease the development of pure-C applications.\nAs a consequence, for freestanding systems, it becomes a necessity to have an available working implementation of the basic code blocks to support at least :\nmemory allocation.\nmemory mapping (if any).\nfile access.\nthreading.\nAnd those blocks happen to be the minimal blocks needed for a kernel.\n#The need for code compatibility between freestanding and hosted environments\nHosted and freestanding environments are different in many ways, but, most probably the most painful difference between the two for any programmer is the difference in debugging capabilities.\nOn the one hand, debugging on a hosted environment (ex : linux process) is very easy and fast. Tools like valgrind are extremely versatile, easy to use and can allow the developer to have an exhaustive list of all invalid accesses, memory leaks, uninitialized accesses, without requiring any additional instrumentation. Program faults are handled by the host and faults’ side-effects are reduced, as the host will close any used file, resource, etc…\nOn the other hand debugging on a freestanding environment is much more of a headache and may require a particularly expensive instrumentation (the most well-known toolkit being the Segger’s J-Link) to set up a gdb server, in the best case.\nAn invaluable tool like valgrind being a simulation-based debug tool, it only supports its host’s instruction set, and as such can’t be adapted to this environment. As such our debugging capabilities are greatly reduced.\nMemory leaks will be harder to detect, uninitialized accessed will be untraceable.\nProgram faults may be fatal, if not handled properly, as they may put the program in an unsafe state. Worst, if the related program is a controller for physical objects, may lead to erratic behaviours with repercussions in the physical world, which may be inconvenient if not harmful.\nAs a consequence, we have two complementary environments, one that depends on a safe and verified code but can’t provide ways to verify it, and another that provides such ways, but for which the consequences of not having a dependable and verified code are much lower.\nAs such we can make up for the shortcoming of the freestanding environment by making the code as much platform-independent as possible. This allows for a rigorous stress-testing and debugging of shared code via standardised, powerful and robust tools in the hosted environment, and safe deployment of this verified code on freestanding platforms.\nA simple user space kernel # Though the definition of what is and what is not a kernel may be a subject of debate, I believe the best definition to start with is the following :\na kernel manages shared resources in a system.\nThose resources may be :\ncores.\nTasks (and threads and processes).\nExecutables, the kernel executable at first, then possible dynamic libraries and modules.\nMemory.\nPeripherals.\nFiles.\nMore generally, a kernel aims to manage any shared resources accessible by multiple programs in a system. The role of the kernel is to supervise the access to those resources, by providing a consistent API to use them.\nContrary to what is most common for a kernel, my kernel will run in user-space.\nNormally kernels do not do this and isolate its threads from itself via the virtualisation / protection mechanisms offered by MMUs or MPUs. My kernel doesn’t include this feature as protection mechanisms from the MMUs and MPUs may be absent on the targeted systems. This can occur when the kernel is run in the process space on a hosted system (no MMU, may be emulated via linux-KVM if relevant but that’s another story), or on a cortex-m4f (implementation-defined MPU capabilities but no MMU).\nThat aside, the kernel implements most of the features that you would expect to encounter :\nBSP : environment dependent code, provides a standardised API to interact with cores, host memory managers, host file manager, etc… Memory manager : responsible for managing primary memory and to provide secondary memory APIs to cores. See my detailed article on this topic. Scheduler : decentralised system responsible for scheduling tasks on each core, and to coordinate the transfer of tasks between cores depending on each core’s load. Introduces the concept of process, i.e. threads accessing the same set of resources. Loader : provides static module initialization capabilities and dynamic modules / libraries loading / unloading capabilities. File system : tracks resources processes can use, manages their access. Resource tracker : tracks resources used by each process, and frees them automatically when the process terminates. Modules # The previous section introduced a particular block of the kernel, the loader, that was in the centre of one of the issues I encountered, the Loader.\nIt is not my intention to go into details regarding the internals of the loader, as this would be long enough for an entire article. Yet, as having a clear comprehension of the concept of modules is essential for understanding the following sections, we will introduce it.\nWe can divide the software that will be executed at runtime in two categories :\nkernel : contains all blocks previously mentioned, i.e. blocks required to manage resources, and to run user code. Kernel code must be linked statically to form the kernel executable. modules : code that is not strictly required in the kernel executable, and so, that can be dynamically loaded. For example, the file system is a component of the kernel, but the code to support a particular type of file manager (ext2, ext4, ntfs) can be provided in a module.\nNow that this nuance is introduced, we next need to introduce the concept of static modules and introduce their use case.\nOn certain platforms, the RAM memory sections are not executable, this implies that Writable memory can’t be executed, and so, such dynamic loading is impossible, as dynamically loading a module means writing it to memory from an external source, and modifying it (ex : applying relocations) such that it can access kernel symbols (functions and variables).\nIn these platforms, dynamic loading will be impossible, and all required modules should be linked statically to the kernel executable.\nOn other environments, like WebAssembly, the executable format is atypical, and dynamic loading may be difficult. Even if a loader implementation for such a format may be possible, it will require a lot more development effort than what I was willing to invest. In such a situation statically linked modules there again seem like a valid option, even if only as a temporary solution.\nAnother benefit of linking modules statically to the kernel executable is time, dynamic loading being a complex problem with complex and time-consuming solutions. Doing the linking work at compile time avoids the need to do it at each execution.\nA module being an implementation-defined executable aiming to have a user-defined impact on the kernel (for example, to start processes or to provide support for file system types), it must comprise a function accomplishing the aforementioned tasks, the module initialization function, that the loader can find and execute .\nThis execution will be done at different times, depending on the module type :\nFor static modules, all init functions of static modules must be retrieved at once and executed sequentially, possibly in a particular order. For dynamic modules, the init function must be executed as soon as the module is loaded in the kernel. Again, what will be of interest here will be the case of static modules.\nAt a high level the build procedure of static modules in our kernel works as the following :\nSource files of the same module are compiled independently. Source files of the same module are merged to form a single relocatable executable, the module executable. Module executables are modified, by possibly forcing symbols locality to prevent multiple symbols definitions. Module executables are merged to form a single relocatable executable, the static modules executable. During this process multiple static modules executables will be generated, each one having its own initialization function. Later, they will be merged into a single relocatable executable, and the position of these functions in the resulting executable may not be known.\nNot knowing the position in the executable of each module’s initialization function will be an issue as, at runtime, the kernel will have to execute each one of these functions, but the kernel being independent of modules, it has no knowledge a priori of the location of these functions.\nThere are two strategies one can use to help overcome this hurdle.\nThe first, and most common solution involved using the concept of sections, supported by the elf format. Practically this involves defining for each module a static function pointer that contains the location of the module’s initialization function, and placing this variable in a custom section of the efl. We shall be calling this custom elf section the .modinit.\nLater in the build process, during the merging of multiple static modules executables, the linker (ld) will merge the content of sections with the same name (default behaviour, but can be configured with the use of a linker script), in our case this means merging into one section the multiple static modules .modinit content. This will cause all our pointers to be located in the same output section, which in the resulting executable, will cause them to be located contiguously in memory.\nA wisely crafted linker script provided to the linker will define two global variables, .modinit_start and .modinit_end, used to help the kernel locate the start and end of this section, which will allow the kernel to retrieve addresses of all modules initialization functions needed to execute them.\nThis aforementioned process is the one that regular executable processes use to trigger the\ninitialization and de-initialization of static variables whose expressions need to be computed at run time.\nFor more details : ELF: From The Programmer\u0026rsquo;s Perspective\nThe second solution, which is in my humble opinion much less elegant and relies on knowing at compile-time the full list of static modules that need to be compiled and linked, consists in providing a unique name to each initialization function (derived from the module name, modules names must be unique), and to assign a global visibility to this function, then to define a single function calling each initialization function by name. The kernel then only needs to execute this function to initialize all modules.\nThough being un-clever, this procedure can be done automatically using a bash script.\nIn my pre-WebAssembly implementation, I used the first solution. This was only possible because I had easy access to the standard elf format as I was developing for the linux environment. But, as we will now observe, WebAssembly’s constraints on the elf format forced me to review my choice and opt for the second solution.\nIn-process kernels, WebAssembly and their difficulties to cohabitate.\nThe second solution, which is in my humble opinion much less elegant and relies on knowing at compile-time the full list of static modules that need to be compiled and linked, consists in providing a unique name to each initialization function (derived from the module name, modules names must be unique), and to assign a global visibility to this function, then to define a single function calling each initialization function by name. The kernel then only needs to execute this function to initialize all modules.\nThough being un-clever, this procedure can be done automatically using a bash script.\nIn my pre-WebAssembly implementation, I used the first solution. This was only possible because I had easy access to the standard elf format as I was developing for the linux environment. But, as we will now observe, WebAssembly’s constraints on the elf format forced me to review my choice and opt for the second solution.\nIn-process kernels, WebAssembly and their difficulties to cohabitate.\n","date":"7 October 2021","permalink":"briztal.github.io/projects/kernel_to_wasm_p1/","section":"Personal Projects","summary":"In-process kernels, WebAssembly and their difficulties to cohabitate.","title":"Joys of porting a kernel to WebAssembly, Part 1"},{"content":"","date":"1 January 0001","permalink":"briztal.github.io/authors/","section":"Authors","summary":"","title":"Authors"}]