[{"content":"","date":"19 June 2025","externalUrl":null,"permalink":"briztal.github.io/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"19 June 2025","externalUrl":null,"permalink":"briztal.github.io/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"19 June 2025","externalUrl":null,"permalink":"briztal.github.io/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"19 June 2025","externalUrl":null,"permalink":"briztal.github.io/series/trading-bot/","section":"Series","summary":"","title":"Trading bot","type":"series"},{"content":"","date":"19 June 2025","externalUrl":null,"permalink":"briztal.github.io/categories/trading-bot/","section":"Categories","summary":"","title":"Trading bot","type":"categories"},{"content":"This chapter describes the design choices made when implementing the data provision system and justify them by covering in a simplified, but functional way, how disk access, memory translation, kernel buffer management and memory mapping work.\nParticipants # The data provider system features the following participants :\nmultiple remote providers, each providing data for a specific set of instruments in a specific time range. It is assumed that those remote providers provide the exact same information and that when multiple providers can provide the same data elements, any can be chosen. multiple data consumers, which want to use data provided by remote providers. Consumers can be strategies, or simulation brokers. Consumer needs # The data consumers need to do the two following types of request :\nrange request : takes a start minute index (stt) and an end minute index (end) and for the time interval [stt, end[, generate : a volumes array with one volume per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0. a values array with one value per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0. value request : takes tgt a target minute index (tgt), and determines mid the greatest minute index inferior or equal to tgt with a non-zero volume, then returns : mid. vol the volume at mid. val the value at mid. In practice, the range request is used when we want to cache data, or compute statistics like moving averages, or correlations, and the value request is used to find the most recent volume and price at a given moment in time.\nRemote provider format # Remote providers like polygonio provide rest-ful APIs to fetch data. Those APIs do likely transfer data in plain text formats like JSon, that the local provider has to parse and convert into floats for the rest of the system to use them.\nRest APIs are thread-safe by design, so there is no need to coordinate the possibly multiple active trading cores to fetch data.\nConstraints # First, as stated in the previous chapters, it is essential to avoid downloading anything from remote providers if it is not strictly necessary. That gives us our first two constraints :\ndata downloaded from the provider must be stored to disk and used in place of the remote provider when possible. local providers running on the same machine must collaboratively use the same storage. Then, as stated in the \u0026lsquo;Consumer needs\u0026rsquo; section, data is ultimately to be either read by value or stored in contiguous memory arrays. This gives us our third constraint :\ndisk storage must be efficient for both value and range read. In practice, this means that our disk storage stores data in contiguous arrays which follows the same padding rules as the arrays generated by range requests : if at a given minute ID there have been no transactions, then both volume and value array elements are 0.\nMy proposal for the design answering those three constraints uses the file system and its memory mapping capabilities as storage medium. To understand why, let\u0026rsquo;s cover how modern operating systems manage disk storage and file memory mapping in a simplified way.\nArchitectural and kernel concepts # The content of this section originally covered the following things :\nhow modern OSes manage storage devices like SATA disks. how the MMU works and how VA -\u0026gt; PA translation is done. what happens when you call mmap. After writing it, I decided to move it in a dedicated article that can be found here, as it may be interesting beyond the scope of the trading bot.\nPlease take the time to read it to get familiar with the concepts that the current design choice of the provider is based on.\nFile storage design # File structure, values, volumes # Data downloaded from the remote provider is stored in the file system, and mmap is used to efficiently access it.\nSince mmap prevents us to resize a file once mapped in memory :\nwe need to store data in fixed-sized files. we need to extend the files to their largest possible size at creation before the first mmap is done. My decision was to use a dedicated file per year per instrument. One could use a different time base but one ultimately has to pick one.\nEach file contains data starting at Jan 1st 00:00:00 for the file\u0026rsquo;s year, and contains all zero-padded data until the end of the year, one data element per minute :\nif at a given minute no transaction happened, the data element is : volume = 0; value = 0; if at a given minute at least one transaction happened, the data element is : volume = sum of all transactions volumes; value = average of all transaction values weighted by their volumes; Data is written in time order, starting at Jan 1st 00:00:00. An element at mid A cannot be written before all elements with mids B \u0026lt; A have been written. If an instrument has no data in a time range, volumes and values for this range are set to 0.\nEach file is thus be divided into the following sections, which are be independently mmap-ed by every provider needing to access the data that it contains :\ndescriptor : identifies the file using the instrument name and the year. sync : data used by multiple providers using this file to synchronize their writes. volumes : the array of all volumes of transactions for each minute of this year. Expressed with f64s. values : the array of all values of transactions for each minute of this year. Expressed with f64s. prv_ids : another array containing an array index for each minute of this year. Expressed with u32s. Has a special purpose which is not needed to understand the structure of the provider. Described in a dedicated section at the end of the article if the reader is curious. Mapping attributes # As stated above, each file section is mmap-ed by every provider who needs to use its data. Let\u0026rsquo;s recap how it is mapped and accessed.\ndescriptor : only written by the provider that creates the file. Mappable as read-only by every other provider. Could be mapped as private once we\u0026rsquo;re sure that the provider that created the file has initialized the descriptor and called msync. sync : is actively read and written using atomics by the different providers who use the file\u0026rsquo;s data. Needs to be mapped as shareable read-write by everyone. volumes, values, prv_ids : is written in order, which causes those sections to be incrementally written. Only one page at a time is written and all others could be read-only. In practice to avoid the cost of TLB updates, all pages are writeable. All pages \u0026gt;= the currently written pages must be shared to observe writes from other providers. All pages \u0026lt; the currently written page could be remapped as private but this brings no benefit in practice since no one is supposed to write to those (or it is a bug). In practice, all those pages are mapped as shareable read-write. Sync # Since providers may use the same data at the same time, they may also need the same non-downloaded data at the same time.\nThe shared section contains data that providers may need to read and write to coordinate themselves in accessing the file\u0026rsquo;s data. In practice it contains :\nu8 lck : a lock which protects the access to the two fields below. u8 wrt : a flag reporting that someone is actively downloading writing data. u64 siz : the number of elements that have been downloaded and written and can be freely read by anyone. One could say that since we assumed that all remote providers provide identical data, all local providers could then just all download data independently and write them to the file sections that they all mapped as shared.\nThis would work in practice but :\nis dirty (personal opinion). would lead to unnecessary cache coherency latency for writes to the same locations. Those writes would ultimately write the same data, but the cache coherency system would probably not be aware of that and the perf hit could be noticeable. we would still need to synchronize the siz field of the sync data to ensure that it is monotonically increasing. Otherwise, a provider A could write a lot of data and increase it and another provider B, who started to write at the same time but downloaded a smaller segment (but in a slower manner) could complete after, and update siz to a lesser value. This would force other providers to re-download and re-write data already written by A. Hence, we need the sync section.\nNow, let\u0026rsquo;s describe a write sequence example that shows how multiple provides collaborate to download the same data and how they all end up seeing what other write :\nThe storage file only contains data for [Jan 1st 00:00:00, Jan 3rd 00:00:00[ (\u0026lt;= note the open bracket, we have no data for the end time). Providers A and B want to read data for [Jan 10th 00:00:00, Jan 20th 00:00:00[ They both attempt to lock lck using atomics. The cache coherency system ensures that exactly one acquires the lock. Provider A succeeds. Provider B is blocked until A releases the lock. Provider A reads siz and sees that the data that it wants is not here. It either has to download it or wait for another provider to do so. Provider A sees that wrt is clear : no one is actively downloading data so provider A must do it itself. Provider A sets wrt and releases lck. Provider B is unblocked, acquires lck. Provider B reads siz and sees that the data that it wants is not here. It either has to download it or wait for another provider to do so. Provider B sees that wrt is set : someone is actively downloading data so provider B must wait for it to complete. Provider B releases lck and goes to sleep for some time. It will periodically re-lock lck and check siz and wrt again to check if data is being downloaded or if it needs to do it itself. Provider A downloads data, processes it and writes it to its locally mmap-ed sections. Updates are propagated to B by the cache coherency system. Provider A attempts and succeeds in acquiring lck. It then clears wrt, updates siz to reflect its writes, and then releases lck. Provider A then proceeds to read the data range that it was originally interested in. Provider B wakes up and acquires lck. It then checks siz and notices that the data that it wants is now here. It releases lck. Provider b then proceeds to read the data range that it was originally interested in. Top level design # Let\u0026rsquo;s take a look at what a system with two bots attempting to process years 2020 and 2021 of NVDA would look like.\nStructure of the data provider. On the top, we see the remote provider, used by bots to download data.\nIn the middle, we see our two bots. Each ultimately wants to create a buffer containing all volumes and values data for NVDA [2020, 2021].\nBoth have their own local provider, and they have opened the storage files containing NVDA data for 2020 and 2021.\nOn the bottom, we see a representation of those two files and of their internal state :\nprv/nvda.2020.stg is complete : all its data has been downloaded, and hence, both bots have been able to extract data from it and to populate half of their destination buffers. prv/nvda.2021.stg is incomplete : half of its data is missing, and both bots 0 and 1 need its missing data to finish populating their buffers. Bot 0 has attempted to acquire write privileges but has failed to do so, as Bot 1 was quicker to acquire it. It is now repeatedly sleeping and checking that the write is complete and that all data that it requires has been written.\nBot 1 has successfully attempted to acquire write privileges, and is now actively downloading data from the remote provider and writing it to its local mapping of the data sections.\nOnce Bot 1 is be done writing all the remaining data :\nprv/nvda.2020.stg will be reported as complete. Bot 0 will detect that the data that it requires is present and finish populating its data buffer from the data that was written by Bot 1. Bot 1 will be able to populate its own buffer from the data that it downloaded. Storage file layout # Offset Description Data ┌────────────┬──────────────┬───────────────────────────┐ │ │ │ │ │ │ │ Marketplace │ │ 0 │ Descriptor │ Symbol │ │ │ │ Total number of minutes │ │ │ │ │ ├────────────┼──────────────┼───────────────────────────┤ │ │ │ Sync data access lock │ │ PG_SIZ │ Sync data │ Write flag │ │ │ │ size counter │ ├────────────┼──────────────┼───────────────────────────┤ │ │ │ │ │ │ │ Raw array of values │ │ 2 * PG_SIZ │ Values │ in mid order │ │ │ │ (f64) │ │ │ │ │ ├────────────┼──────────────┼───────────────────────────┤ │ │ │ │ │ 2 * PG_SIZ │ │ Raw array of volumes │ │ + VA_SIZ │ Volumes │ in mid order │ │ │ │ (f64) │ │ │ │ │ ├────────────┼──────────────┼───────────────────────────┤ │ │ │ │ │ 2 * PG_SIZ │ │ Raw array of indices │ │ + VA_SIZ │ Prv_ids │ in mid order │ │ + VO_SIZ │ │ (u32) │ │ │ │ │ ├────────────┼──────────────┼───────────────────────────┤ │ │ │ │ │ 2 * PG_SIZ │ │ │ │ + VA_SIZ │ End │ N/A │ │ + VO_SIZ │ │ │ │ + PR_SIZ │ │ │ │ │ │ │ └────────────┴──────────────┴───────────────────────────┘ First, let\u0026rsquo;s remember that each section (except END\u0026hellip;) will be mmap-ed by the trading bot\u0026rsquo;s local provider in order to read and write data. Hence, each section needs to be placed at an offset which allows its mapping.\nAs stated in the dedicated article, the MMU only allows us to map entire pages, which means that our section must reside at an offset which is a multiple of a page.\nSince our trading bot may run on multiple systems, we must only ensure that our file layout is compatible with the systems that we want to run on, by choosing a very large page size. PG_SIZ = 64KiB is enough for a modern system.\nSince a year has a maximal number of minutes, equal to 366 * 24 * 60 = 383040, each array (volumes, values and prev IDs) have at most 383040 elements. Hence their maximal size is :\nVA_SIZ, VO_SIZ : 383040 * sizeof(f64) = 383040 * 8 = 3064320 bytes. PR_SIZ : 3064320 * sizeof(u32) = 3064320 * 4 = 1532160 bytes. Since we must respect page alignment, the effective size of those arrays can be determined by :\nVA_SIZ, VO_SIZ : (3064320 + (PG_SIZ - 1)) \u0026amp; ~(PG_SIZ - 1) = 0x2f0000 = 3080192 PR_SIZ : (1532160 + (PG_SIZ - 1)) \u0026amp; ~(PG_SIZ - 1) = 0x180000 = 1572864 (This uses the fact that PG_SIZ is a power of 2, so rounding down to it is just masking some bits, and that rounding a number N up to X is equivalent to rounding N + (X - 1) down to X.\nGoing further : mapping, shareability, cache coherency and performance # As we covered in the file system section, whenever a local provider wants to read data for a given year of a given instrument, it creates (if needed) then opens the file corresponding to this (year, instrument) and maps its different sections in memory.\nIf multiple local providers want to use the same data, they will end up mapping the same data pages in their respective virtual address space. This allows them to use simple atomic operations to coordinate themselves when writing to those buffers.\nWhen doing so, the processor\u0026rsquo;s cache coherency system takes care of :\nactually ensuring the atomicity of those instructions (like Compare And Swap) even in a multi-processing context. propagating the writes that one CPU makes to other CPUs, with the assumption that the correct memory barriers are used at the correct places. Cache coherency operations have an inherent cost. When CPU A writes at a given location which is in CPU B\u0026rsquo;s cache, both CPUs need to coordinate so that they all see the same value. If the architecture has implicit ordering guaranteed, they also need to ensure that all previous writes made by A are visible to B if it tries to read at the related locations.\nThis is why one of the first rules of performance is : \u0026ldquo;avoid mutable shared data\u0026rdquo;. Memory accesses are slow, and shared memory accesses are even slower.\nIn our design, things are not dramatic as :\nthe descriptor section is never written, except by the local provider which takes care of creating the file and initializing it. Other local providers never write this section. the synchronization page is the page whose accesses are the slowest, as it is inherently here to take advantage of the CPU\u0026rsquo;s cache coherency mechanism. We want accesses to this page to be slow, as most of them are atomic operations that are here for correctness rather than perf. data pages should not cause us too much latency, as the data that they contain is written by only one CPU before being read by the others. The situation where multiple local providers wait for the same data element should be relatively rare, but in this case, there is some latency involved, as cache coherency is made at the cache line (64 bytes = 8 64-bits floats) level. What could happen is that : CPU A gets write rights, downloads data, writes it in memory. The write finishes with filling half (4 elements) of a cache line L. CPU B reads the newly written data until the end -\u0026gt; cache line L must be migrated from CPU A to CPU B so that B sees A\u0026rsquo;s updates. CPU B gets write rights for the next data, downloads it, and writes it in memory. The write starts by filling the remaining half of L. CPU A reads the newly written data until the end -\u0026gt; cache line L must be migrated back from CPU B to CPU A so that A sees B\u0026rsquo;s updates. Since our system never assumes that writes made by a local provider to a location should not be visible to other local providers, we can map all our pages as shareable, and avoid the expensive copy-on-write kernel mechanism.\nWe can map the pages that we do not intend to modify (descriptor page + full data pages) as read-only, in order to actually prevent anyone from writing to them and ensure that our code is bug-free. Though it may not be a good performance idea, as remap operations are expensive in term of :\ncode path, since they involve taking a syscall. kernel processing, as they involve updating the kernel\u0026rsquo;s bookkeeping structures to reflect our new attributes. HW cost, as they involve updating our MMU\u0026rsquo;s TLB which could take long. Though, as writing to a full page of data (64KiB / 8 = 8192 minutes = 136 hours = 5 days worth of data) is relatively rare, one could disregard this perf hit.\nGoing further : prv_ids # As I mentioned before, there is a third array in our file storage, that has not been explained yet.\nLet\u0026rsquo;s first understand the problem that it solves, by re-stating the two different data read procedures that the provider must support :\nThe data consumers need to do the two following types of request :\nrange request : takes a start minute index (stt) and an end minute index (end) and for the time interval [stt, end[, generate : a volumes array with one volume per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0. a values array with one value per minute index in increasing mid order. If no transactions were made at a minute index, the related array element is 0. value request : takes tgt a target minute index (tgt), and determines mid the greatest minute index inferior or equal to tgt with a non-zero volume, then returns : mid. vol the volume at mid. val the value at mid. Based on the storage method implemented by the provider, which this chapter gave extensive details about, the first method should look straightforward. To perform a (multi-year) range requests, the provider performs multiple sequential sub range requests, one for each year that the main range request is composed of and for each of these single year sub requests, downloads data to (if needed) and reads data from the year\u0026rsquo;s storage file. It is relatively efficient, in the sense that performing the data reads will cost a lot of memory accesses but those are strictly required so there cannot be much optimizations done.\nA valid optimization could be, instead of having the consumers expect full data buffers, to have them expect an array of pointers to the mapped area where the actual data resides. That would avoid the initial copy but if data must be copied at the end, we would still need to do the copy.\nThe second request (the value request) is less obvious and more prone to footgunning ourselves perf-wise.\nIndeed, in order to find the first mid with a non-null volume starting at a given mid stt, we need to traverse the history minute by minute in reverse mid order starting at stt. This can take an unnecessarily long time and can easily be avoided if, for every mid x, we keep track of the last mid \u0026lt;= x with a non-null volume.\nThe prv_ids are made for this purpose. For every mid m that a file contains data for, it has an element which :\nif the volume at mid m is non-null, it is equal to the index of this mid in the values / volumes array. if the volume at mid m is null, and the previous index at which the volume was non-null is in the same year (and hence, that its index in the file\u0026rsquo;s array exists), it is equal to this index. if the volume at mid m is null, and the previous index at which the volume was non-null is in a previous year (and hence, that its index in the file\u0026rsquo;s array does not exist), it is equal to -1. The value request lookup (starting at stt is then straightforward for the provider :\nit first starts in the file that contains stt. it looks up in the prv_ids array at the offset corresponding to stt. if the read value is -1, it knows that the mid that it looks for is not in this file. It then reiterates on the previous year starting at the year\u0026rsquo;s last mid. if the read value is x != -1, it knows that the mid that it looks for is contained in this file and resides at index x in this file\u0026rsquo;s array. It then performs the reads and returns the expected results. Instead of traversing the volumes array one value at a time, this method allows us to only perform one read per year in which we want to look for (+ the overhead required to map file data which would be required anyway.\nThe reader may wonder what happens if we start a value read request at a mid for which an instrument never actually had any trade (ex : pre market introduction). This would theoretically cause the provider to look up every year in decreasing order until the start of times (i.e 1970 as everyone knows). In practice, we can just consider that an instrument having no transactions in an entire year simply was not traded before and have the request fail.\n","date":"19 June 2025","externalUrl":null,"permalink":"briztal.github.io/projects/bot/bot_2_prv/","section":"Personal Projects","summary":"Explaining the design of the data provider.","title":"Trading bot : data provider.","type":"projects"},{"content":" Context # While writing an article about a component of my trading bot, I needed to explain what mapping a file\u0026rsquo;s content in memory actually means, and cover the various performance issues that arise when doing so.\nSince this explanation ended up taking more lines than what I originally had in mind and it seemed relevant to other topics, I decided to break this up into a dedicated article.\nThe original article can be found here :\nIntroduction # In this article, I will provide an exploration into the banal act of mapping a file\u0026rsquo;s content to memory. Behind this simple act lies a trove of hidden complexities that have a very concrete impact on code performance.\nOnce again, deepening our understanding of how this works also unlocks a new set of design considerations leading to potential optimization techniques. And giving such an overview of this topic is exactly the aim of this article.\nAcronyms used in this article :\nPA will be used as an acronym for \u0026ldquo;Physical Address\u0026rdquo;, aka the addresses used in actual memory transactions on the bus (see below). VA will be used as an acronym for \u0026ldquo;Virtual Address\u0026rdquo;, aka the addresses used by code running in CPUs. The translation between the two and its impact will be covered in the next section.\nStorage device management # First, let\u0026rsquo;s cover how the kernel manages storage devices (disks).\nSome facts first.\nProcessors are distributed systems with many components, among them :\nmultiple CPUs, which all need to access DRAM. multiple peripherals (Ethernet interface, USB interface, PCIE interface) that (in broad terms) are meant to be programmed by CPUs and that in some cases must access DRAM. multiple DRAM slots. In order to make this different components interact with each others in an unified way, we introduce another component, the interconnect, abusively called \u0026ldquo;memory bus\u0026rdquo; in all this chapter. It\u0026rsquo;s role is to connect them all. In particular :\nit defines the notion of physical address. connected clients can be masters (can initiate transactions), slaves (can process transactions) or both. it allows its masters to initiate implementation-defined transactions which target a given physical address. It is a functional simplification to consider that memory reads and writes are particular cases of such transactions. it allows CPUs to configure and control peripherals by initiating transactions at PA ranges that peripherals are programmed to respond to. it allows CPUs and peripherals to read or write to DRAM by initiating transactions at PA ranges that DRAM controllers are programmed to respond to. Storage devices like your average SATA or SSD drive :\nare meant to be pluggable and hence have their dedicated connectors and HW communication protocols. are not meant to interface with processor interconnects directly. One cannot just make an access at a given PA and expect a storage device to just process this access. cannot be accessed at a random granularity (byte, cache line, etc\u0026hellip;). The same way the cache system cannot perform accesses at a granule inferior to the cache line, storage devices perform reads and writes at a minimal granularity, the sector size, which is a HW characteristic. consequence : accessing the data contained in a storage device (ex : a SATA disk) is made via a dedicated peripheral connected to the interconnect, providing a HW interface to talk to your disk, and requiring a kernel driver to control with this peripheral. A high level view.\nHere is a more complex but still extremely simplified view of how we can make code running in CPUs access (DRAM copy of) data located in a disk.\nInteracting with a hard drive. Participants :\nIC (interconnect) : allowing all its clients to communicate with each other. CPU : executes user code. Interconnect master only for this example\u0026rsquo;s purpose. DRAM : the actual memory of your system. Interconnect slave, processes transactions by converting them to actual DRAM read and writes. Disk : a storage device that is controlled via an implementation-defined protocol only handled by the storage interface. Not directly connected to the interconnect. Storage interface : interfaces the interconnect and the disks. Must be programmed by an external entity to do so. Interconnect slave : processes memory transactions targeting its address range as configuration requests, which allows external entities to control its behavior, for example to power up the disk, initiate a sector read or write (etc\u0026hellip;). Interconnect master : forwards disk data to DRAM via write interconnect transactions. Now, let\u0026rsquo;s describe the different sequences involved to allow CPUs to read data at two different locations on the disk. We will here suppose that CPUs can read at arbitrary PAs, and the next section will complexify this model.\nStep A : storage interface setup.\nThe kernel running on CPU 0 does the following things :\nit allocates two buffers of DRAM using its primary memory allocator, in a zone that allows the storage interface to perform memory transactions (DMA16 or more likely DMA32). it sends control transactions to the storage interface via the memory bus, to instruct it to perform two disk reads and to store the resulting data at the PAs of the buffers that it allocated. Step B : disk reads.\nThe storage interface understands the instructions sent by the CPU and starts the disk read sequences. The disk will respond with two consecutive data streams, that the storage interface will then transmit to DRAM using memory bus transactions (DMA).\nOnce the reads are done, the storage interface notifies the kernel of their completion by sending an IRQ targeting one of the CPUs.\nStep C : CPU access.\nNow that DRAM buffers have been initialized with a copy of the data at the required disk locations, CPU can read those buffers by simply performing memory reads at PAs within buffers A and B (provided that they have invalidated their caches for the memory ranges of A and B).\nNow, an immediate consequence of what we covered here is that CPUs cannot modify the data on disk themselves. All they can do is to modify the DRAM copy.\nIf updates need to be propagated to disk, the kernel will need to initiate another transaction (a disk write this time), instructing the storage interface to read from DRAM and write data to disk.\nFrom userspace, this is typically initiated via fsync or msync.\nMMU, virtual address space # In the previous section, we assumed that the different CPUs in the system could write to arbitrary PAs.\nThe reality is more nuanced, as PA access is critical :\nfirst, allowing any userspace process to access any PA basically allows it to read any other process\u0026rsquo;s data. Just for security reasons, we can\u0026rsquo;t let that happen. then, we saw in the previous sections that some PAs correspond to HW configuration ranges, and if mis-used, can cause HW faults which often just make the system immediately panic. For practicality, we also cannot let that happen. finally, most programs are not meant to deal with PA itself. Physical memory management is a critical task of the kernel, and physical pages are usually allocated in a non-contiguous manner, on a page-per-page basis. Programs often need to access large contiguous memory regions (like their whole 4MiB wide executable), and hence, have needs that are incompatible with how physical memory is intrinsically managed by the kernel. For all those reasons and others, code running on CPUs (whether it be kernel code or userspace code) does not operate with physical addresses directly.\nRather, it operates using virtual addresses, which translate (or not) into physical addresses using an in-memory structure that SW can configure but whose format is dictated by the HW, called a page table. Every CPU in the system is equipped with a Memory Management Unit or MMU, which translates any VA used by the CPU to the corresponding PA.\nThis article will not dive into the format of the pagetable, but here are a few facts worth mentioning :\nVA-\u0026gt;PA translation is done at the aligned page granule. Virtual memory is divided into pages of usually 4, 16 or 64 KiBs and each virtual page can map to at most one physical page. When I say map to, understand \u0026lsquo;set by the pagetable content to be translated to\u0026rsquo; multiple virtual pages can map to the same physical page. the pagetable contains the translation data and also attributes like translation permissions (can data be read, written, executed, and by who ?) and cacheability. the pagetable is dynamically writeable, but usually by privileged software (kernel) only. Now let\u0026rsquo;s add some details on how the CPUs will need to map memory to perform the various transactions described in the previous section :\nto program the storage interface, CPUs will need to map the PA region corresponding to the peripheral\u0026rsquo;s control range into a range in their virtual address space (in reality, into the kernel\u0026rsquo;s virtual address space which all CPUs use in kernel mode), and perform memory accessed from this virtual range. The mapping will be RWnE (read/write/non-execute) and non-cacheable, as we want the control transaction not to be affected by the CPU\u0026rsquo;s cache system. to read from and write to DRAM, CPUs will again need to map the PA regions corresponding to the buffer A and B into the kernel\u0026rsquo;s virtual address space and perform their accesses from this virtual range. The mappings will be cacheable, but a few precautions will need to be taken to ensure that reads see the data written by the storage interface, and that reads by the storage interface (during disk writes) see the writes made by the CPU : before reading data written by the storage interface, CPUs will need to invalidate the ranges for buffers A and B, so that when they read at those addresses, data is re-fetched from memory. before instructing the storage interface to write to disk, CPUs will need to flush the ranges for buffers A and B, so that the writes made by CPUs are propagated to memory and observed by the storage interface. To illustrate, here would be how the translation summary would look like for just our example. Here we assume that we have 4KiB (0x1000b) pages, a 3 bytes virtual address space (0x1000000 bytes of addressable VA), a 2 bytes physical address space (0x10000 bytes of addressable PA), among which :\nthe 7 first pages are mapped, and the remaining part of the PA space is not mapped. the storage interface responds to transactions targeting the first page of PA. the DRAM controller responds to transactions targeting the 4 next pages of PA. another peripheral, the UART, responds to transactions targeting the next (and last) page of PA. Translations in our example. As one can expect :\none VA page maps the control range of the storage interface, and is used by CPUs to configure it to initiate disk transactions. two VA pages map buffers A and B (let\u0026rsquo;s assume that they are page-sized) who are located in DRAM, so that cpus can read and write the DRAM copy of the disk data. Note that both buffers have non-contiguous PA and VA locations. No one maps the UART. The attentive reader may ask : how are CPUs executing any code ? Indeed, code resides in memory somewhere in PA and hence, needs a dedicated VA mapping for CPUs to read it. In an effort to keep this diagram simple, I did not add this info, but in reality, there should be DRAM pages allocated to contain code, and a contiguous VA region that maps those PA pages.\nAn important notion to remember is that if two CPUs run two user programs that want to access the same region of disk data, the CPUs may map exactly the same physical pages (buffers) allocated by the kernel under the hood. Or they may map different copies transparently made by the kernel. The next section will cover the modalities of this system.\nMMAP # So far, we know that :\nthe kernel handles storage operations (disk reads and writes) transparently on behalf of users, by allocating buffers and initiating disk read and write to / from those buffers via one of its drivers that interacts with the storage interface. Upon completion of disk read requests, userland code can access those buffers by instructing the kernel to map those buffers in the user\u0026rsquo;s virtual address space. This section will elaborate on the second part.\nThere are two ways for user code to access disk data :\nfile operations like read, write, readat, writeat, et al. Those operations involve invoking the kernel and do not scale well in practice. mapping the buffers managed by the kernel to contain disk data in userland and let user code read and write freely. We will focus on the second method, and in particular, in the way users can configure the mapping made by the kernel.\nThe main system call to map a file into userspace is mmap. It involves the following steps :\nuser code makes an open call to open the proper disk file and acquire its (shared) ownership. user code calls mmap with chosen parameters so that the buffers managed by the kernel are mapped into its virtual address space. Note that the kernel does not necessarily need to read the entire disk file and populate all the DRAM buffers to contain all the file\u0026rsquo;s data. Rather, it can choose to defer the disk reads to when the user code needs it : it will allocate a VA region of the size of the file in the user\u0026rsquo;s virtual address space, and let the VA pages unmapped. When the user code will access those VAs, the HW will generate a translation fault, which will be handled by the kernel. The kernel will then put the user code to sleep, perform the disk read, and upon completion, resume the user process execution at the instruction that originally faulted.\nMmap takes the following parameters :\npermissions : condition the way user code can access the mapped data : readability : if readable, the user can perform memory reads on the mapped VA region; if not, it cannot. writeability : if writeable, the user can perform memory writes on the mapped VA region; if not, it cannot. executability : if executable, the user can branch to the mapped VA region and execute its content; if not, it cannot. shareability : condition if the running process sees disk writes from other programs and vice versa. private : the process sees its local copy of the file. It behaves exactly as if the kernel had read the whole file at map time and the user process was mapping this copy. In practice this is more nuanced than this, as copying (potentially GiB-wide) files results in a huge memory consumption. The kernel likely implements Copy-on-Write (CoW) mechanisms to reduce the memory cost. Writes to this private copy will not be (natively) propagateable to disk. shareable: the process uses the kernel shared buffers and no private copy is used. Updates to those buffers will : be visible to any other process mapping the same buffers (i.e mapping the same portion of the same file) at some point. be propagated to disk when msync is called. It is typically considered a security vulnerability to leave a VA range WE (write + execute) as it allows an attacker with write capabilities to escalate into arbitrary execution capabilities.\nThis article will not elaborate too much on how CoW works, but here is a simple (and probably suboptimal) way it can be achieved : when a process maps a file in private and this file is mapped by other processes, the kernel first disallows any writes to this file\u0026rsquo;s shared buffers, then maps those in the process\u0026rsquo;s virtual address space. Any write to those buffers (by anyone) will generate a translation fault which the kernel will handle by effectively duplicating the buffers, and map the copy in the virtual address space of the process that required private access. Other processes keep the shared buffers mapped. Once that\u0026rsquo;s done, buffer writes are allowed again for both shared and private buffers.\nPerformance considerations # The two biggest performance factors to consider when mapping a file are :\ndo we need to write to the file ? If not, then we can map as readable and simplify the kernel\u0026rsquo;s life, as it is guaranteed that we never modify the mapped buffers, which reduces its CoW implementation. do we need to work on our local copy of the file, or is it acceptable to : observe updates made to the file by other entities. have our own updates observed by other entities that map this file as shareable. If we choose to access the file in shareable, we have to deal with the following performance issues :\nIf someone else maps the same portion of the same file in private, we could see a perf hit when writes are performed for the first time on a page, due to the kernel\u0026rsquo;s CoW implementation. If us and a remote entity access (read or write) the same locations, then the CPU\u0026rsquo;s cache system needs to synchronize those accesses, leading to real-time perf hits. Any multiprocessing system where entities map the same underlying physical pages must be designed in a way that minimizes the simultaneous access to those pages, or be prepared to pay a big performance cost otherwise. If we choose to access the file in private, we have to deal with the following performance issue :\nIf someone else maps the file either in private or in shareable, then the same buffers may be mapped (via the kernel\u0026rsquo;s CoW system) and we may experience random perf hits when us or another processes mapping the file writes to a CoW buffer. ","date":"16 June 2025","externalUrl":null,"permalink":"briztal.github.io/projects/stg_fs_map/","section":"Personal Projects","summary":"Explaining what happens when one mmap-s the content of a file.","title":"Disk access, MMU, mmap, a simple overview.","type":"projects"},{"content":"","date":"16 June 2025","externalUrl":null,"permalink":"briztal.github.io/categories/operating-systems/","section":"Categories","summary":"","title":"Operating Systems","type":"categories"},{"content":"The previous chapter of this series presented the different software components that constitute our trading, and provided some of the reasons for their existence.\nAs we saw in this chapter, the central part of this trading system is the trading core, which runs our strategies, either in real time for actual trading, or in simulation for backtesting.\nThe full picture # For the record and to illustrate before diving in, here is a full diagram of the trading core, presenting its components and the entities it interacts with.\nFull diagram of the trading core. At first glance, we can identify three main groups :\ncore components : the components of the trading core, which are all part of the same executable. Hence, they are all programmed in pure C and are ideally self-contained, in the sense that they do not incorporate third party code. remote providers and remote brokers : the services that provide data used as a strategic decision base, and that receive and process actual trading orders. The protocol that they use to communicate depends on the company that provides those services, and we cannot expect much standardization. connector components : those components bridge the gap between the trading core which is simple and self-contained by design, and the complex protocols that either remote provider or remote brokers require to communicate. They likely use libraries provided by the providers or brokers, and hence, we cannot infer anything in the language that they use. We only need to standardize the way they communicate with the trading core. In practice, connector components are running in their own processes, and communicate with the trading core via either unix sockets or named pipes, using a simple packet-based protocol. In reality, since most data providers support rest-ful APIs, the trading core executable uses libcurl in the provider interface to avoid the need of a provider connector for performance reasons. The broker connector will likely be present though, as interacting with remote brokers is complex and is usually done via libraries provided by the brokers themselves. Also in this case, performance does not really matter.\nLet\u0026rsquo;s now take a look at the fundamental concepts behind this rather convoluted design.\nStrategies # Based on what we established in the previous chapter, we want the strategies ran by the core to be :\ndynamic : we need the trading core to be regularly updated to reflect the most recent findings of the miner. backtestable : the miner needs to simulate the past behavior of a given candidate strategy to deduce its past return on investment, as a hint for its future return on investment. Internally, a strategy is characterized by three things :\na target instrument that it buys or sells. a currency, in which this instrument is traded. a detector, which reads historical data from the past and present and detects certain conditions (ex : an increase of more than X% in a given stock price). Upon detection, it makes a prediction on the future return on investment that buying the target instrument now would generate. trading parameters, which upon detection, describe the process of buying the target instrument, monitoring its price evolution, and deciding when it should be sold. Detectors can vary a lot and characterize the implemented strategy.\nTrading parameters describe in which modalities the trading core buys and sells.\nTrade sequences are very similar across different strategies : they invest a certain amount of money in the target instrument, and wait for certain conditions to be met to sell the previously bought instrument.\nHence, and since buying and selling is a rather critical part of the trading core, it is accomplished by a dedicated library, the trade sequence lib, which will be described in a dedicated chapter.\nAssets and portfolio # The trading core manipulates different assets, and passes orders, entities that are tracked by a library called the portfolio.\nThe first one and most obvious asset is money, which comes in different currencies. The wallet (sub-lib of the portfolio) tracks every money depot available to the trading core.\nThe second type of asset is instruments, like stocks, options, which can be traded.\nOne could argue that currencies can also be traded, a totally acceptable nuance, that I\u0026rsquo;ll happily ignore as my implementation is not targeting currency trading. But that would be a valid point and I have not given enough thought on how/if it would change the portfolio design.\nIn practice, it is likely that the broker offers us ways to query our trading account\u0026rsquo;s assets list. Although, I made the choice of not relying on the broker\u0026rsquo;s info, as I wanted to support multiple bots using the same brokerage account. In this case, there would be no way for bots A and B to know which assets of the brokerage account are allocated to each other.\nConsistency, fault tolerance, and on-disk portfolio storage # Since the previous section argued in favor of each bot having its own portfolio and asset management, managing this per-bot portfolio must be done very carefully. If there is any accounting error, the bot may end up spending more money that was originally assigned to it, or sell more stocks that it bought, possibly eating on stocks of the brokerage account that are owned by other bots using the same brokerage account.\nI made the choice of relying heavily on file system import and export to save the state of the portfolio, while also ensuring fault tolerance.\nSince order passing (whose intricacies with the portfolio are described below) is relatively rare in a system with a response period of one minute, it is acceptable to completely export the portfolio to disc before and after updating (passing, completing, cancelling) any order. This way (and with the careful export of the remaining trading core components at the same time), if the trading core faults for some reason, the previously exported (and valid) state of the portfolio can be used when restarting the trading core, and the asset accounting system is not in danger.\nOrder passing, and assets usage # One of the fundamental goals of the trading core is to pass (profitable) orders to the broker.\nThe order passing logic must respect the limits of the portfolio, understand : we must not invest more money than we have, and we must not sell more instruments than we own.\nA simple strategy (which makes accounting a bit harder for a portfolio with pending orders) is to just un-count any asset used to pass an order from the portfolio. Then, two things can happen :\nif the order is cancelled (either by the bot but ultimately by the broker which confirms the cancellation) then assets are re-added to the portfolio. if the order is (partially or fully) executed, then some assets are consumed (money in case of buying a stock), some are re-added to the portfolio (part of the money if not all of it was invested) and some new assets are added (the bought stocks). A consequence of this is that order descriptors (what we want to buy/sell, how much of it we want to buy/sell, for what price, how much we ended up buying/selling) must be very accurately tracked by the trading core, in collaboration with the broker : when an order is complete, we need to query the broker so that the portfolio has all the info it needs to maintain its bookkeeping.\nOtherwise, the portfolio would inaccurately track its assets which could lead to the catastrophic scenarios mentioned before :\na bot using more money than what it was authorized to handle, which is troublesome if multiple bots use the same underlying trading account. a bot selling more stocks than what it bought, which is also a problem in the scenario above. Allocation # The allocation algorithm is the heartbeat of the trading core.\nIt consists on a routine called periodically, which does three things :\nfirst, it calls the detector of each strategy, which returns : 0 if we should not trade now for this strategy. a non-zero weight if the bot should trade following the strategy\u0026rsquo;s trade parameters. This weight will be used right after to decide how much money is allocated to the trade. then, for each currency listed in the portfolio : it calculates the sum of weights of all strategies whose : detector returned a non-zero weight; and target instruments trade in this currency. then for each of these strategies, it allocates M = C * w / W money to this strategy, with : C : the total amount of this currency that can be traded. w : the weight returned by the strategy\u0026rsquo;s predictor. W : the total weight calculated previously. then for each strategy that it allocated money to, it creates a trade sequence using : the allocated amount. the strategy\u0026rsquo;s trade parameters. As stated before, trade sequences are not implemented by strategies themselves. They use a generic algorithm, and strategies only provide their own configurations for the generic trade sequence. Their involvement in the trading core hence stops here.\nGeneral considerations on the provider # This section will not describe the provider in a technical manner, as this will be done in a dedicated chapter.\nRather, it will first define the entities involved in the provision and consumption of historical data, and introduce the problems that the provider design has to solve.\nLet\u0026rsquo;s first define the terms :\nremote provider : the entity that provides the historical data (ex : polygonio). provider, or local provider : the block of the trading core that : queries the remote providers for historical data. stores the downloaded data locally on disk. reads this data from disk when decisions need to be made based on it. Remote providers like polygonio offer different prices for different historical ranges :\nif you pay the low price, they only allow you to access historical data between (now - 15 minutes) and (now - 3 years). If you want real time data, you have to pay a much higher price. other providers (like your actual broker) may give you access to the most recent data but may not go as far in the past It is thus important to support multiple remote providers. When the local provider needs to download data in a time range, it selects a source that can provide this time range and downloads the data from it.\nThe backtesting capability of the trading core affects the design of the local provider. Indeed, in order to take advantage of the full processing power of our processor, the miner will start multiple backtesting sessions in parallel, each running in a dedicated process, for each strategy candidate. These backtesting sessions will likely test variants of the same high level strategies for the same target instruments, and hence, will use the same historical data.\nEach one of these processes will have its own local provider, and the question is then : does each local provider have its own storage ? Formulated differently : can multiple instances of the local provider share the same disk storage, or do they have to re-download the data from the remote providers ?\nAs the reader can guess, mandating each backtesting session to re-download everything would be dramatic for perf. To give you numbers :\nBacktesting a single strategy involving AAPL and NVDA over a 3 month period takes roughly 3 seconds. Downloading this amount of data from polgonio takes roughly 15 seconds, or 5 times more. Hence, we must reduce unnecessary downloads as much as possible. Thus, having each local provider have its own storage and re-download data from the remote provider just doesn\u0026rsquo;t fly. We need multiple local providers to share the same storage efficiently.\nHence, we must design the local provider in a semi-clever way so that :\nmultiple local provider instances can be spawned; they share the same disk storage. they collaborate on the update of this disk storage. they do this as efficiently as possible. The actual implementation of the local provider is surprisingly simple in terms of number of lines, but has a high conceptual/nb_of_lines ratio. Many design choices do impact the overall perf, and this, the local provider implementation will be covered in a dedicated article.\nLocal broker, remote broker, simulation broker # We will use the words local and remote to qualify brokers the same way we did to qualify providers.\nA remote broker is the entity that our trading core must reach out to to pass orders.\nThe local broker is the part of the trading core that allows strategies to :\npassing new orders. getting the status of our orders in flight. cancelling orders. There are two types of local brokers.\nThe live broker is the local broker that is used for active trading : it forwards the order management requests to a remote broker (ie : crafting requests to and reading responses from the remote broker\u0026rsquo;s servers, possibly using a library provided by the said broker). Note that because of what we covered in the section dedicated to the portfolio, the broker does not get the portfolio\u0026rsquo;s composition from the remote broker.\nThe simulation broker is the local broker that is used for backtesting. Its goal is to simulate the execution of the order passing requests, using historical data. It uses the local data provider to query the price of an instrument target of an order at the (past) order passing time and reports the order complete at that price (or acts in a more evolved way based on the stop and limit prices of the order).\nIt inherently assumes that the orders that are passed by the trading core have no effect on the effective prices, which as of last time I checked my bank account, sounds like a reasonable assumption for a bot using it.\nBacktesting # As a reminder, the trading core has two operation modes :\nreal-time where real orders are passed. simulation where we want to compute the return of a set of strategies. For both testing and correctness, it is important to have the backtesting algorithm behave as close to the real time trading algorithm as possible.\nIdeally, we want to have the exact same code running in both cases, which is possible, with the only exception being the broker, as we cannot ask interactive brokers to pass orders in the past. We will need a dedicated component, the simulation broker, described in the broker section below, which will pass orders based on the provider\u0026rsquo;s historical data.\nAll other elements of the trading core can behave the same way.\nHence, in practice, backtesting only removes the need for a remote broker, which simplifies the structure of the trading core.\nHere is the diagram representing the components the trading core used when backtesting.\nDiagram of the backtesting trading core. ","date":"13 June 2025","externalUrl":null,"permalink":"briztal.github.io/projects/bot/bot_1_top/","section":"Personal Projects","summary":"A general view of the trading core.","title":"Trading core : high level view","type":"projects"},{"content":"This series of articles describe the structure of a low frequency trading bot supporting dynamic, adjustable and backtestable investment strategies, a project that I have been working on and off for three years in the hope that some day, I could just start it in the morning and watch it print money while I drink coffee. Or contemplate it burn all my savings in 10 minutes.💸\nThis chapter covers some necessary precautions, states the objectives of the trading bot, its limitations, the impact of these on the implementation, and defines what strategies are.\nFor a more technical description of the trading bot, please refer to part 1 of this series.\nDisclaimers # Open-source # I am legally bound by my company ( ) to not publish open source code in any form.\nThough, I am legally free to write about my personal projects.\nThis series of articles sometimes contains code snippets but the entire project source code will remain private.\nSponsoring # Across this article, I mention a few company names like polygonio and interactive brokers.\nI just happen to be their client.\nI have not been offered anything (money or reduction or any form of advantage) to mention those names.\nFor what it\u0026rsquo;s worth, none from those companies even knows that this website exists.\nLanguages # I am a kernel developper, and as such, I am proficient in using C.\nIt is not by disregard for other languages : I started programming with python as a teenager, then studied java in classes, wrote quite a lot of C++ with embedded (Arduino) development for personal projects, and then moved to C after I decided to stop using third party code and to just reimplement everything on bare metal.\nC++ could have been a valid choice to implement my trading bot, but I generally try to stick with C when possible, as it is closer to the machine and hence, offers you more control, resulting in more performance when needed.\nThird party libraries in C code # One of my general rules is to reimplement everything I use when possible and relevant. That is to make my code as portable as possible, and to have the best understanding of what it does. You can assume that the entirety of the trading core (see the diagram below) implementation (including the basic design patterns like trees, maps, lists, many functions of the standard library like formatted print and decode, etc\u0026hellip;) is made from scratch, with a few exceptions that are worth mentioning :\nOS interface layer : to be able to use kernel-managed resources, eg : files, sockets. curl : data providers like polygonio provide rest-ful APIs to query the actual data. This implies that you must either use libraries like curl or re-code them yourself, which was definitely out of this project\u0026rsquo;s scope. It kinda did hurt, but I installed libcurl\u0026hellip; broker APIs : brokers like interactive brokers saw the automatic trading trend coming and created dedicated libraries to allow you to procedurally do brokerage-related actions like creating and managing orders, getting your portfolio\u0026rsquo;s composition, etc\u0026hellip; Those libraries are rather complex and re-implementing them in pure C would be too risky (order passing is a sensitive topic !) and useless. In the broker chapter I will discuss the way to have a pure C self-contained code interact properly with those brokerage systems. Introduction # First, let\u0026rsquo;s state the limits and objectives of this project.\nI am not a trader, I am an outsider to the financial world. As such, I recognise that the investment strategies I can come up with would be naive and of poor quality compared to industry standards. I am a kernel engineer, and I wrote this trading bot with objectives that make sense in the kernel world.\nA kernel is essentially a shared resource manager. It coordinates the shared access to resources like CPU execution time, memory, storage, devices like networking cards, etc\u0026hellip; The fundamental problem of a kernel is : how do we manage these resources, in order to offer the most performant result ? \u0026ldquo;Performant\u0026rdquo; here can mean a lot of things : fastest, most energy-efficient, most secure, most fault-tolerant. Those factors are sometimes antagonist : more security often implies less performance, and vice versa. Before providing an implementation, one must carefully study what their use case is, deduce their performance metric, and then, design how resources should be handled by the kernel based on those axioms.\nIf we think about it, investment strategies essentially do two things : consume historical data steamed by data providers, and produce trading decisions consumed by the broker. It is the role of the trading bot to allow the strategies to do this in a performant manner. Historical data is a shared computational resource, to which strategies must have a very fast access to. The performance metric here is the access speed. Trading decisions are rarely made, and involve money and instruments, which must be managed with care. The performance metric here is the fault tolerance.\nThe same way a kernel aims to provide performant access to shared resources, my objective when writing this trading bot was to design a system which allows the easy implementation of complex strategies and that lets them access historical data and manage orders in a performant way.\nBut enough with the warnings, let\u0026rsquo;s get started.\nFundamental objectives # First let\u0026rsquo;s state the very basic objectives of our trading bot.\nBase objectives :\nobserve the variations of different instruments in real time. trade based on those variations in the least money-loosing way. Strategies # The term strategy is used to describe the process of looking in past/present data to identify when a set of conditions are met, and when this occurs, make a set of investment decisions.\nSome simple examples are :\nstatistical correlation between an instrument\u0026rsquo;s current value and another instrument\u0026rsquo;s past value : stock NVDA gained 5% between one day ago and now, we buy AMZN stock . If AMZN gains more than 5%, or loses more than 1%, we sell. arbitrary rule based on local variations of a single instrument : stock NVDA lost 5% of its value, we buy NVDA. If it regains more than 3 ppts or loses 2 more ppts, we sell. Those simple examples show that strategies can be designed as generic algorithms that receive parameters which conditions their behavior. The first example can be summarized as : \u0026ldquo;detect a price increase of stock \\(A\\) of at least \\(x\\)% between time \\(\u0026rsquo;now-d\u0026rsquo;\\) and \\(\u0026rsquo;now\u0026rsquo;\\), and when it is the case, buy stock \\(B\\). Then, if it loses more than \\(m\\), or takes more than \\(M\\)%, sell.\u0026rdquo;\nwith parameters :\n\\((x=5, d=one day, m=1, M=5, A=NVDA, B=AMZN)\\)\nThis implies that there is an infinite number of potential strategies, and that one of our goals is to find the best strategies to use.\nLuckily for us (and this is actually rare in my domain) the performance criterion here is very straightforward : return on investment, aka: how much did we make.\nBut we still have to choose a finite number of strategies among an infinite set. This requires us to explore the space of possible strategies, and backtest a few chosen ones. By backtesting, I mean simulating the behavior of those strategies in the recent past to determine what their return on investment would have been.\nFrom what we covered until now, we can deduce a high level view of what the trading system looks like.\nFundamental components # Full diagram of the trading bot. External entities # First, we have two types of external entities, on the bottom :\ndata providers : provide historical data that our bot bases its decisions on. There can be many. the broker : allows the bot to pass order creation and management requests in real time. There can only be one. Trading core # Then, going up, we see the trading core. It is composed of a single executable, coded in pure C, it constitutes the central piece of the trading bot : it is the one that supervises the execution of our strategies, and that allows them to access historical data and manage orders in the most simple and performant way.\nThe trading core supports two different execution modes :\ntrading mode : runs strategies in real-time using present data, and forwards their order management requests to the real broker, in order to generate actual profits. simulation (backtesting) mode : runs strategies using past data, and simulates the order management requests, in order to determine if those strategies would have been profitable in the past. The strategies run by the trading core in trading mode need to be configurable, hence, it uses either named pipes or unix sockets to receive configuration requests from the agent.\nDatabase # Then, on the sides of the diagram, we have a database which contains two types of data :\nmining data (see miner below) : base data used by the miner to search for strategies. agent config (see agent below) : describes the strategies that should currently be run by the agent\u0026rsquo;s real time trading core. Updated by the miner, and read by the agent. Agent # At the center of the diagram on the right, we see the agent, which has two responsibilities :\nsupervising the execution of the trading core (as a subprocess) configured in trading mode. periodically querying the agent config data of the database which is updated by the miner (see miner below) via SQL queries and updating the trading core\u0026rsquo;s configuration to reflect this. Since it needs to access database data by performing SQL queries, it will be coded in python.\nMiner # In the center of the diagram, on the left, we see the miner, which has three roles :\nexploring the space of all possible strategies to find profitable ones. when it finds a profitable strategy, it adds these strategies to the agent config. it regularly backtests the strategies in the agent configs to verify that they are still profitable. If not, it removes them from the agent config. Its job consists on selecting candidate strategies and starting backtesting sessions using subprocesses running the trading core in simulation mode to calculate their past return on investment. It has no real requirement for performance (as the computational work is done by the trading core), but it must efficiently explore the space of possible strategies, and avoid backtesting the same strategies twice. Thus :\nit uses the database to store its backtesting state. it is coded in python to interact painlessly with the database. Controller # Finally, at the top, we see the control system, which :\noversees the execution of all the previously mentioned components (except the database which runs as a system service). allows admin controls to, eg: stop or start the agent, run custom backtesting sessions with admin-specified configs. It is implemented in python and is accessible through a web browser (using flask).\nLimitations # I am not a trading company # First, I am just an individual investor, running code from my computer (or a server) not directly connected to any exchange.\nThis has the following implications :\nReal time orderbook feed will not be available. We must use an aggregated data feed, which averages instrument prices over a base time slice. It could be any time duration, like a second, a minute, at day, etc\u0026hellip; Though a smaller period would mean the need for more storage, it would give me the advantage of having more info on which to base a trading decision. This data feed will come from a dedicated data provider like polygonio. That implies that we do not have that much choice in the data feed period. Most data providers provide data on a minute basis. This will be the base period that we will use to architect the system. For every minute, each instrument will have the following info :\nmid : minute index, identifier of the minute, i.e number of minutes between this minute and Jan 1st 1970 00:00:00. vol : cumulated volume of all transactions for this instrument in this minute. val : average of transaction prices for this instrument at this minute, weighted by transaction volumes. Latency # The second limitation is that the bot will be running only on CPUs (no hardware acceleration like FPGAs), and given what we already stated, will only be able to trade based on a per-minute data feed, which will imply a gigantic reactivity time of one minute.\nA consequence is that we cannot implement strategies that more sophisticated market participants can get an advantage from like :\nanything that requires reconstructing the orderbook. anything that is remotely latency critical, as our trading bot will work on the granularity of the minute. This low reactivity is a fundamental element in the trading bot design that this series of articles focus on.\nConsequences # From my work and other projects, I am very much aware that designing a real time microkernel for embedded systems differs from designing a consumer-facing kernel.\nIn the same manner, I understand that removing these two limitations would fundamentally modify the structure of the trading bot. I also realise that the problems I faced when doing my design and implementation were noticeably different than those that must be resolved in the professional trading world, where servers on which the bots run are directly co-located within the exchange and specialized hardware accelerators are available. Nevertheless I believe that this project is a relevant introductory exercise to designing automatic trading systems.\n","date":"10 June 2025","externalUrl":null,"permalink":"briztal.github.io/projects/bot/bot_0_intro/","section":"Personal Projects","summary":"Presentation","title":"Trading bot : introduction","type":"projects"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/categories/c/","section":"Categories","summary":"","title":"C","type":"categories"},{"content":" Context # In April 2023 my wonderful wife had the best birthday idea ever, and made me a custom development board based on an STM32H7 MCU.\nThe chip was rather cool and had an USB connector, an SD-card interface, and a JLink connector. It was all I needed at the time.\nAs it was my first time selecting a processor, I just cared about the CPU architecture, presence of caches, and clock frequency. I didn\u0026rsquo;t request a large flash or RAM size.\nThis detail ended up causing me some troubles, as I \u0026ldquo;quickly\u0026rdquo; (in a few months of adding code and developing a Kasan that needed a lot more code) filled the 128KiB of builtin Flash.\nI then decided to upgrade the CPU, and since new capabilities are never worth nothing, I attempted to design the devboard V2 myself.\nDuring this year of development, I also had written a custom USB stack and a driver to support ST\u0026rsquo;s USB implementation. And if it doesn\u0026rsquo;t sound hard, trust me, it is. ST\u0026rsquo;s USB module is the biggest pile of hardware crap I ever encountered. It had it all. Undocumented registers, hardware xprop, sudden freezes for no reasons, almost undocumented HW capabilities, pseudo-documented capabilities that aren\u0026rsquo;t actually implemented, name it and it had it. I bumped my head on the wall so much during the month and a half where I wrote that code, that the noise still resonates in the place that my now liquified brains used to occupy.\nBut that\u0026rsquo;s another story.\nAn USB stack I had, and I wanted to experiment with it. More precisely, I wanted to try using an USB-C connector instead of my old USB-B, and also wanted to give High Speed USB a try, because ST\u0026rsquo;s Full Speed USB wasn\u0026rsquo;t painfully broken enough.\nI also wanted to try and support the Ethernet, since there could be a lot more fun places where ST could fuck up, so why not visiting them all !\nI also wanted to experiment more on smaller peripherals, like I2C, UARTs et al., and I wanted dedicated connectors on the board for them because why not.\nPin multiplexing # An SoC is essentially one or more CPUs and multiple peripherals connected together via various interconnects.\nWhile CPUs are essentially processing data, peripherals sometimes use signals from the outer world. For example, an UART will have at least two signals, RX and TX, and likely have one control-flow signal per direction, which will add two more pins, CTS (Clear to Send) and RTS (Request to Send).\nThat\u0026rsquo;s already 4 signals, for a single peripheral. And that\u0026rsquo;s a rather low number of signals : memory access peripherals use data buses which use at least 8 signals (one byte), plus the various DIR/CLK signals.\nNow imagine that we have 10 UART peripherals on the chip, that\u0026rsquo;s already 40 pins used. Add the various SPI, I2C that may be present, the LCD peripherals, and so on, and you easily end up with hundreds or thousands of pins.\nA normal chip package doesn\u0026rsquo;t have that many pins : the LQFP-144, which is already a big one, only has 144 pins as the name states.\nIt simply cannot allow all peripherals to be used at the same time.\nHow is this not a problem then ? Well, simply because typical applications do not use all those peripherals.\nThanks to this, MCU manufacturers like ST have come up with a reasonable approach that we\u0026rsquo;ll call pin multiplexing : every pin of the package is internally connectable to multiple signals (but to at most one at a time). The GPIO driver has to configure which alternate function (which index of each pin\u0026rsquo;s multiplexer) is used. This is one of the first duties of any communication (USB, UART, etc\u0026hellip;) device driver of such a chip : configure the pin multiplexing so that the signals used by the driven device are actually connected to the chip\u0026rsquo;s pins. Then it can do other fun(damental) things like configuring the clocks and generating kernel panics.\nThe multiplexer can be large though. For ST\u0026rsquo;s H7 line, it is 16 entries wide, meaning that each pin can be internally connected to at most 16 signals.\nBut the fun goes the other way too : multiple pins can be connected to the same signal, which makes this connection system a good old nightmare-ish graph.\nTo see how deep out of fun-land we are, this is an excerpt from my chip\u0026rsquo;s multiplexing array.\npage 1/14 of the stm32h750vb pin multiplexer. This can be (painfully) translated into a machine readable text file that we\u0026rsquo;ll use later.\nHere is the one for my chip.\nFor the record, that took almost 3 days of part-time fun and cost me a few neurons.\nThe sytax is quite simple and doesn\u0026rsquo;t deserve a detailed explanation, it\u0026rsquo;s just a line by line description where the first non whitespace char determines the described entity.\nIt could be possible to directly get this information from the pdf using a pdf-to-text-like manner. Though I tried that for a couple of hours and quickly ran out of patience.\nThe problem # Now back to our original problem : a devboard I wanted to design. And I knew what I wanted :\n4 UARTs, among 8. 3 SPIs, among 6. 2 I2Cs, among 4. 1 CAN, among 2. 1 4 bit SD interface, among 2. 1 USB with an ULPI interface, only 1 in the chip. 1 USB with a standard FS interface, only 1 in the chip. 1 Ethernet MDC + RMII interface, only 1 in the chip. When I say \u0026lsquo;among\u0026rsquo; in the list above, I mean that there are many devices of a kind in the SoC, but that I only want a certain number of them. Those devices are, for all intents and purposes, strictly identical. Any of those can be selected.\nThe issue, as one could imagine, is that because the total number of pins is pretty low, and the number of signals is pretty high, there is a high chance of collision.\nNow, if you think about it, I could just have try-and-errored in Kicad until I found a valid solution : if I saw that one peripheral needed one pin that was used by another, I would have discarded the one with the smallest priority and repeated the process.\nAs a matter of fact, I did try that. And it gets old. Veeeeeeryyyyyy quickly.\nThat\u0026rsquo;s mostly because if you think about it for a moment, there are a LOT of possible combinations : let\u0026rsquo;s imagine that we know exactly every peripheral that we need (forget the M among N-s in the previous list), and that we have 16 of them. Every one of them has 4 signals, and each signal can be routed to 2 pins. We literally have 64 pins that each have two possible values, which makes a total of 2^64 possible configurations.\nAnd that\u0026rsquo;s a lot. More than what your computer can index.\nAnd doing that manually can be very long.\nSo after the first evening of failed attempts with the donk-ish manual way, I told myself it would be great to use the few remaining connected neurons to at least code something to accelerate the process.\nObjective # With a given problem (certain peripherals to select) there could be a lot of possible solutions.\nThe objective of the algorithm will not be to find all solutions, not because it would be a bad idea, but simply because it would potentially consume all disk space on the machine.\nIn fact, as the reader will see, the resolution method contains the procedure to find all possible solutions. It is just that in most cases, finding just one solution is enough.\nThus, the objective of the algorithm will be to find one solution. If for some reason, that solution is not what the user expects, then the algorithm will easily find the next possible solutions.\nThe expected output # Ideally, we would like to be able to iteratively test multiple solutions for the same problem, in the same design.\nTo do that, we would like our algorithm to modify the connections in our Kicad schematics.\nThis is not doable per-se, but can be accomplished by structuring our Kicad schematics like the following :\nThe microcontroller only has labels connected to its GPIO pins, like what follows.\nLabels on the stm32h750 chip. All the hardware connected to the GPIO pins (USB physical layer, ethernet PHY chip, UART connectors, etc\u0026hellip;) only has labels connected to its signal pins, like what follows.\nLabels on the board hardware. Then, the only remaining thing is for our algorithm to generate a label associative array that for each GPIO pin of the microcontroller, connects to a hardware label, or to to GPIOs if the pin is not used like follows.\nLabels associative array. Luckily, Kicad uses text-based representations for the schematic components of a design, which allows us to copy-paste them. We will use that in a hacky manner, and make our algorithm generate the text representation of a label connection array.\nHere is the one for the previous image.\nWe will then copy paste that into kicad\u0026rsquo;s schematics editor and that will be it.\nNumerical complexity # There are two factors that cause the numerical complexity of the problem to exponentially increase : signals being connected to multiple pins, and us wanting to choose \u0026ldquo;K peripherals in a set of N\u0026rdquo;.\nLet\u0026rsquo;s formally establish the number of possible combinations that we should theoretically test.\nFirst, each signal sig can be connected to multiple pins pin_nb(sig).\nEach peripheral per uses the set of signals per_sigs(per).\nThen, for each group grp (ex UART) of peripherals that we must use, we have per_nb(grp) peripherals available for that group and we must pick chosen_nb(grp) in that group.\nEach group grp has per_nb(grp) choose chosen_nb(grp) possible configurations, with n choose k = n! / (k! * (n - k)!).\nEach configuration uses the set of peripherals cfg_pers(cfg).\nThe total number of possible combinations to test can be written the following way :\nN = PROD(grp in grps){ SUM(cfg in cfgs(grp)) { PROD(per in cfg_pers(cfg)) { PROD(per_sigs(per)) {pin_nb(sig)} } } }\nTheoretically, we could just iterate over all those combinations, check if each is valid, and stop at the first valid one.\nThough, if our model has a lot of peripherals with a lot of signals, we could iterate over a lot of combinations before finding a valid one.\nIn order for that search to be quicker, there are a few optimizations that we can do. During the development of this project, I was using a template design / peripheral selection as an example, and those optimizations made the number of potential solutions go from more than 2 ^ 64 to actually 16.\nNot 2 ^ 16. I actually mean 16 combinations to try.\nAlgorithmic data structures # The first thing we must do is translate the pin multiplexing array in the microcontroller\u0026rsquo;s doc into a machine readable text file that our algorithm will process.\nThat is long and painful, but trust me, it\u0026rsquo;s worth it. It is very likely that all chips in the same family (ST\u0026rsquo;s H7 for example) share the same multiplexing layout, so you may not have to do that frequently.\nThen we can read this file, and build a graph-like in-ram data structure, where :\neach group references its possible configurations (k choose n) each configuration references its peripherals. each peripheral references its signals. each signal references its pins. Here one must note :\neach configuration is part of one and exactly one group. each peripheral can be referenced by multiple configurations of the same group. (A) each signal is part of one and exactly one peripheral. each pin can be referenced by multiple signals. (B) A and B are the two factors that were previously mentioned to make the complexity explode.\nThose are the places where we must optimize things, by reducing the exploration space. Though, we cannot afford to remove a potential solution for the sake of speed. We must ensure that we only remove invalid configurations from the exploration space.\nPeripheral types # In the next sections, when the expression \u0026ldquo;for exploration purposes\u0026rdquo; or \u0026ldquo;FEP\u0026rdquo; is used, it means \u0026ldquo;given the current optimizations we found for the exploration\u0026rdquo;. For example, a signal S can be connectable to a pin P, that is guaranteed to be connected to another signal in every valid combination. In this case, S will be considered disconnected from P for exploration purposes.\nFirst, we can note that some peripherals are optional and some are mandatory :\nif a peripheral is part of a group where all peripherals will be chosen (FEP) (i.e. we choose N peripherals in a set of N, which is equivalent to having a group for each) then it is mandatory (FEP), in the sense that if a valid combination exists, this peripheral will be part of it. otherwise, the peripheral is optional FEP in the sense that if a valid combination exists, it may or may not be part of it. If a peripheral is mandatory, then we can start to optimize its signals.\nSignals optimizations # If a peripheral is mandatory, so are its signals. That means that if such a signal S can only be connected to one pin P, then it must be connected to it.\nOther signals of other peripherals connected to this pin can just be considered not connected to it for exploration purposes, since if there is a valid configuration, S will be connected to P.\nIf a signal is found to be connected to no pin for exploration purposes (i.e all its possible pins are known to be connected to other signals), it means that the related peripheral cannot be present in the final solution, and that we can just remove this peripheral from the graph.\nIf the peripheral is mandatory, then no solution exists.\nIf the peripheral is optional, then we can actually remove it from the graph.\nIf we find a signal to be connected to multiple pins, and one of those pins P is connected to no other signal, then we can disconnect the signal connected to P and disconnect it from all other signals.\nThis effectively reduces the number of connections on other pins, which potentially allows us to re-apply the first optimization discussed above.\nSignals optimizations # If the peripheral is optional, then we can remove it from the exploration space (i.e. from the graph). This has two consequences.\nFirst, every signal that we remove removes a connection to the pin that it was previously connected to. This potentially creates new pins with a single connection and we can re-apply the signal optimization sequence.\nThen, every peripheral that we remove is optional, and thus, is part of a group where we chose K peripherals among N, with K \u0026lt; N. Removing the peripheral reformulates the problem, by forcing us to choose K peripherals among N - 1 peripherals.\nIf K == N - 1, then all remaining peripherals become mandatory, and we can optimize them all using signal optimizations.\nIf K \u0026lt; N - 1, then all remaining peripherals are still optional.\nIn order to simplify the exploration space, we apply the signal optimizations and peripheral removal procedures repeatedly to the graph, until it leaves the graph unaffected.\nThen, we can bruteforce the graph by testing the validity of all remaining combinations.\nPathological case # There is a case where those optimisations bring no improvement.\nLet\u0026rsquo;s imagine a simple scenario where we have 2 pins on our SOC, we want to select 2 mandatory peripherals, each peripheral has 1 signal that can be connected to both pins.\nNo optimization will work here, since each pin is connectable to 2 signals, and each signal is connectable to 2 pins.\nThus, those optimizations will not always improve performance, but they will statistically improve it a lot, since this kind of pathological case is rather rare, or only involves a low number of pins.\n","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/projects/chip_pinout_generator/","section":"Personal Projects","summary":"A rather-not-smart but efficient way of connecting your PCBs.","title":"Chip pinout generator","type":"projects"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/categories/kicad/","section":"Categories","summary":"","title":"KiCAD","type":"categories"},{"content":"As stated in the previous sections, to verify our accesses, we will use the MPU to blacklist all the RAM minus the stacks, which will make any access to these instructions trap.\nThen, the memory checker will use the context saved data to retrieve the PC of the instruction that caused the memory access, then decode it, then emulate it, checking the various accesses performed by the instruction in the meantime.\nOur emulator will be composed of two base blocks : First, a set of instruction emulation functions that will, for each instructions that we support, emulate the instruction. That involves :\nread the content of the context-saved registers. write the content of the context-saved registers. read memory. write memory. perform KASAN check whenever a memory access is performed. Second, a decoder, that will, given the binary encoding of an instruction, extract the instruction opcode, extract the relevant fields in the instruction, then call the C emulator function associated with that opcode.\nDecoder # The objective of the decoder will be to decide which instruction emulation function to call and with which argument, given the binary encoding of an instruction that caused a memory access.\nThis is where things get tricky.\nIndeed, an ARMV7M CPU like the cortex M7 executes ARM-tumb assembly. Sadly for us, the ARM Thumb assembly is NOT a fixed length instruction.\nAn ARM Thumb assembly instruction has one of two forms :\n16 bits instructions. 32 bits instructions, composed of two 16 bits words. No field crosses the two 16 bits words. This historically comes from the various additions to the THUMB instruction set, that forced the adoption of 32 bits instructions, as 16 bits were not enough to represent all required instructions, like VFP.\nAs we will see, writing a decoder is a real pain, but can be done really fast and automatically if we have the right tools. Though, decoding a variable-length instruction set is a REAL problem, and is much more complex than writing a fixed-length instruction set decoder.\nThink about it : in order for our decoder to process the instruction, it must read the actual value of the instruction. But in the case of a variable-length instruction, how can it know the length to read ? It should read byte by byte only whenever needed, which will greatly complexify the code, compared to a fixed length instruction set, where it only has to read once and (in a couple of words) bisect depending on the values of certain bits.\nLuckily, ARM makes our life easier by allowing us to detect if a word known to be the start of an instruction is the start of a 16 bits instruction or a 32 bits instruction :\nhttps://developer.arm.com/documentation/ddi0403/d/Application-Level-Architecture/The-Thumb-Instruction-Set-Encoding/Thumb-instruction-set-encoding#:~:text=The%20Thumb%20instruction%20stream%20is,consecutive%20halfwords%20in%20that%20stream.\nIf bits [15:11] of the halfword being decoded take any of the following values, the halfword is the first halfword of a 32-bit instruction: 0b11101 0b11110 0b11111.\nThis will allow us to divide the tough problem of decoding the variable length THUMB ISA into two much simpler problems, being decoding either the fixed length THUMB32 ISA or the THUMB16 ISA.\nThe first task of our decoder will be to determine if the instruction that faulted is a 16 bits instruction or a 32 bits instruction. Then, the decoder will call the relevant sub-decoder.\nBasic structure of a fixed-length instruction decoder # An instruction is composed of a fixed number of bits, each one having a particular meaning.\nWe can group these bits into three groups, for a given instruction :\nopcode bits : those bits will determine the sequence operation performed by the instruction, aka in our case, the emulation function to call to execute this instruction. data bits : those bits will provide configs and arguments like register ids to the sequence of operations, aka in our case, arguments to our emulation functions. unused bits : bits whose value do not influence the behavior of a given instruction. TODO PROVIDE AN EXAMPLE\nThe objective of our decoder will be, given a particular instruction represented by its binary encoding :\nto read the opcode bits and to determine which An instruction is decoded in hardware by the CPU (by a block called, oh surpruse, the decoder), and as such, the structure of the instruction is made to allow a fast decoding.\nIn real life :\ndata bits representing the same entity (an immediate, a register ID) will almost always be contiguous. We will call that entity a field. opcode bits may or may not be contiguous. the location of the different fields and opcode parts of the various instructions are often similar accross instructions of the same category. Let\u0026rsquo;s take a look at the structure of the THUMB16 ISA encoding to illustrate that point\nAs you can see, we definitly can see a pattern here in the placement of the opcodes and registers.\nThere is one exception to the rules stated before, which will kind of complefify the generation of the decoer : there are cases where two different instructions will have the exact same opcode bits, but one should be executed if a field (ex : register ID) has a specific value, and the other should be executed otherwise.\nEven if we can\u0026rsquo;t thank ARM for this rather unpleasant corner case, let\u0026rsquo;s thank them for not getting completely out of their minds and supporting the same situation but with a possible SET of values on each side rather than one value VS all others. That would have been a nightmare to manage. Thanks ARM.\nAll jokes aside, this will add a non-outrageous level of complexity to the decoder.\nGiven what I have described until now, you should start to have an idea of the structure of our decoder. It will be composed of :\na HUGE nested ifelse section (rather an ifelse tree) where we will bisect on the value of one single bit at a time; in the leaf parts of the ifelse tree either : a jump to handle an UNDEFINED instruction (aka an instruction not supported by the decoder); or a section to extract all data fields from the instruction encoding, and to call the related emulator function. If you are intrested by writing such piece of code, be assured that I\u0026rsquo;m not, as it is a nightmare to read, to write and to debug : any incorrect bit index can radically change the behavior of the decoder and cause some undebuggable issue.\nARM exploration tools # Luckily for US, ARM is in an even worse situation than us.\nIndeed, we only have to generate a decoder for the memory instructions. They have to generate decoders for ALL instructions, both in HW and in their SW tools. Due to that, it is highly unlikely that they didn\u0026rsquo;t come up with a solution to have those pieces of code easily generated.\nWhen releasing the ARMV8, ARM also released what they call the ARM Exploration tools, aka HUGE xml/html files that describe the structure of instructions, and in particular, THEIR ENCODINGS.\nThe reader could think, \u0026ldquo;Yeah, but they only released it for ARMV8, right ?\u0026rdquo;, which is true but kind of false in practice, as ARM ISAs are backwards compatible : the A32/T32 xml doc contains the encoding for the legacy ARM32 instruction, and all releases of the Thumb ISA.\nThis is \u0026hellip; PERFECT. I can\u0026rsquo;t thank ARM enough for doing so, as it will prevent us from needing to manually read the spec and translate the text-based encoding into C code.\nThanks to that, it is easy to come up with a python script that will transform those XML files into an easily processable text representation like the following :\nTODO EXAMPLE + EXPLANATION.\nThis code is not open-source.\nThe spec has downsides though :\nit doesn\u0026rsquo;t really gives the encoding in a direct manner. Rather, it gives the indices and lengths of the various fields that compose an instruction. Bit values within fields are represented using multiple notations (0, 1, \u0026lsquo;\u0026rsquo;, \u0026lsquo;Z\u0026rsquo;, \u0026lsquo;N\u0026rsquo;) and lack a bit of structure. When writing your parser, you will discover most of these corner cases the hard way. some instructions use values derived from MULTIPLE fields (ex : imm3:imm8). I searched a lot and I couldn\u0026rsquo;t find any machine-readable information that would allow us to automatically generate code to do those combinations automatically. For now, the emulation functions will have to take care of that. Going crazy : generating our decoder automatically # Using the instruction representation provided by the python script, it was easy and fun to write a small program that generates a C decoder using this representation. The program proceeds the following way :\nit reads the description of each instruction to decode. then it generates a bisection tree by recursively dividing the instruction set in two based on the value of a particular bit know (either 0 or 1 but not x) for all bits. then, it generates the ifelse tree based on that, and the field extraction code based on each instruction description. This code is again close-source, but if the reader needs to be convinced that what I describe is possible, they may find a procedurally generated decoder for a part of the AArch32 ISA (with minor changes), through this link\nThis code is not public domain and you should not use it as it will not work as it : the minor changes that I just mentioned are actually me intentionally inverting the encoding of some frequently used instructions because I\u0026rsquo;m a nice person.\nIf you need the real decoder, reach out to me directly.\n","date":"15 November 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_6_emulator/","section":"Personal Projects","summary":"Executing instruction the way it should not be done.","title":"Micro-KASAN : ARM Thumb Emulator.","type":"projects"},{"content":"","date":"15 November 2023","externalUrl":null,"permalink":"briztal.github.io/series/ukasan/","section":"Series","summary":"","title":"uKASAN","type":"series"},{"content":"","date":"15 November 2023","externalUrl":null,"permalink":"briztal.github.io/categories/ukasan/","section":"Categories","summary":"","title":"uKASAN","type":"categories"},{"content":" Introduction # Now that we have a better idea of how the kernel manages memory from a high level point of view, let\u0026rsquo;s elaborate on how the KASAN will fit in this system.\nMetadata # The KASAN will have to track the status of each byte of memory in the system.\nFor this, it will need metadata, that it will store in dedicated memory : each byte of memory managed by the memory system will have attributes, that will describe the status of this byte, or more precisely :\nif the byte is accessible (as defined in the region manager section) ? who can access the byte, either user, allocator or both ? is the byte writable ? is the byte initialized ? This can be achieved by storing 4 bits of attributes per byte of memory.\nAttributes location # Attributes must be located somewhere in memory. To place them, we will use the same strategy as the Regions allocator.\nWhen registering a memory regions in the region manager, before the region manager allocates all its metadata, the KASAN will split the region in two parts :\nkernel-accessible memory : this part will be forwarded to the region manager, that will in turn divide it in two subregions as described in the previous part : pages, and pages metadata. KASAN attributes : this part will contain 4 bits of attributes for each byte of memory in the kernel-accessible part. Attributes lifecycle # Attributes are the base information that the KASAN will use to verify a memory access. A memory access of N bytes starting at A will cause the KASAN to fetch attributes for the address range [A, A + N[, and verify that they allow the access to occur. If so, the KASAN will let the access happen. Otherwise, it will trigger an error.\nIn this context, let the access happen means emulate the access, as described in part 3. Here are some accesses that should cause an error to occur :\nreading from an uninitialized byte. writing to a non-writable byte. user accessing a byte not accessible to the user. allocator accessing a byte not accessible to the allocator. access to a non accessible byte. The following diagram describes the lifecycle of attributes, and the various events (accesses) that can occur, as well as the errors generated by the KASAN checker if any.\nTODO one does not simply find the time to draw diagrams\u0026hellip;\nCPU state # To make our attributes model work, we need an additional per-cpu variable that will report whether the said CPU is running allocator code or user code.\nThis will be used as a base to verify the accesses :\na CPU running in user mode accessing a memory block accessible to allocator only will trigger a KASAN error (use after free) a CPU running in allocator mode accessing a memory block accessible to user only will trigger a KASAN error (use after alloc) Tuning the allocators # The access checking relies on the accuracy of the KASAN attributes.\nThose attributes are not only modified by a direct access like the \u0026lsquo;initialized\u0026rsquo; field.\nIn particular, any allocation or free in both the region manager or secondary allocators must be reflected in attributes.\nThis requires the said memory managers to be modified, so that :\nwhen the region manager allocates pages, all the related bytes go from not accessible to accessible to allocator only. when the region manager frees pages, all the related bytes go from accessible to allocator only to not accessible. when secondary allocators allocate memory to the user, all the related bytes go from accessible to allocator only to accessible to user only. when secondary allocators free memory, all the related bytes go from accessible to user only to accessible to allocator only. When one of those transitions occurs, the KASAN must verify that the related bytes have the expected start state.\nAs mentioned earlier, the allocators must also be updated to report their entry / exit in the cpu state.\nCorner cases # The rules stated in the two previous sections are true in the general case. The FSM described is a high level representation of the reality, but there are some corner cases that make the actual implementation look less like an actual FSM.\nAllocators data # The main corner case is that allocators themselves (the actual allocator structs), so as CPU states, and some other main kernel components must be accessible both in allocator and user state.\nThat causes us to need a specific attribute state accessible to anyone and dedicated KASAN entrypoints to report that a specified memory block falls into this category.\nGlobal variables # Another corner case that we have to handle is the state of global variables.\nAs much as we would like to avoid them, global variables are a necessary evil. Take the memory system data structure for example : it must be initialized during the early stages of boot, when we have not yet registered any memory region, thus, when dynamic allocation is not available. In any case, memory allocation goes through it, so we literally can\u0026rsquo;t have it dynamically allocated.\nThere are a few cases like this one, where we theoretically could avoid using globals, but where there is no real benefit of doing so.\nOur kernel executable will have the two regular .data and .bss sections in which globals / static variables are located.\nThose variables are initialized as part of the very early bootstrap sequence, by copying the content of the initializer of .data (likely located in flash) directly in .data, and by zeroing-out the content of .bss.\nThose sections must be considered initialized by the KASAN, as they indeed are initialized during very early boot.\nThough, those sections are located in RAM, and will ultimately be included in a particular memory region.\nThis will add two tasks to the memory management system when registering a memory region :\nthe region manager will have to tag all pages in these sections as not allocatable, as those pages are implicitly allocated to the kernel. the KASAN will have to report any byte in such a section as accessible by anyone, and initialized, as opposed to other bytes whose initial state is accessible by no one. Attributes management # I will not provide a detailed explanation on how to actually handle attributes, as it is very implementation-specific, complex, and I have little time.\nRather, here is a link to my public code repo. Files ksn.h and ksn.c contain the platform-independent KASAN code of my kernel, whose main job is to manage attributes.\nnull You won\u0026rsquo;t be able to compile it as it requires the rest of the kernel to work, but this will give you an example of working attributes management.\n","date":"9 November 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_5_kasanmem/","section":"Personal Projects","summary":"How KASAN manages memory.","title":"Micro-KASAN : KASAN memory management.","type":"projects"},{"content":" Introduction # Before elaborating on the principles of operation of the memory checker, I will give a brief description of how a kernel manages memory in a system without an MMU.\nMemory region # The term region will be used here to refer to a region of physical memory (addressable memory) and not an MPU region.\nA memory region is a portion of the address space :\nat which memory is mapped. around which memory is not mapped. In particular, if an address range R is a memory region :\nall bytes inside R are accessible and are mapped to memory. the two bytes before and after R are not mapped to memory. A microcontroller has multiple memory regions. For example, SoCs of the the STM32H7* family have the following memory regions available :\nStm32H7 family memory map. Flash for STM32H750xB : - user main memory : 0x08000000, length 0x00020000 (128Ki). - system memory 0 (RO) : 0x1ff00000, length 0x00020000 (128Ki). - system memory 1 (RO) : 0x1ff40000, length 0x00020000 (128Ki). Flash for STM32H742xI/743xI/753xI : - user main memory : 0x08000000, length 0x00200000 (2Mi). - system memory 0 (RO): 0x1ff00000, length 0x00020000 (128Ki). - system memory 1 (RO) : 0x1ff40000, length 0x00020000 (128Ki). Flash for STM32H742xG/743xG : - user main memory 0 : 0x08000000, length 0x00080000 (512Ki). - user main memory 1 : 0x08000000, length 0x00080000 (512Ki). - system memory 0 (RO) : 0x1ff00000, length 0x00020000 (128Ki). - system memory 1 (RO) : 0x1ff40000, length 0x00020000 (128Ki). RAM : - TCM Inst RAM : 0x00000000, length 0x00010000 (64Ki). - TCM Data RAM : 0x20000000, length 0x00020000 (128Ki). - D1 System AXI SRAM : 0x24000000, length 0x00080000 (512Ki). - D2 System AHB SRAM1 : 0x30000000, length 0x00020000 (128Ki). - D2 System AHB SRAM2 : 0x30020000, length 0x00020000 (128Ki). - D2 System AHB SRAM3 : 0x30040000, length 0x00008000 (32Ki). - D3 System AHB SRAM4 : 0x38000000, length 0x00010000 (64Ki). - D3 backup SRAM : 0x38800000, length 0x00001000 (4K). Region manager # One of the most fundamental jobs of the kernel is to manage memory, that is, to allow itself and its users to allocate and free memory.\nThe precise architecture, and the reason that guide the architecture of such a memory management system will not be discussed here, as it requires a dedicated chapter. I may or may not write about that in the future. Rather, I will focus on the high level blocks and features, with few justifications.\nManaging memory is not easy, particularly when dealing with small memory blocks, as it often requires metadata.\nVarious allocation techniques exist and each have their tradeoff.\nThe memory management\u0026rsquo;s fundamental block is the region manager. It has to :\nmanage memory regions, as in tracking the allocation state of every byte of every region. allow dynamic allocation of blocks of arbitrary sizes. allow dynamic free of allocated blocks. Page # To accomplish the three goals listed in the previous section in a manner that is not too catastrophic perf-wise, the region manager will define arbitrarily the notion of page, as a block of size 2 ^ PAGE_ORDER, aligned on its size, with PAGE_ORDER the order of a page.\nPAGE_ORDER may or may not be hardware-related. On systems with an MMU, it is likely a multiple of the order of the MMU translation granule. On systems without an MMU, we can choose it more freely. The region allocator will then manage memory on a per-page basis : rather than keeping track of the allocation status of each and every byte in the system, it will keep track of the status of every page in the system, and make allocations on a per-page basis.\nThe order of the page is arbitrary, but the reader can have the following orders in mind :\nmodern systems : size = 64KiB, order = 16 (or higher). older systems (and windows lol) size = 4KiB, order = 12. embedded systems : size = 1KiB, order = 10 (or lower). As said earlier, the order of a page may be tied to the hardware in a system with an MMU, as it must match the translation granule of the said MMU.\nHere, we do not have that constraint. The page order will be selected by the kernel developer depending on the use case, using the following criteria :\na larger memory order will mean less pages in the system, which will mean more fragmentation in the allocators that will use those pages, but less metadata per page, so a potential better use of the memory. May be better for specialized applications where large blocks are required. a smaller memory order will mean more pages in the system, while will mean less fragmentation in the allocators that will use those pages, but more metadata per page, so a potentially underuse of the memory. May be better for generic applications that allocate a lot of blocks of different sizes in a non-predictable manner. The way to allocate and free pages is implementation-defined and depends on the use case :\nif allocation of blocks of contiguous pages is required, a buddy allocator can be used. This method only supports allocating blocks of 2 ^ k pages (with k an integer \u0026gt;= PAGE_ORDER), which proves to be enough in practice. if the system only needs pages and not particularly blocks of pages, a simple linked list can be used. Per-page metadata # To manage the state of the pages, the region manager will need per-page metadata.\nThis per-page metadata is a net cost, in the sense that it can\u0026rsquo;t be repurposed when the page is free, or allocated. The region manager is the fundamental block of the memory allocator. Other allocators are built on top of it. A consequence to that is that it cannot rely on dynamic memory allocation to allocate its metadata.\nThink about it : when the system boots, all allocators are empty. During bootstrap, when the region manager will register its first memory region, how could it allocate metadata, since calling some form of malloc would ultimately cause its own page allocation functions to be called, yielding a failure, since no memory is yet registered. This is a classical memory chicken and egg problem.\nThe region manager solves this problem by carving out a block of static (non-reusable, not provided by another allocator) per-page metadata in the actual memory block that will contain the pages. In other words, it will use some of the (theoretical) pages to contain the metadata of the remaining pages. The memory block containing per-page-metadata will obviously not be available for allocation.\nFor the region manager, a page will be either :\nnot accessible : the page hasn\u0026rsquo;t been allocated to any user of the region manager and should never be accessed by anyone. accessible : the page has been allocated to a secondary allocator, who may access it, and provide access to portions of it to users. Secondary allocators # As described in the previous section, the region manager\u0026rsquo;s job is to manage memory regions.\nIt can only allocate memory on a per-page basis, and cannot allocate smaller blocks.\nThough, it is kind of rare for a user software to need an actual page of memory. A rough estimation is that most allocations require around 1 to 4 cache lines of memory (64B -\u0026gt; 256B) which is way smaller than a page, even in a microcontroller environment.\nThe secondary allocators handle this use case.\nTheir behavior is simple : they act as an interface between the user and the region manager, by :\nallocating large blocks of memory (blocks of 2 ^ k pages); dividing them into smaller blocks in an implementation-defined manner; allocating these blocks to the user. There are many secondary allocators, each one having its own capabilities and tradeoffs. We can mention :\nslab : supports both allocation and free of blocks of a fixed size. Works by dividing a page into blocks of the same size. Block state is either stored in a bitmap, or directly in the data of the free blocks, to form a linked list of free blocks. slab array : supports both allocation and free of blocks of a set of sizes. Uses multiple slabs, one for each supported size. stack : supports allocation of blocks of arbitrary size and mass-free only. heap : supports allocation and free of blocks of arbitrary size, allocating blocks of different sizes in the same pages. Inefficient. Any memory block that the allocators manage is provided by the region manager. As such, any such block is always accessible.\nFrom the allocator\u0026rsquo;s perspective, a block of memory is :\nfree : the block is not accessible by any user. It can be used by the allocator to store metadata. allocated : the block is accessible to all users. It should never be accessed by the allocator. Summary # The following diagram summarizes the architecture of the memory system.\nTODO it seems that I have less time than I expected\u0026hellip; Sorry.\n","date":"3 November 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_4_kernmem/","section":"Personal Projects","summary":"How the kernel manages primary memory.","title":"Micro-KASAN : Kernel memory management.","type":"projects"},{"content":"This chapter will describe the basic principles of operation of the KASAN.\nStructure # Base # The previous chapters laid the foundations on which we will build the KASAN :\nwe cannot use any transpiling-oriented method to wire our KASAN to the executable due to potential code size increase. we have an MPU that can trigger a MemManage fault whenever a specific portion of memory is accessed. the handler of this MemManage fault can modify execution context (registers) of the program that caused the memory access, and update its execution flow. the MemManage fault handler gives us the location of the fault, so as the address of the instruction that generated the fault. Memory checker execution flow # To implement our KASAN, we will first use the MPU to disable access to the whole RAM region.\nThat will make any access to any RAM to cause a MemManage fault, causing an exception and the execution of the related handler.\nThe MemManage fault handler will save the part of the context that hasn\u0026rsquo;t been saved by the HW somewhere in memory for the emulator (defined below) to read or write it later.\nThe MemManage fault handler will then read the PC of the instruction that caused the fault, read the instruction and decode it.\nIt will then check that the related instruction is a memory access. If it is not one, it will just execute the classic MemManage fault exception.\nIf it is a memory access, it will emulate the instruction and perform its checking in the meantime.\nThis will be the place where the memory checking is done. It involves :\ndecoding the instruction. verifying that every access performed by the instruction is valid. updating the KASAN memory metadata to reflect the new state caused by the memory access (ex : write to an uninitialized location causes the location to be treated as initialized in subsequent accesses to that location). emulating the instruction, by performing the underlying accesses : register reads will cause the emulator to read the context-saved values. register writes will cause the emulator to write the context-saved values. memory reads will cause the emulator to actually perform the read now that the MPU is disabled. memory writes will cause the emulator to actually perform the write now that the MPU is disabled. if the instruction emulation is successful, modifying the PC that will be restored when the exception will return, so that the instruction after the one that just was emulated is executed at that time, and not the one that we emulated again. The implementation of such an emulator will have a dedicated chapter.\nWhen this is done, it will re-enable the KASAN MPU regions, reload the SW-saved context (possibly updated by the emulator since it was saved), and return from exception.\nThen, the processor will restore the HW saved context (possibly updated by the emulator since it was saved), and give control back to the interrupted thread, but at the instruction after the one that just trapped and got emulated.\nSummary # The following diagram summarizes the high level behavior or the KASAN.\nTODO : summarize initialization sequence.\nTODO : summarize validation operation.\nDesign notes # Whitelisting the stacks # In order for our MPU-based trap system to work, we need to add two other MPU regions, with a greater priority than the RAM blacklist region, to allow access to the user and exception stack (PSP, MSP).\nIf we didn\u0026rsquo;t do this, any code making a memory access in the RAM would cause an exception.\nThe processor, in the handling of this exception, would push the context to the stack in use at that moment (either Main or Process stack), which, if those regions were blacklisted by the MPU, would cause the MemManage fault to be escalated in a hardfault. This is not what we want, as it has different privileges, and less recoverability.\nThis increases to 3 the number of MPU regions necessary to implement our KASAN.\nDisabling MPU regions in the MemManage fault handler # The KASAN memory access checker will run in the MemManage fault handler.\nThis checker will likely access variables not located in the stack.\nThis is a potential problem, as the MPU may still be active when this handler is executed.\nEven if we had the option to have the MPU automatically disabled in handler mode, this is to coarse to suit our needs, as that would prevent the KASAN to check accesses made in handler mode, under which a large part of the kernel operates (syscall, scheduling, interrupts, etc\u0026hellip;).\nTo handle this situation, we will need to reprogram the MPU when we enter and exit the MemManage fault handler.\nOn entry, we will disable the regions related to KASAN, and on exit, we will re-enable them.\nKASAN regular entry # The next chapters we will talk about how to manage the KASAN memory attributes, and in particular, how those memory attributes are updated when an allocator allocates or frees a memory block. The allocators will need to call a KASAN entrypoint to verify that the attribute change is valid, and to perform it.\nIn reality, there are some issues when trying to enter KASAN in a function-like manner.\nAs described in the memory access trap section, the basic working principle of our KASAN is to blacklist the whole RAM minus the stacks, such as any access to this region causes a MemManage fault and causes the KASAN memory access checker to be executed.\nThe issue, is that the KASAN is meant to check all accesses made by the kernel, interrupts included.\nAs such, when entering KASAN The software way, we must prevent any interrupt to occur.\nThis can be done by disabling interrupts, and re-enabling them on exit. Though, we must be careful here : interrupts may have been already disabled before entering KASAN, so we must not re-enable them in this case.\nEntering KASAN \u0026lsquo;The Software Way\u0026rsquo; also requires us to be able to disable MPU regions. To do this, we need to have read / write access to registers of the System Control Space, which requires us to run in Privileged mode.\nARMV7M ARM for CortexM3 :\nUser access prevents: - use of some instructions such as CPS to set FAULTMASK and PRIMASK - access to most registers in System Control Space (SCS). As such, if we plan to have memory management code that runs on Unprivileged mode (ex : secondary allocator), or if we plan to support user code that reports memory as read-only to debug something, we need to escalate to Privileged mode.\nWe could have a dedicated syscall in the kernel to handle those cases. But then we would have to have two paths, for code that is already privileged, and for code that is not.\nA single KASAN entrypoint # There is a more clever way.\nUnluckily for us, in microcontroller-land, address 0 is often accessible. In my board, this is the start of the TCM-D RAM region.\nWe will use another MPU region to prevent any access at address 0.\nThen, we will define a dedicated software_kasan_entry function, implemented in assembly, that receives a pointer to a function to execute, and its arguments.\nThis entry function will be located at a fixed address in the executable. This function will manually trigger a read at address 0 on purpose.\nThis will cause a memory management exception to be triggered, and will cause the execution of the generic KASAN handler, which will setup the environment for KASAN execution.\nThe generic KASAN handler will then compare the fault PC to the location where the entry function causes the read at access 0.\nIf the values are equal, then it will treat the MemManage fault as a KASAN entry, and will read the function and its arguments from the saved context, execute the function and return.\nIf the values are not equal, then it will know that the MemManage fault was not made on purpose by the entry function, and will proceed with the regular KASAN access checking.\nThe related assembly code is :\n# Enter kasan # Receives 4 args at most : # - hdl : function to call, receives at most 4 args. # - arg0 : first arg to pass to hdl. # - arg1 : second arg to pass to hdl. # - arg2 : third arg to pass to hdl. # - arg3 : fourth arg to pass to hdl. .align 4 .global kr0_tgt_ksn_enter .type kr0_tgt_ksn_enter, %function kr0_tgt_ksn_enter: # Store arg3 in r4 for the handler to have an easier job. sub sp, #8 str r4, [sp, #0] str lr, [sp, #4] ldr r4, [sp, #8] # Store 0 in r12. mov r12, #0 # Trigger a read at address 0. # The fault handler will detect that it came from this PC and forward # the call to the kasan handler. enter_pc: str r12, [r12] # Complete. ldr r4, [sp, #0] ldr lr, [sp, #4] add sp, #8 bx lr .size kr0_tgt_ksn_enter, .-kr0_tgt_ksn_enter .global armv7m_kasan_enter_pc .set armv7m_kasan_enter_pc, enter_pc This assembly code defines a global variable whose address is equal to the address of the instruction that causes the access at PA 0.\nThe related KASAN C handler (called by the assembly MemManage fault handler after it saved the software context and disabled the KASAN MPU regions) will then compare the fault PC against this value and act accordingly.\nKasan handler C code :\n/* * Kasan entrypoint. * Called by armv7m_kasan_handler defined in armv7m.S. * @fault_pc is the location where the address of the instruction that * faulted is stored (@hw_ctx + 0x18). * @hw_ctx is the start of the hw-pushed context. * @sw_ctx is the start of the sw-pushed context. */ void armv7m_kasan_entry( u32 *fault_pc, u32 *hw_ctx, u32 *sw_ctx ) { check(hw_ctx, sw_ctx); check((void *) fault_pc == psum(hw_ctx, 0x18)); /* Only handle memfaults with a valid MMFAR. * All other faults go to kernel. */ SCS_SHCSR SHCSR; SHCSR.bits = *(volatile u32 *) SCS_SHCSR__ADDR; if ((!SHCSR.MEMFAULTACT) || (SHCSR.BUSFAULTACT) || (SHCSR.USGFAULTACT)) _flt_forward(); SCS_CFSR CFSR; CFSR.bits = *(volatile u32 *) SCS_CFSR__ADDR; if (!CFSR.MMARVALID) _flt_forward(); /* Read the MMFAR. */\tconst u32 mmfar = *(volatile u32 *) SCS_MMFAR__ADDR; /* If the fault PC is equal to @armv7m_kasan_enter_pc, this is a kasan * entry call. */ if (*fault_pc == (((u32) \u0026amp;armv7m_kasan_enter_pc) \u0026amp; ~(u32) 1)) { /* MMFAR should be 0. */ assert(mmfar == 0, \u0026#34;MMFAR expected 0 on kasan entry.\\n\u0026#34;); /* Fetch the handler and its arguments. */ void (*hdl)(uaddr, uaddr, uaddr, uaddr) = (void (*)(uaddr, uaddr, uaddr, uaddr)) hw_ctx[0]; uaddr arg0 = hw_ctx[1]; uaddr arg1 = hw_ctx[2]; uaddr arg2 = hw_ctx[3]; uaddr arg3 = sw_ctx[0]; /* Call the handler. */ (*hdl)(arg0, arg1, arg2, arg3); /* Clear the fault. */ *(volatile u32 *) SCS_CFSR__ADDR = 0xffffffff; /* Recover from the fault. */ *fault_pc += 4; return; } /* Otherwise, we must check the operation in progress. */ /* Determine the fault address. */ const u32 pc = *(volatile u32 *) fault_pc; /* Determine if we are in allocator or user context. */ const u8 from_all = !!all_ctr; /* Decode the instruction, execute it and return the * PC of the next instruction. * If an error occurs, do not return. */ const ksn_emu emu = { .emu = {\u0026amp;ksn_emut}, .hw_ctx = hw_ctx, .sw_ctx = sw_ctx, .flt_addr = mmfar, .from_all = from_all, .chk_flt = 1, .is_ici = 0 }; const u32 new_pc = _process_pc(pc, \u0026amp;emu); check(pc \u0026lt; new_pc); assert(!emu.chk_flt, \u0026#34;no memory access made by the emulator.\\n\u0026#34;); const u32 diff = new_pc - pc; check((diff == 2) || (diff == 4)); /* Clear the fault. */ *(volatile u32 *) SCS_CFSR__ADDR = 0xffffffff; /* Update the PC and resume execution. */ *fault_pc = new_pc; } ","date":"15 October 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_3_blocks/","section":"Personal Projects","summary":"KASAN in details.","title":"Micro-KASAN : KASAN structure.","type":"projects"},{"content":"This article will give technical details on the hardware capabilities used to implement our KASAN.\nI\u0026rsquo;d like to point out that the KASAN implementation is not specific to this chip. The only features needed for the KASAN to theoretically work are :\nan exception handling mechanism. an MPU. The chip # Development board. Let\u0026rsquo;s introduce the development board on which I built the KASAN.\nThe devboard that my dear wife made for me has :\nSTM32H750 processor; featuring a CortexM7 CPU implementing the ARMV7M architecture; a Memory Protection Unit (MPU) conforming to the ARMV7M architecture; a Floating Point Unit; 128KiB of Flash; TODO Around 512KiB of RAM. The MPU is the main component on which our KASAN implementation will be based. It is a needed feature.\nExecution privileges # A processor implementing the ARMV7M architecture has two levels of privileges :\nunprivileged mode : code running in this privilege level has limited access to the processor system registers. privileged mode : code running in this privilege level has unlimited access to all processor features. In particular, the CPU starts in privileged mode at reset. Then, it is up to the firmware (kernel) to run threads in unprivileged mode.\nCode running in unprivileged mode can do a system call to execute a system handler. This handler will execute in privileged mode. The actual function that will be executed can\u0026rsquo;t be programmed by unprivileged software.\nStacks # A processor implementing the ARMV7M architecture supports two different stacks :\nthe Main Stack, used in handler mode. Stack pointer : MSP the Process Stack (should have been called Thread Stack), used in thread mode. Stack pointer : PSP. The MSP / PSP are aliased to SP. Accessing SP actually accesses the stack pointer active in the current mode (thread or stack).\nExceptions # A processor implementing the ARMV7M architecture supports the notion of exception : code running at a given time can be temporarily stopped, for another piece of code (a handler) to be executed instead.\nWhen the handler is done (understand : when the handler function returns), the stopped code resumes its execution transparently. This mechanism is purely hardware, and serves among other things, to handle system calls and interrupts.\nThe piece of code that is executed is called a handler; when the processor handles an exception, it is said to be in handler mode. When it doesn\u0026rsquo;t, it is said to be thread mode. Thread mode can be privileged or not. Handler mode is always privileged.\nAn exception has an ID, which is used to determine :\nif this exception is enabled (i.e. if the handler is allowed to be called at all when the exception is triggered). the priority of this exception (i.e if the handler is allowed to be called given the currently executed exception, if any). the handler to be executed : handlers addresses are listed in a table in memory called the vector table. The location of the vector table is defaulted at boot and can be changed later by configuring the VTOR (Vector Table Offset Register) register. The ARMV7M architecture supports at most 512 exceptions.\nThe ARMV7M architecture defines the 16 first exception IDs as system exception\nARMV7M exceptions. Source : ARMV7M Architecture Reference Manual. Those system exceptions are interesting for us :\nReset : function called when the processor is reset. NMI : function called when the Non Maskable Interrupt exception is triggered. HardFault : handler for certain non-recoverable faults, or for some faults occurring inside their related handler (ex : nested memfault). MemFault : handler for a MPU access fault (see after) and others. BusFault : handler for invalid transactions on the memory bus. UsageFault : handler for misuse of the assembly (ex : division by zero). SVCall : syscall handler. DebugMonitor : handler for debug events when there is no debugger. PendSV : preemption handler. Systick : system timer overflow handler. The reader may have noticed that no exception has index 0. This is on purpose, as the first 32 bit word of the exception table used during reset contains the stack pointer loaded during reset.\nThe (at most) 496 remaining exceptions are interrupts.\nWhen an exception A is triggered, the processor determines the priority of A, so as the priority of B, the exception currently handled by the processor, if any. If A is enabled and has a greater priority than B (or if the processor is not currently executing any exception), the current execution context is saved in the local stack (either Main Stack or Process Stack) by the hardware, and the handler of A executed. When the handler of A is complete (when it returns), the processor detects it, and restores the execution context saved on exception entry. Then, the execution of B (or thread code) continues transparently.\nContext # The ARMV7M context is composed of\ngeneral purpose registers : R0 -\u0026gt; R3 : Caller-saved. R4 -\u0026gt; R11 : Callee-saved. R12 : Caller saved, scratch register. R13 : Caller saved, Stack pointer, alias : SP. R14 : Caller saved, Link register, alias LR. R15 : Caller saved, Program counter, alias PC. Floating point registers (When FPU is implemented and active) : S0 -\u0026gt; S15 : Caller saved. S16 -\u0026gt; S31 : Callee saved. Status registers : xPSR : composite of all status registers, see the ARMV7M ARM for more details. Context saving and exception # Saving the execution context means atomically storing a subset of the context registers to the currently used stack (Main / Process). Restoring the execution context means atomically loading a subset of the context registers from the stack, and as such, updating the state of the processor. (The loaded subsets and the stored subsets are the same).\nIn the ARMV7M architecture, This subset of saved/restored registers is composed of all the caller-saved registers and of the program status registers. This makes the exception entry / return procedures follow the standard ARMV7M calling conventions, and allows us to implement exception handlers using C code only when required (not here lol).\nHere is the set of context saved registers when the FPU is not enabled :\nSaved context with FPU disabled. Source : ARMV7M Architecture Reference Manual Here is the set of context saved registers when the FPU enabled :\nSaved context with FPU disabled. Source : ARMV7M Architecture Reference Manual The layout may be different depending on if the CPU is programmed to preserve 8 bytes stack alignment. In particular, the Reserved words may be omitted to achieve that purpose. Please refer to the ARMV7M Architecture Reference Manual for an exact description. If after a context save, the stored value of a register is modified, the modified value will be loaded in the register during context restore. This will be the base trick on top of which we will build our emulator.\nMPU # The ARMV7M features an optional MPU, always implemented in the CortexM7.\nThe MPU for Memory Protection Unit, allows the kernel to define up to 8 or 16 (depending on the implementation) MPU regions, and to define access permissions for these MPU regions.\nThese MPU regions must be :\nof a size that is a power of 2 and that is greater or equal than 32 bytes. aligned on their size (start address is a multiple of the size). If an access is made in one of those regions, with incorrect permissions (ex : write to a location in non-writable region, executing a location in a non-executable region), the MPU will issue a permission fault, and the MemManage exception will be triggered, causing the interruption of the program that caused the access, the saving of the register context, and the execution of the MemManage handler.\nI will not elaborate a lot on the various access permissions since that would require a chapter of its own, with little added value. What is important to us is that the MPU allows us to :\nblacklist a memory block, i.e. prevent the CPU from accessing this memory block while the MPU is active. whitelist a memory block, i.e. allow the CPU from accessing this memory block while the MPU is active. The MPU regions are indexed from 0 to N (N = nb_mpu_regions - 1), and have a fixed priority : when determining the access permissions for a byte at address X, if multiple MPU regions (let\u0026rsquo;s say A and B) cover X, then the MPU region with the highest index max(A, B) will be selected to decide which permissions use to access X.\nThat allows us, for example, to blacklist a large memory block using a MPU region with low priority, and to whitelist smaller portions of this memory block using MPU regions with a higher priority : when an access to one of the smaller blocks will be made, both the whitelist and the blacklist MPU regions will be considered, but since the whitelist MPU region has a higher priority, it will be selected and access will be allowed.\nThis will be the base trick on top of which we will build the memory access checking system.\n","date":"10 October 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_2_hw/","section":"Personal Projects","summary":"What our KASAN will run on.","title":"Micro-KASAN : Hardware.","type":"projects"},{"content":" Introduction # This article will state the theoretical base on memory checking required to understand how the KASAN checks its access.\nDisclaimer : I have no prior experience with the internals of existing memory checkers like valgrind. The rough explanation that I\u0026rsquo;m giving here may be partially / completely false. It just reflects my high level understanding. This should have no impact on the validity of the further sections. Let\u0026rsquo;s first define the base concepts.\nTerminology # Definitions # Memory location : location in CPU memory accessible by address. Memory access operation : read or write to a memory location. Memory area state : attributes of a memory area that define how it is supposed to be used. Ex : allocated, not allocated, writable, initialized, not initialized, accessible, not accessible. Memory management operation : software operations that change the state of a memory are. Ex : allocation, free (and friends, ex : realloc\u0026hellip;) operations. Memory operations : memory access operations, memory management operations. Memory checker : system that checks a subset of the memory operation made by a program. Checking a memory operation means intercepting that memory operation in some way before it occurs, then verifying that the said memory operation is valid, then if so, allowing the operation to complete.\nConsequences # A register is not a memory location.\nAccessing a register is not a memory access.\nSubset # The subset of memory operations that the memory checker checks can include all memory operations or an actual subset.\nExamples of hypothetical checkers that would meaningfully check a small subset of operations :\na memory checker could check only allocation and free (and related) operations, to ensure that no double free (by the user) or double alloc (by the allocator) is done. a memory checker could check only memory reads and track memory writes, to ensure that software only reads from initialized (written before) locations. Validity of an operation # Valid memory operation : an operation that philosophically makes sense, given the paradigms of the program.\nExamples :\nread to an initialized local variable. write to a writable local variable. read to a memory region allocated by the program, not freed since allocation. Invalid memory operation : an operation that philosophically doesn\u0026rsquo;t make sense, given the paradigms of the program.\nExamples :\ndouble free. read of uninitialized memory. use by the user after free. double allocation (by the allocator). use by the allocator after the allocation. write to non-writable memory. write to a memory location that philosophically doesn\u0026rsquo;t belong to the part of the program that does the access (mem corruption ?) The strategy to intercept a memory operation (the some way stated before) depends on the type of the operation.\nMemory management operations are easy to intercept.\nMemory management operations like malloc and free are regular functions with dedicated entrypoints. The memory checker just has to override these entrypoints either at compile time or at link time so that its own implementations of malloc or free are called. Then it has the opportunity to verify the validity of operations, and (potentially) call the real malloc or free if they are valid (or just manage memory itself, it doesn\u0026rsquo;t matter).\nMemory accesses are very hard to intercept.\nThose accesses are not made using a function code, they are made using actual assembly operations, and there is no easy way to just \u0026lsquo;override\u0026rsquo; those instructions.\nIntercepting memory accesses : transpiling and why we cannot # Transpiling # A potential strategy to intercept memory access operations is to transpile the program : the memory checker will disassemble the assembly code and replace all memory access operations with assembly stubs to instead call the memory checker\u0026rsquo;s entrypoints, or just do the checking in place.\nCost # this is a very tricky operation, as for example, the jump addresses / offsets have to be carefully updated. this has a significant impact on : the code size, as a single memory access assembly operation is converted to a sequence of assembly operations. the code performance, as every access will have to be checked. Applicability for microcontrollers # This strategy could theoretically work for a microcontroller code, but we have to consider the impact on code size very seriously.\nComputers have a large virtual address space and kernel-powered swapping capabilities that allows enlarging the size of the executable way beyond what we could ever need. Taking a 10x hit on the executable size won\u0026rsquo;t affect the possibility of executing the code.\nMicrocontrollers are not in this situation.\nMicrocontrollers have flash that range from a few kilobytes to a few megabytes.\nIn my case, my code was already taking 63% of the chip\u0026rsquo;s flash (128KiB large).\nEven taking a 2x hit wasn\u0026rsquo;t acceptable in my case, and the transpiling method only gives a worst case guarantee of Nx (N being the number of instructions required to emulate a single memory access instruction, N \u0026gt;= 2).\nThis strategy was not applicable in our case.\nTo implement a KASAN on such a device, we need to have a solution that is taking advantage of the processor hardware.\nThis will be the subject of two other chapters.\nMemory operation subset, and limitations # Variability in complexity # In the introduction I mentioned that the memory checker was checking a subset of the memory operations.\nIdeally, we would like our memory checker to check all memory operations.\nThough, depending on the implementation of the memory checker, some operations will be more difficult to handle than others.\nA trivial example is memory access operations vs allocation / free operations. It is very easy for a debugger to hook up to memory allocation primitives like malloc and free and to verify the validity of those operations, since those (with their realloc counterparts) support the entire lifecycle memory (you\u0026rsquo;re not supposed to free a block not acquired via free and freeing a block means that you shouldn\u0026rsquo;t free it again).\nThough, as we said earlier, checking actual memory accesses is non-trivial as involves directly working with the assembly, either directly as described in the transpiling section, or indirectly, as it will be described later for our implementation.\nStack and non-stack (heap or globals section) # Another example showing the variation in complexity between check operations can be found by thinking how we would check local variables as opposed to allocated variables.\nLet\u0026rsquo;s assume in this example that registers and compiler optimization do not exist and that when we access any type of variable, a memory access instruction is generated by the compiler.\nWhen compiling this function :\nvoid f(void) { volatile uint64_t i = 0; } the compiler will detect that a local variable i is used, and needs a location. It will place this variable in the execution stack allocated to f, and will generate :\na function prologue to reserve space on the stack. a memory access to store 0 at the location of i in the stack. a function epilogue to restore the stack to the state where it was before the execution of the prologue. In practice this hardly requires more than subtracting 8 to the stack pointer, storing 0 at the new top of stack and adding 8 to the stack pointer.\nNow, let\u0026rsquo;s note that the actual address of i (that \u0026amp;i evaluates to) is a stack address, and will become invalid as soon as the function returns. After that, it will potentially be allocated to another stack variable, or never used again, depending on the executed code.\nDue to this, if we want to check accesses to local variables (to detect a read from an uninitialized location), we also need to modify the function prologue and epilogue to report stack adjustments to the memory checker.\nNow, let\u0026rsquo;s describe what happens when the compiler processes this function :\nvoid f(void) { uint64_t *ptr = malloc(8); *ptr = 0; free(ptr); } the compiler will still allocate space on the stack for a local variable, but this time this local variable will be a pointer that will contain the address of the memory block to write 0 to.\nIn this case, it will be a lot easier for the memory checker to verify accesses to *ptr : its life starts after allocation by malloc, after which it can be considered uninitialized, and ends before free by free.\nThis life cycle will be easier to check, because as stated before, it is \u0026rsquo;trivial\u0026rsquo; for a memory checker to intercept actual calls to malloc and free, but it is way harder if not hardly possible to intercept all modifications to the stack in a meaningful manner.\nThe conclusion to this point is that it is more difficult to check accesses to local variables than it is to check accesses to dynamically allocated variables.\nLuckily for us, the compilers nowadays feature decent function-local analysers that will warn against such usages at compile time. Added to that, the main bugs that we statistically encounter in real life are due to dynamically allocated memory. Checking local variables is a nice feature to have, but not an actual requirement.\n","date":"3 October 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_1_memcheck/","section":"Personal Projects","summary":"General memory checking considerations.","title":"Micro-KASAN : Memory checking.","type":"projects"},{"content":"I am Raphael Outhier, a french engineer working in Texas and this is my website.\nI graduated in 2019 from a French engineering school, worked for ARM for a year and am now working for Apple as a platform (HW) kernel engineer.\nAt work, I specialize in CPU bringup and CPU tracing (coresight).\nProgramming is one of my passions, a passion that I invest a lot of time in.\nThough, I\u0026rsquo;m legally not authorized by my employer to contribute to open source projects.\nBut that still leaves me the possibility to work on those projects, and to write about those.\nThe latter is the reason why I created this website.\nIn this website, you\u0026rsquo;ll find articles about the various projects that I\u0026rsquo;ve been working on these last years.\nThose articles will likely revolve around some of the following themes.\nRobotics I originally started studying embedded systems development with the objective of understanding how robots (CNC machines, drones etc\u0026hellip;) work.\nAnd by understanding, I mean that I wanted to build them from scratch (as much as possible), and write the code to make them work, also from scratch.\nI have currently wandered far away from this objective, mostly because of the second theme described below that took precedence these last 6 years, but still have the hope to get back to finishing my CNC control software in the future.\nKernel development Controlling robots is essentially done using microcontrollers.\nMy first robot was a quadcopter that I built myself, and that I tried to write the control software for.\nUsing ARM MBed at the time, I realized that there was a lot happening under the hood. Actually, I realized that the control code that I wrote was nothing in comparison to the kernel that was running on the processor, and that was handling 99% of the work.\nHow can one be proud of his code, when it\u0026rsquo;s running in such a helped and catered environment. I was not.\nThat was the moment where I decided to write my own microkernel from scratch using just a C compiler and the datasheet for my microcontroller.\nThis has evolved into a 6 years (so far) project targeting more modern processors than my original 8 bits AVR, in which I implemented every major kernel block, including the memory system, the scheduler, the file system, the driver abstraction layer, the resource tracking system, and many more.\nThis is essentially thanks to this project and to the many hours that I spent refining my understanding of how kernel and processor work that I was able to get hired by Apple to do the kernel bringup for their next generation of devices.\n3D CAD algorithms Before I started programming, I was doing small projects on Solidworks, a CAD tool that I really like.\nWhat I found the most interesting, and that I didn\u0026rsquo;t really understand at that time, was how the core algorithms worked : how can you, from a sequence of abstract instructions, generate an actual 3d file.\nThose kinds of algorithms are called CSG algorithms, for Constructive Solid Geometry, and quickly got my attention after I became fluent with C.\nMy dream objective was to reimplement a program \u0026ldquo;like\u0026rdquo; (i.e. with only the basic part edition features) of solidworks, that would allow me to create my robot parts myself.\nI have been working on this project for around four years, and now have a pure C library that does boolean operations on 3d polyhedrons in a highly performant and robust way, using octrees and BSP trees, so as brep-based remeshing, 2d polygon boolean operations, and many other fun stuff.\nThat makes a lot of projects and code. As stated earlier, I am not authorized to publish any project because of my current job at Apple. But maybe that will change in some years, who knows.\nIn the meantime, the best I can do is to try to make good descriptive articles about those, and hope that you will find them interesting.\n","date":"27 September 2023","externalUrl":null,"permalink":"briztal.github.io/about/","section":"","summary":"I am Raphael Outhier, a french engineer working in Texas and this is my website.","title":"About me","type":"page"},{"content":" Foreword # This series of articles will focus on how to implement a KASAN (or a memory access checker) on a microcontroller kernel.\nThe introduction is a reflection of my personal opinions and of the motivations that lead me to implement my KASAN. The reader may find it cleaving, but they should find the rest of the article more technically oriented.\nThe need for a proper debug infrastructure # Recently my wonderful wife designed and offered me the greatest present a kernel developer could imagine : a custom development board with a builtin SWD interface.\nDevelopment board. I had been looking for a devboard to get back to microcontroller programming for a while and my only need was a proper SWD interface to attach a debugger.\nSeems like a pretty unrestrictive criterion, right ? False.\nMainstream development boards provide a dedicated bootloader chip connected to the main chip\u0026rsquo;s SWD/JTAG interface to handle the flashing for you. Though it may seem like a good idea, as it avoids the pain of buying your own debug tools, it prevents you from being able to connect your own debugger.\nUnless you are willing to trash those bootloader chips\nA debugger I needed.\nI rely a lot on debug tools to verify my code. My two most used tools are GDB to live debug, and valgrind to check for memory leaks / improper memory accesses. I consider valgrind to be the most essential correctness test, after \u0026ldquo;my code seems to do what it is supposed to do\u0026rdquo;.\nUntil recently, my strategy to debug code that aims to work on a microcontroller has been to code it in pure-C (no assembly) and to make it somehow work in a userspace process. That allowed me to debug it using gdb and valgrind.\nThough this strategy works for abstract pieces of the kernel (not dependent on the actual hardware, ex : file-system, scheduler, resource tracking, etc\u0026hellip;), this is hardly achievable for hardware-dependent ones, like drivers. One could think that we could just write an emulator of some sort and again run the driver in an emulated way. This works but will only give coverage on the behavior that the emulator supports. Failure to emulate an HW feature will lead to lack of actual coverage on that feature when run in real HW.\nThe best coverage we can get is to test the code under real operation conditions.\nTo achieve that, we need our favorite debug tools to work on embedded platforms.\nLet\u0026rsquo;s make enemies # The opinion in the embedded world seems to be\nYour code just has to be correct, and if it's not, you can just debug with printf.\nThis is not a satisfactory answer. This barely works for simple software, at the cost of bisecting the code, but becomes a nightmare when you deploy complex software that does memory allocation.\nMoreover, this relies on the fact that you have a kernel that handles your printfs correctly, to the UART that serves as log output. This implies either a booted kernel with device management initialized, or a very small kernel with embedded UART management. But what if we must debug the kernel itself ? What if the UART isn\u0026rsquo;t even available at that stage ? What if we are doing the bringup of the chip, i.e. testing boot / reset ? You won\u0026rsquo;t get any printf at that stage.\nStating that you can always debug using printf is absurd to the extent that, if you have access to a form of printf, you are potentially running your code in an environment like the Arduino framework. I can guarantee you that the people that developed this framework had some form of probe to debug the chip. When their programming and verification was done, they designed a board without this interface and sold it to you.\nIf the answer to that last statement is :\nCode on a microcontroller should only be running simple software with statically allocated memory\nThis is not a satisfactory answer either.\nModern microcontrollers (whose name should likely be reviewed) operate at the megahertz scale, and can have around one MiB of flash or RAM. This is the very definition of \u0026ldquo;potential to run complex software\u0026rdquo;.\nRather, that answer is merely a consequence of the lack of proper debugging tools widely available in the mainstream embedded world, and of the lack of knowledge around it : developers took the lack of proper debugging infrastructure as an immutable truth and their practices evolved in that direction, validating that assumption.\nBut it is not an immutable truth. To be more precise, it is completely and utterly false.\nThis series of articles aims to prove that running your code in a microcontroller environment gives you a leverage to actually perform memory checking of the same or even better quality than one available in a typical userspace process.\nDisclaimer # The KASAN implementation presented in this series is not open-source nor free, and this series of articles is not a tutorial on how to use such code. This is mostly due to two factors :\na KASAN is tightly coupled to the kernel that it is implemented in. As my kernel is closed source, such is my KASAN. I\u0026rsquo;m not a very nice guy :). Rather, it aims to give a solid base for whoever wants to add a KASAN to their kernel, so that when executing the following piece of code :\nu8 faulty_function( void ) { u32 *mem = kernel_malloc(sizeof(u32)); kernel_free(mem); kernel_free(mem); } the kernel goes from an ugly crash to :\nKr0 started. Welcome. Kr0 initialized. Kr1 started, core1 0x0 online. core1 : 0x30044900. Kr1 initialized. Kr2 started. Kr2 initialized. Initializing pre-smp kernel modules. Double free. Unexpected kasan attrs found during attrs set at address 0x20014038. Expected type : USR_ANY (exact match). NO ACCESS_ALLOCATOR | ACCESS_USER Read type : ALL_RWI (exact match). ACCESS_ALLOCATOR | NO ACCESS_USER | READ_WRITE | INITIALIZED Incompatible attributes are : ACCESS_ALLOCATOR | NO ACCESS_USER Though, to illustrate my points and give implementation details, I\u0026rsquo;ll sometimes provide code samples. For all intents and purposes, I declare this code as part of the public domain.\n","date":"27 September 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_0_intro/","section":"Personal Projects","summary":"Valgrind on a microcontroller.","title":"Micro-KASAN : Introduction.","type":"projects"},{"content":"","date":"27 September 2023","externalUrl":null,"permalink":"briztal.github.io/projects/","section":"Personal Projects","summary":"","title":"Personal Projects","type":"projects"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/c/","section":"Tags","summary":"","title":"C","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/emscripten/","section":"Tags","summary":"","title":"Emscripten","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/gnu/","section":"Tags","summary":"","title":"gnu","type":"tags"},{"content":"In the previous part, I introduced the base components of what I consider to be a minimalistic kernel, and spent some time defining static modules, and why their correct initialization raises some issues.\nIn this chapter, I will be going into more details on WebAssembly and describe the issues I faced when porting my kernel to this environment.\nWebAssembly C Toolchain # Though the work of porting a whole codebase to a browser environment may seem like a hellish amount of work, the Emscripten toolchain greatly eases the workload, by providing a C environment resembling (relatively speaking…) to the posix environment.\nThe C compiler used for emscripten is LLVM (abstracted by the emcc program), which helps keep the C developer in a world he is familiar with.\nBut the pleasure of enjoying compatibility stops here, due to the fact that WebAssembly chose to reimplement its own object format, instead of using a familiar standard for its executable format like ELF (Executable and Linkable Format).\nNow, I am not claiming the authority to judge whether this was a bad idea, nor question their objectives, and I am certain that they had their reasons for making such a choice. I could, for example, imagine them desiring to get rid of the executable side of the ELF format (program headers etc…) to avoid wasteful memory consumption.\nIn the end, the resulting format shares many similar features with the linkable side of the ELF format. But as we will observe in the next sections, they also chose not to support some of the ELF formats useful features that a C developer may be expecting.\nWebAssembly Object format # A quick look at the official documentation shows that the WebAssembly object format (the official documentation uses the term “module” but I will rather use the term “object” to avoid any ambiguity with the concept of module defined in the previous chapter) is similar to what a C developer expects of a linkable format, namely: the definition of sections referring to binary data. Each section having a attributes that define the format and content of this binary data :\n“ The binary encoding of modules is organized into sections. Most sections correspond to one component of a module record, except that function definitions are split into two sections, separating their type declarations in the function section from their bodies in the code section.”\nsource : https://webassembly.github.io/spec/core/binary/modules.html\nThe format defines standard section types, similarly to the ELF format, and assigns a particular meaning to them, such as code, data, import, etc…\nThey even provide support for ,what they referred to as custom sections, which may lead one to think that the feature evocated in the previous chapter (defining a custom section where a variable should be placed) may be supportable.\nBut all our hopes crumble, after observing that the compilation of the following code doesn’t result in the definition of any custom section :\n#include \u0026lt;stdio.h\u0026gt; __attribute__((section(“.xxx”))) const char *str = “MYSTRING”; int main() {printf(str);} Rather, the ‘str’ variable will be placed in the data section, meaning that the __attribute__((section(“xxx”))) is basically ignored :\nwasm-objdump -x main.wasm main.wasm: file format wasm 0x1 Section Details: Type[22]: … Import[3]: … Function[51]: … Table[1]: … Memory[1]: … Global[3]: … Export[13]: … Elem[1]: … Code[51]: … Data[3]: - segment[0] memory=0 size=565 — init i32=1024 - 0000400: 4d59 5354 5249 4e47 0000 0000 3806 0000 MYSTRING….8… - 0000410: 2d2b 2020 2030 5830 7800 286e 756c 6c29 -+ 0X0x.(null) For reference, the same file compiled with clang (C frontend for llvm) produces the expected result of placing the MYSTRING data in the custom xxx section. From this we can deduce that there isn’t any issue with the code itself but rather that the problem lies in the lack of support for this kind of operation in the emscripten toolchain.\nThis lack of support is in itself, not an issue for standard-C-compliant libraries. As the notion of sections resides outside of the C standard’s jurisdiction, and the __attribute__ keyword is an extension that just so happens to be supported by both gcc and clang. But, for any type of software that relies on the use of sections, WebAssembly will be incompatible.\nThough I may seem critical in my conclusion, I perfectly understand the reasons for this lack of support and their validity.\nIndeed the support for programmer-defined custom sections presupposes the existence of a more complex tool for defining the behaviour of the linker when merging multiple files containing custom sections.\nIn order to fill this need, standard toolchains rely on the provision by the programmer of linker scripts (.ld files), that define these behaviours.\nSupporting linker script represents a hellish workload, as such I perfectly understand why WebAssembly designers chose not to invest time on it, after all the usage of programmer-defined custom sections is actually very infrequent.\nMore details on the gnu linker scripts syntax : Command Language\nConclusion # This article served two purposes.\nFirst, introducing the requirements that may lead a developer to implementing or using a kernel, and describing the base software blocks that compose a minimalistic kernel.\nThen, showing that the concept of heterogeneous environments (bare-metal, OS-hosted, Web-browser-hosted) is not simply a concept but a reality, that implies an heterogeneity of toolchains, which force the developer aiming for portability to strictly rely on common toolchain capabilities, and that the lack of support for a toolchain feature may directly prevent particular programs to be ported to the considered environment.\nIn my example, the lack of support for user-defined per-variable custom sections forced me to rewrite parts of my kernel that depended on this functionality.\n","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/projects/kernel_to_wasm_p2/","section":"Personal Projects","summary":"WebAssembly, or why the toolchain matters.","title":"Joys of porting a kernel to WebAssembly, Part 2.","type":"projects"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/kernel/","section":"Tags","summary":"","title":"Kernel","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/wasm/","section":"Tags","summary":"","title":"wasm","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/web-development/","section":"Tags","summary":"","title":"Web Development","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/webassembly/","section":"Tags","summary":"","title":"Webassembly","type":"tags"},{"content":" The back story # For as long as I can remember, understanding how kernels were designed and written was something that has interested me.\nIn the beginning, a newcomer to the world of programming, I saw them as mysterious, incredibly complex and barely comprehensible software blocks, handling equally mysterious tasks in the execution of a program.\nA few years later, after having perfected my skills as a programmer, I dived into the concept with a new perspective, and discovered the real issues that required the use of kernels and the possible implementations choices that solved these problems.\nAfterwards, I did what every normal programmer would do in my situation and spent more than the next half decade implementing my own one.\nLast year, I discovered WebAssembly, an Intermediate Representation allowing the deployment of C software in a web-browser. In order to make this port possible I had to update a part of my code base to support WebAssembly, and it was within that context that I set upon myself the challenge of :\nporting the kernel to the wasm environment. WebAssembly and its related toolchain, Emscripten, have done a very respectable work in providing support for C99 with gnu extensions (ex : constructs like ({statements})), allowing regular C libraries to be ported without any issue.\nYet, I encountered my fair share of interesting issues when porting my kernel to this environment. After all, kernel’s are quite far from the simple standard-compliant C code, and as such forced me to push to the boundaries of the emscripten wasm toolchain and get familiar with it’s nooks and crannies.\nThis article’s aim is to describe these difficulties in more detail and hopes to, in the process, give you glimpses of how kernel’s on one hand and the emscripten toolchain, on the other, work under the hood.\nI will start this article by briefly introducing Webassembly, and I will then tell you more about the main needs that pushed me to implement my own kernel, and in doing so, I will give a short description of its main blocks. Following this, I will provide a more detailed look at the Emscripten toolchain, to expose a corner case that forced me to reimplement a part of the kernel’s init procedure to support WebAssembly.\nWeb-browsers and Javascript # Historically designed to render statically defined text-based pages, web-browsers have drastically evolved to the point we know, being nowadays having more in common with actual Operating Systems than simple page renderers.\nWeb-browsers getting more and more complex and varying in their implementations, and web-designers desiring to avoid as much as possible taking the execution browser brand (firefox, chrome, …) into account during development, it was central to provide a stable common API. Javascript rapidly took this place, the Javascript VM now being one of the central components of a web-browser.\nJavascript being an interpreted complex language, its execution performances rapidly showed their limits, even with JIT features enabled. In a world where Moore’s law applies, this may not seem like a real issue, as the assumption that the next generation of computer will provide enough performance increase to compensate for this slowness. Though, in our years where this law clearly started to present its validity limits, this issue had to be addressed.\nWebAssembly # Webassembly is a relatively new (2017) trending IR (Intermediate Representation) code format designed to be loaded and interpreted, translated or JITed in, among others, web-browsers.\nWasm logo An intermediate representation is, as the name states, a manner to represent a program that doesn’t aim to be directly executed, but rather :\nto serve as storage or exchange format.\nto be interpreted efficiently by a wide range of program processors (compilers, VMs etc…), to be either translated into other formats (ex : actual assembly) or directly executed.\nWebAssembly is, as a consequence, not a programming language (though the textual version may be used this way), but rather an object format that compilers may use to store the result of their compilation, and that a virtual machine may use as executable source.\nThis allows a possible wide range of programming languages to be compiled into it, for example C or Rust for the most widely known.\nThough its reputation may be tainted due to its strong use by web hackers to inject (among others) crypto-mining payloads in client side, webassembly remains a good option for porting C libraries in the Web world.\nKernels, or the need for environment abstraction layers # As any software engineer I aim to write “good” code. Naturally, pretty fast, the problem of defining what “good code” is arises. After all, given a choice between multiple possible implementations, it would be best to have some criterion.\nThough I recognize that these criterion are not universal, in my implementations at least I now consider 4 performance indicators that I have listed them below in decreasing importance :\nportability : the code must be compatible with the widest scope of environments.\nfunctionalities : the code must offer an answer to the most general problem.\nmemory consumption : the code must use the smallest amount of memory, considering the previous rules.\nexecution time : the code must run as fast as possible, considering the previous rules.\nI wish to once more remind the reader that these criterion are completely subjective, and are only the reflection of my experience and my objectives.\nIt is certain that real-time software engineers would have radically different guidelines, and would, for example, place execution time invariability, or avoidance of linked lists traversals inside critical sections, on top of their list.\nBut let us go back to my criterion.\nGenerally, I tend to place code reusability across use-cases and environments among every other requirement.\nWhen I say environment, I mean the platforms (x8664, x8632, arm32, aarch64), host OS (freestanding, linux, windows, mbedOS) and toolchain (gcc, llvm).\nThis means that the code I write must be as independent as possible of the host API its executable version will access.\nHaving this constraint implies defining standard custom APIs for the most basic host requirements of a program :\nhost memory access : ex for stdlib : malloc, free.\nhost memory mapping : ex for linux : mmap, mprotect, munmap.\nhost file access : ex : printf, for linux : open, close, read, write, seek.\nhost directory access : ex for linux : readdir, closedir, seekdir.\nthreading : ex for linux : pthread environment.\nThe need for freestanding implementation # To achieve the aforementioned code reusability objective, in a classic hosted environment defining a simple function indirections (static inline functions or macro aliases in a shared header) will do the job.\nThat being said, as an embedded software engineer, my primary environment targets are freestanding (without OS, bare-metal) systems, not hosted ones, which complicates the problem.\nThough, it is still possible to use frameworks for those freestanding systems, that would provide some semblance of a standard environment. The Arduino framework, being one example, offers a default standard for low-performance embedded systems. In this case the previous simple solution would still be viable.\nBut, after a more thorough examination and more experience with these frameworks I have started to have several objections regarding their use :\ncode quality : a simple look in an Arduino framework implementation is enough to leave one, at best doubtful, at worst frightened by the code quality and the apparent absence of debugging efforts.\ncode performance : for example, the quality of the memory management system may be weak, which could lead to memory allocation poor performances, which could impact the whole program.\nmissing features : platforms like the teensy35 (a platform nevertheless close to my heart as it was my entry point into embedded systems and my hardware of choice for a few years) is based on a cortex-m4f based on the ARMV7M architecture. As such, it features all of the architectural components required to implement a context-switch based mono-core threading API. Yet, the Arduino framework is primarily made to provide an uniform API for extremely low-power cores like AVR8 chips. As such it doesn’t provide a satisfying threading API, and so, if an implementation is required, it must be implemented manually.\nlanguage constraints : the Arduino framework is written in C++, which doesn’t ease the development of pure-C applications.\nAs a consequence, for freestanding systems, it becomes a necessity to have an available working implementation of the basic code blocks to support at least :\nmemory allocation.\nmemory mapping (if any).\nfile access.\nthreading.\nAnd those blocks happen to be the minimal blocks needed for a kernel.\n#The need for code compatibility between freestanding and hosted environments\nHosted and freestanding environments are different in many ways, but, most probably the most painful difference between the two for any programmer is the difference in debugging capabilities.\nOn the one hand, debugging on a hosted environment (ex : linux process) is very easy and fast. Tools like valgrind are extremely versatile, easy to use and can allow the developer to have an exhaustive list of all invalid accesses, memory leaks, uninitialized accesses, without requiring any additional instrumentation. Program faults are handled by the host and faults’ side-effects are reduced, as the host will close any used file, resource, etc…\nOn the other hand debugging on a freestanding environment is much more of a headache and may require a particularly expensive instrumentation (the most well-known toolkit being the Segger’s J-Link) to set up a gdb server, in the best case.\nAn invaluable tool like valgrind being a simulation-based debug tool, it only supports its host’s instruction set, and as such can’t be adapted to this environment. As such our debugging capabilities are greatly reduced.\nMemory leaks will be harder to detect, uninitialized accessed will be untraceable.\nProgram faults may be fatal, if not handled properly, as they may put the program in an unsafe state. Worst, if the related program is a controller for physical objects, may lead to erratic behaviours with repercussions in the physical world, which may be inconvenient if not harmful.\nAs a consequence, we have two complementary environments, one that depends on a safe and verified code but can’t provide ways to verify it, and another that provides such ways, but for which the consequences of not having a dependable and verified code are much lower.\nAs such we can make up for the shortcoming of the freestanding environment by making the code as much platform-independent as possible. This allows for a rigorous stress-testing and debugging of shared code via standardised, powerful and robust tools in the hosted environment, and safe deployment of this verified code on freestanding platforms.\nA simple user space kernel # Though the definition of what is and what is not a kernel may be a subject of debate, I believe the best definition to start with is the following :\na kernel manages shared resources in a system.\nThose resources may be :\ncores.\nTasks (and threads and processes).\nExecutables, the kernel executable at first, then possible dynamic libraries and modules.\nMemory.\nPeripherals.\nFiles.\nMore generally, a kernel aims to manage any shared resources accessible by multiple programs in a system. The role of the kernel is to supervise the access to those resources, by providing a consistent API to use them.\nContrary to what is most common for a kernel, my kernel will run in user-space.\nNormally kernels do not do this and isolate its threads from itself via the virtualisation / protection mechanisms offered by MMUs or MPUs. My kernel doesn’t include this feature as protection mechanisms from the MMUs and MPUs may be absent on the targeted systems. This can occur when the kernel is run in the process space on a hosted system (no MMU, may be emulated via linux-KVM if relevant but that’s another story), or on a cortex-m4f (implementation-defined MPU capabilities but no MMU).\nThat aside, the kernel implements most of the features that you would expect to encounter :\nBSP : environment dependent code, provides a standardised API to interact with cores, host memory managers, host file manager, etc… Memory manager : responsible for managing primary memory and to provide secondary memory APIs to cores. See my detailed article on this topic. Scheduler : decentralised system responsible for scheduling tasks on each core, and to coordinate the transfer of tasks between cores depending on each core’s load. Introduces the concept of process, i.e. threads accessing the same set of resources. Loader : provides static module initialization capabilities and dynamic modules / libraries loading / unloading capabilities. File system : tracks resources processes can use, manages their access. Resource tracker : tracks resources used by each process, and frees them automatically when the process terminates. Modules # The previous section introduced a particular block of the kernel, the loader, that was in the centre of one of the issues I encountered, the Loader.\nIt is not my intention to go into details regarding the internals of the loader, as this would be long enough for an entire article. Yet, as having a clear comprehension of the concept of modules is essential for understanding the following sections, we will introduce it.\nWe can divide the software that will be executed at runtime in two categories :\nkernel : contains all blocks previously mentioned, i.e. blocks required to manage resources, and to run user code. Kernel code must be linked statically to form the kernel executable. modules : code that is not strictly required in the kernel executable, and so, that can be dynamically loaded. For example, the file system is a component of the kernel, but the code to support a particular type of file manager (ext2, ext4, ntfs) can be provided in a module.\nNow that this nuance is introduced, we next need to introduce the concept of static modules and introduce their use case.\nOn certain platforms, the RAM memory sections are not executable, this implies that Writable memory can’t be executed, and so, such dynamic loading is impossible, as dynamically loading a module means writing it to memory from an external source, and modifying it (ex : applying relocations) such that it can access kernel symbols (functions and variables).\nIn these platforms, dynamic loading will be impossible, and all required modules should be linked statically to the kernel executable.\nOn other environments, like WebAssembly, the executable format is atypical, and dynamic loading may be difficult. Even if a loader implementation for such a format may be possible, it will require a lot more development effort than what I was willing to invest. In such a situation statically linked modules there again seem like a valid option, even if only as a temporary solution.\nAnother benefit of linking modules statically to the kernel executable is time, dynamic loading being a complex problem with complex and time-consuming solutions. Doing the linking work at compile time avoids the need to do it at each execution.\nA module being an implementation-defined executable aiming to have a user-defined impact on the kernel (for example, to start processes or to provide support for file system types), it must comprise a function accomplishing the aforementioned tasks, the module initialization function, that the loader can find and execute .\nThis execution will be done at different times, depending on the module type :\nFor static modules, all init functions of static modules must be retrieved at once and executed sequentially, possibly in a particular order. For dynamic modules, the init function must be executed as soon as the module is loaded in the kernel. Again, what will be of interest here will be the case of static modules.\nAt a high level the build procedure of static modules in our kernel works as the following :\nSource files of the same module are compiled independently. Source files of the same module are merged to form a single relocatable executable, the module executable. Module executables are modified, by possibly forcing symbols locality to prevent multiple symbols definitions. Module executables are merged to form a single relocatable executable, the static modules executable. During this process multiple static modules executables will be generated, each one having its own initialization function. Later, they will be merged into a single relocatable executable, and the position of these functions in the resulting executable may not be known.\nNot knowing the position in the executable of each module’s initialization function will be an issue as, at runtime, the kernel will have to execute each one of these functions, but the kernel being independent of modules, it has no knowledge a priori of the location of these functions.\nThere are two strategies one can use to help overcome this hurdle.\nThe first, and most common solution involved using the concept of sections, supported by the elf format. Practically this involves defining for each module a static function pointer that contains the location of the module’s initialization function, and placing this variable in a custom section of the efl. We shall be calling this custom elf section the .modinit.\nLater in the build process, during the merging of multiple static modules executables, the linker (ld) will merge the content of sections with the same name (default behaviour, but can be configured with the use of a linker script), in our case this means merging into one section the multiple static modules .modinit content. This will cause all our pointers to be located in the same output section, which in the resulting executable, will cause them to be located contiguously in memory.\nA wisely crafted linker script provided to the linker will define two global variables, .modinit_start and .modinit_end, used to help the kernel locate the start and end of this section, which will allow the kernel to retrieve addresses of all modules initialization functions needed to execute them.\nThis aforementioned process is the one that regular executable processes use to trigger the\ninitialization and de-initialization of static variables whose expressions need to be computed at run time.\nFor more details : ELF: From The Programmer\u0026rsquo;s Perspective\nThe second solution, which is in my humble opinion much less elegant and relies on knowing at compile-time the full list of static modules that need to be compiled and linked, consists in providing a unique name to each initialization function (derived from the module name, modules names must be unique), and to assign a global visibility to this function, then to define a single function calling each initialization function by name. The kernel then only needs to execute this function to initialize all modules.\nThough being un-clever, this procedure can be done automatically using a bash script.\nIn my pre-WebAssembly implementation, I used the first solution. This was only possible because I had easy access to the standard elf format as I was developing for the linux environment. But, as we will now observe, WebAssembly’s constraints on the elf format forced me to review my choice and opt for the second solution.\nIn-process kernels, WebAssembly and their difficulties to cohabitate.\nThe second solution, which is in my humble opinion much less elegant and relies on knowing at compile-time the full list of static modules that need to be compiled and linked, consists in providing a unique name to each initialization function (derived from the module name, modules names must be unique), and to assign a global visibility to this function, then to define a single function calling each initialization function by name. The kernel then only needs to execute this function to initialize all modules.\nThough being un-clever, this procedure can be done automatically using a bash script.\nIn my pre-WebAssembly implementation, I used the first solution. This was only possible because I had easy access to the standard elf format as I was developing for the linux environment. But, as we will now observe, WebAssembly’s constraints on the elf format forced me to review my choice and opt for the second solution.\nIn-process kernels, WebAssembly and their difficulties to cohabitate.\n","date":"7 October 2021","externalUrl":null,"permalink":"briztal.github.io/projects/kernel_to_wasm_p1/","section":"Personal Projects","summary":"In-process kernels, WebAssembly and their difficulties to cohabitate.","title":"Joys of porting a kernel to WebAssembly, Part 1.","type":"projects"},{"content":"","date":"21 September 2021","externalUrl":null,"permalink":"briztal.github.io/categories/kernel/","section":"Categories","summary":"","title":"Kernel","type":"categories"},{"content":"","date":"21 September 2021","externalUrl":null,"permalink":"briztal.github.io/series/memory-management/","section":"Series","summary":"","title":"Memory Management","type":"series"},{"content":"","date":"21 September 2021","externalUrl":null,"permalink":"briztal.github.io/categories/memory-management/","section":"Categories","summary":"","title":"Memory Management","type":"categories"},{"content":" Introduction # In the previous chapters, we defined the base characteristics of the system supervised by the memory manager, and we detailed the structure of such memory managers with a focus on primary memory management.\nIn this chapter we aim to explore the secondary memory manager but, detailing secondary memory management will require a different approach, as it is less constrained by the characteristics of the system. Since the primary memory manager is responsible for managing and supervising the allocation of primary memory it abstracts all the details of the system for the secondary memory manager and provides a high level page block allocation / free API.\nThe secondary memory management can be defined as:\na set of techniques that, from page blocks provided by a primary memory manager, provide APIs to allocate or free memory blocks of smaller sizes.\nThese techniques are numerous, and this article does not intend to be exhaustive. Rather, I wish to share the ones I found to be the most useful in my different implementations, and provide, for each of them, an overview of their different objectives, trade-offs, and working principles.\nPhoto credit : Photo by pipp from FreeImages Allocation types, the detailed version # The secondary allocator is the allocator the end-user accesses to obtain and recycle its memory.\nThis allocator is, as a consequence, a critical block, as any additional allocation latency may heavily impact the user.\nTo design such an allocator efficiently, the requirements of the user in terms of speed and memory sizes best be known.\nA first estimation we need to make is that of the allocation frequency per allocation size, as the user may require more blocks of certain size ranges, and less blocks of other size ranges.\nThe higher the frequency is, the more critical optimizing this case will be, and as such it should ideally :\nrequire the fastest allocation methods. not be concerned by internal superblock fragmentation issues, i.e. allocating a superblock and using very few blocks in it. For block sizes which are less frequently used are thus less critical, we can :\nallow slower allocation methods. require methods to maximise the use of allocated superblocks to avoid fragmentation. Real world experiment # Let’s use an experiment to classify such ranges.\nFor this I turned to the allocator of my kernel and added functionality to report each allocation and its requested size.\nThen I ran the kernel with some user programs: one of them was an instance of my home made VM doing its memory-consuming job of lexing, parsing, computing, compiling and executing.\nI then stored the size of all resulting allocation requests in a spreadsheet.\nFinally, I determined the amount of allocation of each size and compounded the results to the following graph :\nSame data using a log scale on the Y axis :\nThe difficulty to read it comes from the fact that a rare part of some of these allocations were large (max 2700B), and the vast majority concerns at most the range 0B -\u0026gt; 256B.\nRedrawing the first graph on this range gives the most informative result :\nAs we can see, the most used range is the 1B-\u0026gt;64B range, which corresponds to the small blocks allocation type that I evocated in the first chapter : results show that only 360 allocations over 9963 were strictly superior to 64 bytes, which makes less than 4 percents (3.61) of all allocations.\nThe small blocks type is, as a consequence, the allocation type on which I must focus my optimization efforts.\nSlab # A Slab allocator receives superblocks (aka : page blocks), from the primary memory manager, and treats them as arrays of blocks of a constant size \\(N\\), that it frees and allocates to its users.\nIt can only allocate blocks of this size. Each superblock has a header that keeps track of the number of its blocks that are allocated or in the allocator’s cache, and of the non-allocated zone, i.e. the zone of the superblock that contains only non-allocated blocks.\nWhen the superblock is allocated, this zone occupies the whole superblock. When the superblock must allocate one block, the first block of the zone is allocated, and the zone is truncated with this block.\nEach superblock also has a cache (i.e. linked list) that references all its free blocks that are not in the non-allocated zone or in the allocator’s cache.\nThe allocator also has a cache (i.e linked list) that references free blocks of all superblocks that are not in their superblock’s cache or non-allocated zone.\nThe slab works the following way :\nAllocation # If a block is present in the allocator’s cache, remove it from the cache and use it. Otherwise, if a superblock has free blocks, either in its cache or in its non-allocated zone, use a block in this superblock (and increment this superblocks’ number of allocated blocks). Otherwise, allocate a new superblock and use a block in it Free # Push the block in the allocator’s cache. Recycling # For each block in the allocator’s cache :\nFind the superblock the block is part of. Push the block in the superblock’s cache and decrement superblock’s number of allocated blocks. Superblock collection # Any superblock with no allocated blocks can be freed. The slab is one of the simplest and most fundamental allocator as it is freestanding, all the metadata it uses can be stored in the superblocks or statically :\nthe allocator struct can be statically allocated. superblock headers can be stored at the start of the superblock, the region to allocate blocks in occupying the remaining part. cache nodes can be stored in the block they reference, a singly linked list requiring only one pointer. Trade-offs of the slab allocator # Only allocates one size. No block header -\u0026gt; no extra memory consumption. Fastest possible allocation and free. Causes superblock internal fragmentation if only few allocation requests are made compared to the number of blocks in a superblock. Memory recycling is slow, as it requires reassigning each block to its originating superblock. Superblock free depends on memory recycling and so, is time-consuming and should not be done frequently. Visual representation of the internal structure of the slab.The diagram shows a system with two allocated superblocks, each one with its own non-allocated zone in yellow, and multiple allocated blocks in red, and multiple blocks in the allocator’s cache (in cyan), and multiple blocks in their slab’s cache. The rightmost superblock’s non-allocated zone could be enlarged. Slabs # Now that the first and simplest brick of our secondary allocation library has been defined, the next easy step we can take is to use an array of them. In this array we will be allocating multiple slabs of consecutive sizes.\nThere are e two types of slab arrays we can define :\nSize increment array : slab at index i in the array allocates blocks of size: \\(base size + (i * size step)\\). Order increment array : slab at index i in the array allocates blocks of order: \\(base order + (i * order step)\\). Visual representation of the nuance between size and order increment. With \\(base size\\) and \\(size step\\), or \\(base order\\) and \\(order step\\) being parameters of the resulting allocator.\nWhen a request for memory is received by the slab array’s handler, in order to get the appropriately sized memory, this request must be forwarded to the appropriate slab. In order to meet the constraint on the size of the requested memory the slabs array only forwards allocation and free requests to the slab allocating blocks of the smallest greater or equal size. This imposes the additional constraint that the size of the block must be provided both at allocation and free time.\nTrade-offs of the slabs array # Only allocates blocks of a size range. No block header -\u0026gt; no extra memory consumption. Size must be provided at allocation and free. Very fast allocation and free (time of the selection of the slab and the slab operation). Freestanding, if the amount of slabs of the array is known at compile-time. Heap # The secondary allocators described in the previous sections are very time-efficient, but suffer from the issue of superblock fragmentation that I have described before : once a superblock is allocated, it can only serve as a memory reserve for blocks of a fixed size. If I allocate a block of size 2 only once and never again, and free it in a long time, it will cause a whole superblock to be allocated for an used amount of 2 bytes, thus causing a loss of 4094 (if 4KiB pages), the superblock being dedicated to this size.\nIn ranges at which memory allocation is sparse, an allocation strategy that would allow a superblock to be used as a memory source for blocks of different sizes could be desirable.\nThis is the reason the Heap allocator exists.\nNote: the name “heap” is ambiguous, as heap may be related to different concepts :\nA segment in the classic process user space layout, possibly used by the allocator implementing malloc/free. A referencing design pattern (see binary heap). This kind of allocator. The naming similarity may come from the fact that implementations of the allocator I will describe may, in the past, have used a binary heap data structure to reference their free blocks, and the aforementioned segment as primary memory source.\nA quick look at the GNU allocator will reveal that the use of a binary heap is deprecated, and modern allocators tend to rely on mmap more than on brk/sbrk (syscall that update the heap segment) to get their superblocks.\nThat being said, I will, more by convention, keep on using the name of Heap allocator.\nThe solution used in this allocator to address the problem of allocating blocks of different sizes in the same superblock consists of :\ntrack consecutive blocks of the same superblock in a linked list headed by the superblock. At the allocation of a superblock, a single free block occupying all its available space is allocated. track free blocks by size in the allocator. Multiple methods can be used, the ones deserving to be mentioned being the usage of a binary heap (may be unwise because metadata / time consuming), or of a cache in the form of a set of linked lists, each one referencing free blocks of a particular size. This method is used by the dl (Doug Lea) allocator, the allocator that the gnu memory allocator is based on. support split of a free block into two blocks of inferior sizes. support coalescing of two consecutive free blocks into a larger block. In order for this to work, each block must at least have a header containing data to reference it in the list of blocks of its superblock. This form of referencing implicitly contains the size of the block, thus providing the interesting functionality to free a block without knowing its size : given a pointer to the start of a block, simply subtracting the header size to its provider the header pointer, which provides the block size.\nThis is the reason why the free function from the C standard library doesn’t take the block size in argument.\nAnother piece of information that the header should contain is either the binary heap node or the linked list node that references the block in the allocator by size when it is free. Though, we can remark that this node will only be used when the node is free, thus, when the block’s memory is unused. A smart optimization is then to use the block’s memory to contain the node. It implies that the minimal allocation size of the allocator is greater or equal to the size of this referencing node.\nThe allocation algorithm works as follows :\nAllocation (size \\(N\\)) : search for a free block of size \\(M \u0026gt;= N\\) in the cache. If the remaining size \\(M — N\\) can contain a block header and a memory block of the minimal size, split the block in two, one of size \\(N\\) and the other occupying the remaining part. Then return the first part of the split block if split, or the selected block otherwise. Free : retrieve the block header, and attempt to merge it with its successor and its predecessor if they exist and are free. Then, insert the resulting block in the allocator’s cache using its size. The coalescing part (merge with neighbour) may be costly and so can be delayed, but may result in external fragmentation, as shown in the next diagram, leading to fragmentation.\nVisual representation : showing an allocation / free scenario of blocks of size N in a superblock of size 3 * N (with headers ignored for the sake of simplicity). Free areas are represented in green, allocated areas in grey. Numbers at the bottom represent the time line, referred to in the article as “steps”. In steps 1 to 3, all blocks are allocated.\nThen, in steps 4 to 6, they are freed in a different order, with the middle block at last, and not coalesced, keeping 3 free blocks in cache. Then, an allocation is done again at step 7, which, depending on the caching mechanism, may provide the middle block.\nThough the amount of memory is available, an attempt to allocate a block of size \\(2 * N\\) will fail, as no contiguous block of this size is available.\nThe same scenario with direct block coalescing is presented in steps 6’ and 7’, and solves this issue.\nTrade-offs of the heap allocator # Supports all sizes that fit in a superblock. Supports block allocation and free. Block size is not required at block free. Block header -\u0026gt; extra memory consumption. Block header -\u0026gt; no alignment guarantees except for the fundamental order. Aligned allocation is possible but requires more metadata or allocation time. Allocation and free are slower than other methods due to block split and coalescing. Visual representation of the internal structure of the heap. Stack # The complexity of the heap allocator comes from its requirement to support the per-block free operation. Due to this constraint, it must track blocks per superblock in order to coalesce them, and reference free blocks in order to reuse them.\nFor some use cases, the free per-block operation may not be necessary, only a global free or a free of all blocks allocated after a particular moment. This is the base idea of the stack allocator, a very simple and fast allocator. It will receive and track superblocks in allocation order, the last one being the one with available free memory, while all blocks afterwards are considered full. Each superblock tracks its non-allocated zone like slab superblocks, and allocates blocks at the start of their non-allocated zone.\nThe stack allocator’s algorithm is the following :\nAllocation (size \\(N\\)) : if the last superblock has enough free memory, allocate a block at the lowest possible location in it. Otherwise, tag the superblock used, allocate another and allocate in it. Get-position : return the insertion position of the last superblock. Free (position) : find the superblock the position belongs to, free all further superblocks, set the non-allocated zone of the superblock from the position to the superblock’s end, set the superblock as the last superblock. Trade-offs of the stack allocator # Supports all sizes that fit in a superblock. Block free not supported. Atypical allocation method. No block header -\u0026gt; no extra memory consumption. No block header -\u0026gt; aligned allocation can be easily achieved. Fast allocation. Secondary memory manager # Now that we overviewed this basic set of secondary allocators, let’s focus on the design principles of a secondary memory manager.\nThe allocators that we have described all have their trade-offs in terms of operations support, speed and overhead, and as detailed in part II of this article, secondary allocation can be split in different types :\nsmall blocks : \\([1B, cache line size ]\\) : frequent use (96% of allocations), many use of all many allocation sizes, must be fast, no superblock allocation issue due to the frequent usage of all sizes. intermediate blocks : \\(]cache line size, page size]\\) : infrequent use (4%), uses a few allocation sizes, can allow trading performance for avoiding superblock fragmentation. For small blocks, the most efficient choice to make seems to be the use of a slab array with a small size step of for, based on our use case data, 8 bytes. The allocation will then well fit our program’s expected behavior, and as stated before, superblock fragmentation will not be a problem, as the huge amount of allocation will hugely reduce the probability of the rare use of a particular block size.\nThough, if superblock fragmentation becomes a concern for the designer, given a different program behaviour, the size step of the slab array can be increased, reducing the total number of slabs and concentrating multiple allocation sizes on one slab. This will, though, increase the internal fragmentation of blocks. Again, this is a trade-off that depends on the target system’s behaviour.\nFor intermediate blocks, the strategy to employ is more dependent on usage constraints.\nIf the memory manager is the one of a kernel, that is, a system with a large amount of physical memory available, superblock fragmentation may not be an issue, and the designer may choose to use also a slab array for intermediate blocks. But this time with an order step to avoid an enormous and unnecessary amount of possible block sizes.\nIntermediate block sizes will then be rounded up to the closest power of two, and a block of this size will be allocated by a slab. This provides fast allocation time, with low superblock fragmentation (only 5 orders to allocate in a system with cache line order 6 (64B) and page order 12 (4KiB)) but with high block internal fragmentation, due to the rounding to the higher power of 2.\nAnother advantage in this strategy is that in this configuration (block of size power of 2) the slab can be tuned at low costs to provide blocks aligned on their size, which guarantees that any intermediate block is aligned on its own size.\nIf the memory manager, on the other hand, is made for a system with a small amount of memory, or for a system with a small number of allocation (ex : process private memory allocator), this choice may not be the appropriate, as alignment guarantees may not be necessary, and intermediate blocks being allocated sparsely. In this situation, using a heap allocator for the intermediate block range may be a better solution, as it mutualists superblocks among all supported block sizes, and avoids internal block fragmentation, though causing a little memory overhead per block, neglectable for this size range.\nPage blocks tracking # One of the interesting features of the secondary memory manager we described in the previous section is to be composed only of sub-allocators that manage their superblocks by themselves.\nPut differently, if required, all allocated memory can be freed at once, by simply freeing superblocks used by each sub-allocator.\nA corner case, though, concerns the allocation of page blocks, that is delegated directly to the primary allocator. Tracking those allocated page blocks allows to also free them if necessary, extending the aforementioned feature to every allocation type, providing a fully functional and covering secondary allocator.\nThis causes a small memory overhead (at most one cache line per page size) neglectable compared to the size of a page (\u0026lt; 2%) , due to tracking metadata that must be allocated separately. But this can be allocated internally by the secondary memory manager, thus adding no structural complexity to the defined model.\nConclusion # This chapter concludes this series on memory management.\nAfter reading this, you should have acquired a solid understanding of the mechanisms involved in system memory management, that transparently occur when an end-user requests a memory allocation or free.\n","date":"21 September 2021","externalUrl":null,"permalink":"briztal.github.io/projects/memory_management/mm_2/","section":"Personal Projects","summary":"Secondary allocator","title":"Memory Manager : Secondary allocators.","type":"projects"},{"content":" Introduction # In the previous chapter we defined the base characteristics of the system managed by a memory manager. Additionally, we also defined the primary memory manager as the sub-system of the memory manager responsible for :\nreferencing primary memory blocks. allocating page blocks in these primary memory blocks. The primary memory manager’s allocation system is required to :\nbe stand alone and not depend on any external allocator to store it’s allocation metadata. support both allocation and free. avoid internal fragmentation, i.e. allocate more memory than needed. avoid external fragmentation, i.e. coalesce contiguous free regions when possible. This chapter aims to complexify and provide a more detailed look at the system the primary manager has to work in by introducing the concept of memory zone, as well as developing the previously introduced concept of memory node.\nThen, we will present the base algorithm for managing the primary memory for a single zone of a single node.\nFinally, we will conclude by a detailed diagram summarising the memory manager’s architecture.\nCredits : Photo by Hanisch from FreeImages DMA and the need for memory zones # In the first chapter we described physical memory as an uniform resource with no other differentiating characteristics apart from it the NUMA node dimension. This was an oversimplification, and in order to better explain how things really work we need to introduce the concept of Direct Memory Access.\nSome peripherals like UART may rely exclusively on memory-mapped registers to receive or send data to and from the operating system. In these cases, they may have a small internal buffer accessible through these memory mapped registers, that the OS must periodically read/write to.\nThis read/write sequence is often executed in an interrupt sequence, triggered by the device when it has transmitted or received data.\nThough this sequence may be suitable for embedded systems where the relative temporal cost of an interrupt is not that high (note to the reader : I am not saying the cost is low), with low bandwidth devices that do not force the OS to permanently poll the device, it clearly shows its limits in modern systems where the cost of an interrupt is non-neglectable, or with devices with a higher bandwidth.\nTo solve this issue, communication peripherals now present Direct Memory Access (DMA) capabilities that give them the means to directly read/write the data they receive/send to memory.\nTheir memory operations are done in physical address space, they directly (or indirectly in the presence of an IOMMU, which we will not go into as this is out of the scope of our topic) access the physical address space.\nThe Operating System has to allocate primary memory buffers that these devices can use, and notify them that they must read/write to those buffers.\nThe device now has the liberty to schedule its reads/writes depending on its own constraints, and will only wake up the OS when it needs new buffers.\nBut this raises an issue though : the physical address space may be a 64 bits address space, and some devices may not support it : their accessible addresses will be 16 or 32 bits long. The OS nowhas to ensure that primary memory allocated for peripheral DMA is effectively in the target periperal’s accessible address range.\nFor this reason (there are other use cases, for example Linux’s HIGHMEM zone), the concept of memory zone emerged. It simply consists of considering not one homogeneous set of primary memory blocks, but rather considering multiple sets, or zones, each one containing memory suited for one type of usage.\nLinux systems generally exhibit the following zones :\nDMA16 : primary memory with 16 bits addresses. DMA32 : primary memory with 32 bits addresses. NORMAL : primary memory used by regular software. HIGHMEM : primary memory that cannot be permanently mapped in kernel space. Each zone will have its own primary memory allocator.\nAdditional details :\nDescribing Physical Memory | The Linux Kernel documentation Exploring the HIGHMEM concept : ndocumentation\nWhat are high memory and low memory on Linux? | Unix Stack Exchange An end to high memory? | lwn.net NUMA and the need for a memory manager per node # As defined in the previous chapter, a node is a conceptual set containing memory banks and cores. The respective nodes of a core and a memory bank are the only information the OS uses to determine the latency of the access from a core to the memory bank.\nThis latency information is fundamental for the primary memory manager, as its role is to provide memory to all different cores. Its objective will be to provide to each core primary memory that will be optimized for the smallest access latency. All whilst respecting external constraints such as the zone the primary memory must be part of.\nTo achieve this, the memory manager must be decentralised, and each node will have its own memory manager.\nThis node memory manager will have a set of zones, and each zone will have it’s own primary memory allocator. Each core will know the node it belongs to, and so, will query the related node’s manager for allocation / deallocation of primary memory. The node manager will then transfer the request to the primary memory allocator of the related zone.\nMore details on NUMA :\nWhat is NUMA | The Linux Kernel documentation Non-uniform memory access | Wikipedia Understanding NUMA Architecture NUMA Fallback # The procedure described in the previous paragraph raises the question : what if the selected zone allocator runs out of memory ?\nOne of the constraints of the NUMA system that we have described was that each core in the system could access each memory bank. Only the access latency varies.\nFollowing this constraint, all the allocator has to do is to select a compatible fallback zone allocator and forward the allocation request to it.\nThis fallback zone could be determined at allocation time, but, as the constraints that would help guide this choice are entirely known at compile-time, this might not result in the most optimized solution. As you might already have guessed, these constraints are :\nmemory latency, purely node-dependent. zone compatibility, known by design : DMA16 should be compatible with DMA32, DMA32 with NORMAL etc… As a consequence, it is possible to define this zone that the fallback allocator should use both at startup or compile-time.\nLinux, for example, keeps for each zone of each node a constant array of fallback zones references, that can be used when the zone’s allocator runs out of memory.\nProblems really arise when all of these fallback zones also run out of memory. We are left in a bind as the memory manager cannot invent memory : if all of our allocators end up starving, the system should crash, at least in a controlled way.\nControlled emergency landing : JetBlue Flight 292 makes an emergency landing at Los Angeles International Airport. Additional details ( if you dare ):\nmmzone.h - include/linux/mmzone.h - Linux source code (v5.14.6) | Bootlin mm_types.h - include/linux/mm_types.h - Linux source code (v4.6) | Bootlin Primary memory dispatch # When the system boots, primary memory blocks positions and sizes are either :\nRead in a static descriptor of some sort (static memory block boundaries, device tree, …). Provided by a host or hypervisor. The node itself doesn’t impose constraints on the location of its primary memory, only the zone may impose such constraints.\nThe memory manager must then forward it to the primary allocator of the zone that manages it, either by :\nexplicit request when the zones don’t have position constraints by address calculation otherwise Anyway, the zone primary memory allocator only ends up with blocks of memory that concern it.\nPage cache # Pages are the base granule for address space mapping. When a process requires any type of memory (stack, heap, file mapping, memory mapping), it calls the kernel so that the latter can allocate it the required number of pages and then configure the virtual address space to map a contiguous block of virtual memory to these allocated pages.\nNote : pages being an important resource, the OS may adopt different strategies to reduce unnecessary page consumption, one of them being deferred allocation : the OS will at first allocate a block in the user’s virtual address space but map it to nothing. Later, when the user accesses this unmapped memory, it causes a translation fault. At this moment, the OS will allocate pages for this accessed location and effectively do the mapping. This allows the OS to allocate pages only if they are used. To illustrate how useful this is, we take the example of a user requiring 256 MiB of memory and not accessing it afterwards. With such a system in place, we have simply prevented any wasteful page allocations and saved ourselves 256MiB of memory.\nIf a page is used by a core, because of how the core’s cache system exploits temporal and spatial memory locality, it is likely that it already has some content of this page in cache. Once this page is freed for any reason, subsequent allocations in this core have a significant advantage in reusing this page. Otherwise, the cache system’s replacement policy will move in new cache lines and discard unused ones, but, there is a possibility this might not immediately concern the recently freed page. Reciprocally, a different core would have little interest to use this page given the choice, as it would cause the related cache lines to be shared among caches of those cores.\nAs a consequence, it may be interesting for the memory manager to implement a per-core page cache sorting pages by recency to exploit this temporal locality to its fullest : a core requiring a page now looks into its page cache and if empty, allocates a page using its node’s allocator. When the core frees a page, it puts it in its cache for future reuse, and eventually frees the oldest page in cache if the cache is full.\nPrimary memory allocator : the buddy allocator # Now that we have described in detail the objectives of the primary memory allocator, let us give a more practical example with a description of one of its possible and most widely known implementations : the buddy allocator.\nTo solve the primary memory allocation / free problem efficiently, we will first add two constraints :\nthe allocator must only support allocation and free of page blocks containing a number of pages that are a power of two. This is the core characteristic of the buddy allocator algorithm. the size of the block must be provided at both allocation (void *alloc(size_t size)) and free (void free(void *ptr, size_t size)). This will avoid the need to store block sizes alongside the block as metadata. A solution to this problem with these constraints is provided by the well known Buddy Allocator algorithm.\nThis implementation is the one used by linux, among others.\nAs the Buddy Allocator has already been very well described in many other articles, as such, I will only briefly describe its working principles and provide adequate references as to where to find more detailed explanations.\nThe main working principles behind this algorithm is to : define valid blocks as blocks of size \\(2^i\\) page, aligned with their own size. observe that given a block with two neighbours of its own size, there is only one other block that it can be merged with to form a valid block of the closest greater size. We call this block its ‘buddy’. reference valid free blocks by order \\(log2(size)\\) in a cache, i.e. a linked list array : the list at index i references valid free blocks of size \\(2^{(i + page order)}\\). Free blocks are used to contain their own metadata, including the linked list node that references them. When a new primary memory block is received, divide it into a set of valid free blocks, as big as possible, and reference these blocks in the cache. If a the allocation of a block of size \\(2^i\\) pages is required :\nIf a valid free block of this size is available, use it. Otherwise, if a valid free block of size \\(2^j\\), (j \u0026gt; i) is available, split it into a set of smaller valid free blocks, of which a valid free block of the required size, use this block and reference other sub-blocks in the cache. Otherwise, the allocation fails. If the free of a valid block of size \\(2^i\\) is required :\nIf its buddy is free, merge them and reiterate with the resulting valid block. insert the resulting block in the cache. The status (allocated / free) of a block’s neighbours can be obtained easily with no dynamic metadata by using a bitmap containing for each valid block, the status of the block (free, allocated). The size of this bitmap depends only on the number of pages \\(N\\) of the primary memory block \\(N\\) pages, \\(N/2\\) valid couples, \\(N/4\\) valid quadruples, etc…), and its allocation is not a problem, as the primary memory block can have its end truncated to contain it without causing an alignment (and so external fragmentation) problem for the remaining area (it would be the case if the bitmap was put on the start of the primary block).\nDocumentation :\nThe Buddy System Algorithm | Linux Kernel Reference Buddy memory allocation | Wikipedia Secondary allocation # Though the detailed overview of secondary allocation algorithms is reserved for further chapters, it is necessary to mention it here to better understand where it fit’s in the whole memory manager.\nWhereas the primary memory allocator is in charge of the allocation of pages and larger blocks, the secondary allocators handle the allocation of small and intermediate blocks. This secondary allocator will use memory provided by the primary memory manager. This secondary allocator is node-centric : each node will have a secondary allocator using the node’s NORMAL zone as a superblock provider.\nMemory manager architecture # The architecture that we described in this chapter is summarised in the following diagram with an example.\nRepresentation of a simple memory manager system including both a primary and secondary allocator. The example consists of a system with 4 cores and two nodes, cores 0 and 1 belonging to node 0, core 2 and 3 belonging to node 1.\nEach node has three memory zones, DMA16, DMA32 and NORMAL.\nOnly the NORMAL zone of the node 0 has fallback zones, namely DMA32 and NORMAL zones from the node 1.\nEach zone has its own primary memory allocator.\nEach none has its own secondary allocator, using primary memory provided by the node’s NORMAL zone.\nEach core uses the primary memory provided by its node, which forwards the allocation request to the required zone.\nConclusion # After this chapter, we have provided both a high-level view of a memory manager supporting the constraints of the system we defined, and a presentation of one of the most known primary memory allocation algorithms.\nThe next chapter will focus on the problems and algorithms involved in secondary memory management, thus achieving the overview of the memory manager.\n","date":"19 September 2021","externalUrl":null,"permalink":"briztal.github.io/projects/memory_management/mm_1/","section":"Personal Projects","summary":"The primary memory manager","title":"Memory Manager : Primary allocators.","type":"projects"},{"content":"Since this series of articles discuss memory, let\u0026rsquo;s first define the concept of order of magnitude, which will be used as a more relevant metric than size to differentiate memory blocks.\n$$ size = 2^{order} $$\nAn order increment multiplies the size by 2.\nIntroduction # Memory is one of the most important resources of any running program, but is perhaps also one of the least correctly understood by programmers.\nIn most modern languages, an effort is made to take memory management out of the programmer’s hands.\nEven in low level languages such as C or Rust, memory management, though being accessible to the programmer, is confined to the use of malloc/free-ish primitives, implemented in the language\u0026rsquo;s standard library (ex : glibc).\nThe underlying memory managers and their implementations are meant to be invisible to the user, which may explain why so many developers are so unfamiliar with the different memory management techniques, and their associated costs, trade-offs, and efficiencies.\nThis series of article will take the opposite direction, and provide a simplified overview of different memory management techniques, their objectives, trade-offs, and explore how they interact with each other.\nIn this, we will cover the basics needed to properly understand memory managers, their design and performance.\nThe theory and designs that I will introduce in this article are based on my study of different kernels (linux being on the list), and on my own implementations and trials.\nImage credits : Photo by pipp from FreeImages The memory bottleneck # As software developers we tend to view memory as an uniform resource, with a constant and negligible access time.\nI once asked a group of (computer science) students which type of instructions takes the longest time to complete among :\nmemory accesses (read / write). computational operations (int / float sum / diff / div). control flow operations (conditional (or not) jumps. They all chose computational operations, which is not surprising given what we are taught at school, and also considering the fact that in a program, memory accesses are simple enough to write (if not just transparent to the programmer) but computational and control flow operations are what most of the code will be about.\nHowever, this is also incorrect in most cases.\nMemory is the major bottleneck of modern computer micro architectures, and has been so since CPU cycle time and memory access time diverged, back in the 90s.\nCache # To mitigate this memory access bottleneck, modern CPU feature a transparent multi-level cache system.\nCache levels store and exchange contiguous memory blocks called cache lines. Cache lines have a constant size and are aligned on this size. The cache line is the smallest granule a cache can request memory on, and as a result, is the minumal granule at which all operations are executed from the external memory\u0026rsquo;s standpoint.\nHence, on a CPU with empty caches :\na memory read will cause a read of the entire cache line from memory to the cache system. a memory write will cause a read plus at some point a write of the entire cache line to the memory system. any operation requiring exclusivity on a memory location, like an atomic operation, will require the exclusive ownership of the whole cache line by the CPU\u0026rsquo;s cache Additional details :\nReducing Memory Access Times with Caches | Red Hat Developer How L1 and L2 CPU Caches Work, and Why They\u0026rsquo;re an Essential Part of Modern Chips | ExtremeTech Page # Modern systems are equipped with an MMU (Memory Management Unit) that provides, among others uses, memory access protection and virtualization capabilities.\nIn such systems, two different address spaces are to consider :\nvirtual address space : the address space accessed by programs (processes or kernel). Appart from the very early stages of boot nearly all code is using virtual address space. physical address space : the address space in which hardware devices are accessible at determined addresses, this includes DRAM, peripherals like UART, USB, ETHERNET, Bus interfaces like PCIE, etc… The relation between virtual and physical address relies on two concepts :\nthe partition of both virtual and physical address spaces in blocks of constant sizes called pages (typical smallest size of 4KiB), aligned on their size. Virtual pages are called pages, physical pages are sometimes referred to as frames. the definition by software (using a format defined by the hardware) of a set of translation rules, that associate at most one physical page to each virtual page, called a page-tree or page-table. The MMU uses this informatuon to translate a given virtual address into a physical address. The minimal granule at which translations can be decided is called the page order \\((log2(page size))\\) and is a hardware constant.\nThough, the operating system may require, for its own reasons, to work on a larger base block.\nPaging and Segmentation | Enterprise Storage Forum The Translation Lookaside Buffer (TLB) | Arm Developer Access Time # To estimate the time of a memory access, it is necessary to estimate the access time for all cache levels. And to estimate the average access time in a program, it is necessary to know the size of all cache levels and have some notions of each of the cache’s replacement policies, to be able to guess at which level the access might hit.\nAs such, this estimation is highly unreliable, a better estimation can be made by profiling the execution of the code or by using tools like cachegrind that simulate a cache hierarchy. Though still not exact, these can help the developer in making its software design choice and help evaluate the chosen development paradigms.\nTo illustrate how wide the differences in memory access times are between cache levels, let me provide indicative values for their access time in CPU cycles and typical size. These numbers are not meant to be taken literally, but rather, serve to show orders of magnitude.\nRegisters : ~ 256B, ~1 cycle. L1 cache : ~ 64KiB, ~ 4 cycles.\nL2 cache : ~ 512KiB, ~ 20 cycles.\nL3 cache : ~ 4MiB, ~ 100 cycles.\nRAM : ~ 8GiB, ~ 200 cycles.\nThe following thread provides a more detailed comparison to a real CPU benchmark :\nApproximate cost to access various caches and main memory | Stack Overflow With this in mind, in order to make a program as efficient as possible, the two fundamental rules will be :\nTo reduce as much as possible the amount of memory accesses. If possible, increase spatial and temporal locality, i.e. to generate accesses at memory locations close to previous ones and not long after their use. System constraints : NUMA # NUMA (Non Uniform Memory Access) is a qualifier for a system where the physical memory access latency for a same address depends on the core the access originates from.\nFor example, in such systems, there may be multiple primary memory banks, located at different places in the motherboard, and multiple cores, also located at different places.\nTo simplify the management of these systems, we can conceptually group memory banks and cores in sets that we will call node (linuxian terminology), the node of a core and the node of a memory bank being the only factors taken into account when considering the latency of the core/bank access.\nIn all this series, we will place ourselves (and our allocators) in a NUMA system.\nie: a system with :\nOne or more cores, i.e. processing elements, cpus… One or more blocks of primary memory, i.e. primary memory that is provided as-it in the system (eg : DRAM), mapped in kernel address space and accessible by all cores of the system, but with a latency that may depend on the core (see node below). One or more nodes. In this system, the first objectives of our memory manager is to :\nReference primary memory blocks of each node. Provide a primary memory allocation / free interface to each core. Ensure that memory allocated to a CPU is as close as possible to this CPU. Additional details on NUMA :\nWhat is NUMA ? | The Linux Kernel documentation Non-uniform memory access | Wikipedia Understanding NUMA Architecture | LinuxHint Fundamental orders # Here are some orders to keep in mind when discussing memory management :\nByte : order 0, constant. Pointer : order ~ 2 or 3 (= 4B or 8B), depends on the size of the virtual address space. Cache line : order ~ 8 (= 64B), depends on the cache management system. Page : order ~ 12 (= 4KiB), depends on the granularity of the page-table. Primary memory block : order ~20 to 30 or more (1MiB -\u0026gt; 1GiB or more). Visual representation of the fundamental orders. Allocation types # With those orders in mind, let\u0026rsquo;s consider the different allocation types, to help us classify allocators and the objectives :\nsmall blocks (\u0026lt; cache_line_size) : frequent allocation, used for a huge part of object allocations (average size of malloced structures = 60 bytes), must be fast, small blocks must respect the fundamental alignment. intermediate blocks ( \u0026lt; page_size) : least frequent allocation, used for larger structs, may not be fast. Intermediate blocks must respect the fundamental alignment. page ( = page_size) : used by the kernel to populate page-tables, usage by secondary allocators as superblocks, must be very fast. Pages must be aligned on the page order. page blocks : contiguous page blocks, used by secondary allocators, less frequent. Page blocks must be aligned on the page order. Physical memory management, 🐔 \u0026amp; 🥚 problem # Physical memory management is a difficult problem, because it cannot require the use of any other allocator, as it is THE first allocator, the one managing physical memory, which is provided as-it.\nIt cannot use per-page or per-page block metadata inside those pages (or page blocks, blocks), but it split the block of physical memory that it manages into two :\nallocatable pages. metadata for the allocatable pages. Let\u0026rsquo;s note two things first :\nAllocating a page is a subproblem of allocating a page block. An allocator able to provide page blocks can be used as a memory source for memory allocators that allocate smaller blocks. Hence, we can define two types of memory allocators :\nPrimary allocators who manage primary memory blocks, and that allocate page blocks in those. Secondary allocators who divide page blocks provided by primary allocators into smaller blocks that they allocate to their users. The anatomy of a primary allocator, (the buddy), and of several secondary allocators will be detailed in the next chapter.\nCrossing accesses # I stated before that the base granule for memory operations is the cache line.\nNow, let’s look the following C code, assume that compiler optimisations and registers do not exist, and try to predict what accesses will be generated by the assembly produced by the compiler :\nint main() { /* Part 1. */ uint8_t i = 5; uint64_t j = 5; /* Part 2. */ uint8_t t[16] __attribute__((aligned(8))); *(uint64_t *) (t + 1) = 5; } In part 1, the compiler will generate two memory writes instructions targetting the stack (no registers) for i and j : i will be stored using a 1-byte write, and j will be stored with an 8-bytes write.\nIn part 2, we define char array of 16 bytes on the stack, whose first element’s address is aligned on 8 bytes (__attribute__(…)). Let its address (dynamically determined in a real world example since the stack is used to store local variables) be 56 for our exmple. To perform the write, the compiler to generate a 8-bytes write at addresses [57, 64].\nFor the record, it is explicitly mentioned in all the C language standards that this type of pointer casts has an undefined behavior.\nNow if we suppose that the cache line size of our system is 64 Bytes, we just generated a memory write that spans across two cache lines : the first part from 57 to 63 (on cache line ranging from addresses 0–63), the second on 64 (on cache line ranging from addresses 64–127), this type of access being infamously known as an unaligned access.\nUnaligned accesses may carry a severe penalty and are generally something to avoid generating, as it may lead to undefined behaviors. The example we described was fairly simple, and the answer “ yeah I don’t care generate two accesses instead of one ”, could fly. But supporting this in HW may make CPU designers unhappy.\nBut if an access can cross cache lines, it can also cross a page boundary and can involve pages of virtual memory, which could be mapped with different access privileges and cacheability (another attribute held by the page table).\nWhat should the CPU do in this case ? Fault ? Generate two accesses on the different cache lines with different attributes ? CPU designers just went from \u0026lsquo;unhappy\u0026rsquo; to \u0026lsquo;complaining\u0026rsquo;.\nWorse case : one of the two virtual page could be mapped, and the other could be non-mapped.\nWhat should the CPU do in this case ? Perform none of the accesses ? Perform the valid part of the access, and not the other ? CPU designers just transitioned to \u0026lsquo;angry\u0026rsquo;.\nWorse, the access could be an atomic instruction that requires the exclusivity on one cache line, which is a hell of a heavy work for a CPU.\nWhat should the CPU do in this case ? Fault ? Require the exclusivity on two cache lines and split the access ? CPU designers just transitioned to \u0026rsquo;trying to find out where you live\u0026rsquo;.\nSome architectures like the old ARM processors (ARMV4T) did not support unaligned accesses, others tolerated it and optionally trapped them, or profiled them so they could be reported to the respectful developer.\nThe safest certainty we have is that they should be avoided if at all possible. As a matter of fact, the C compiler never generates any unaligned memory access.\nExcept when it does.\nI once had an unpleasant experience dealing with a version of the C standard library that had some functions that generated such accesses. For the record, those functions were memcpy and memset, and they generated unaligned accesses by uncarefully extending the read / copy / write granularity without checking the alignment of source/dest addresses.\nAdditional reading :\nUnaligned accesses in C/C++: what, why and solutions to do it properly | Quarkslab\u0026rsquo;s blog Unaligned Memory Accesses | Linux Kernel Documentation Impact on the memory manager # The key takeaway to be remembered from the last paragraph is that unaligned access cause perf and power hits and therefore should be avoided.\nThis means that any object legitimately instantiated in memory should not generate unaligned accesses.\nFor example, an instance of the following struct allocated with our memory allocator\nstruct s { uint32_t x; uint8_t y; uint32_t z; }; should not generate any unaligned accesses when accessing any of its fields.\nThe C compiler (unless manually instructed to do so by the snarky developper) will add padding to this struct the following way :\nstruct s { uint32_t x; uint8_t y; uint8_t pad[3]; uint32_t z; }; But for every field to be accessed without unaligned accesses, when providing a memory block B, an allocator must ensure that B\u0026rsquo;s start address meets alignment requirements for any primitive type that can be placed in an aligned manner in B.\nEg \u0026quot; in 64 bits systems, a block of size 1 can be placed everywhere in memory, a block of size 2 start at an address that is a multiple of 2, a block of size 4 must start at an address that is a multiple of 4, a block of size 8 or more must start at an address that is a multiple of 8._\nIf you wonder, the documentation of the malloc C function, states a similar constraint :\n“If allocation succeeds, returns a pointer that is suitably aligned for any object type with fundamental alignment.”\nImpact of fundamental orders on the allocator # The different orders we defined to characterise our memory system may vary among different systems, their variations having an impact on the memory manager and the amount of allocated memory for a same program:\nByte order : 1, invariant.\nPointer order : has a direct impact on the amount of memory used by a program.\nIndeed, when reading the source of large C projects, linux being a good example, it is easy to notice that most data structures are packed with pointers. A variation of the pointer size will cause a similar variation on the size of those structs.\nCache line order : has an impact on the secondary memory manager.\nIndeed, the cache line is the base granule on which synchronization instructions operate on modern CPUs : when you execute an atomic operation, the core’s memory system requires the exclusivity on the cache line, then executes your operation, then confirms that the exclusivity was held during the whole operation.\nFor this reason, it is important that memory allocated to an end-user that may use synchronization instructions on it, is allocated on the granule of a cache line (and aligned on the cache line size). This is needed to avoid a situation where two users could, for example, attempt to lock two different spinlocks, present in two different structures each one of size cache_line_size / 2, but allocated on the same cache line by the memory manager. In this case, when trying to acquire the spinlock and requiring the exclusivity of the cache line. Each of the two users will be blocking the other’s ability to acquire the other spinlock, thus creating additional, unnecessary and expensive memory access conflicts.\nPage size order : has a possible impact on internal fragmentation of secondary allocators, and on efficiency of secondary allocators.\nSecondary allocators, as stated before, manage superblocks provided by primary allocators, and use those to allocate smaller blocks. Increasing the page order will increase the minimal superblock size, and so, the amount of primary memory allocated at each superblock allocation.\nNow let’s say that a secondary allocator is constructed and receives a single allocation request for 64 Bytes of memory. A superblock will have to be allocated, and will be used to provide the 64 Bytes.\nNow if no more memory is required from this secondary allocator, the whole superblock will still be allocated and not usable by other software. This could cause a large external fragmentation.\nNow, increasing the superblock size will also decrease the rate at which superblocks will be allocated, which will slightly increase the allocator’s efficiency.\nVisual representation of size differences between fundamental orders. Conclusion # This chapter will have stated what I believe are the most basic things to keep in mind when speaking of memory managers.\nIn the next chapter, we will be focusing on the structure of the primary and secondary memory managers.\n","date":"17 September 2021","externalUrl":null,"permalink":"briztal.github.io/projects/memory_management/mm_0_into/","section":"Personal Projects","summary":"General considerations on memory and memory managers.","title":"Memory Managers : Introduction.","type":"projects"},{"content":"","externalUrl":null,"permalink":"briztal.github.io/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"}]