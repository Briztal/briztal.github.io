[{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/categories/c/","section":"Categories","summary":"","title":"C","type":"categories"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Context # In April 2023 my wonderful wife had the best birthday idea ever, and made me a custom development board based on an STM32H7 MCU.\nThe chip was rather cool and had an USB connector, an SD-card interface, and a JLink connector. It was all I needed at the time.\nAs it was my first time selecting a processor, I just cared about the CPU architecture, presence of caches, and clock frequency. I didn\u0026rsquo;t request a large flash or RAM size.\nThis detail ended up causing me some troubles, as I \u0026ldquo;quickly\u0026rdquo; (in a few months of adding code and developing a Kasan that needed a lot more code) filled the 128KiB of builtin Flash.\nI then decided to upgrade the CPU, and since new capabilities are never worth nothing, I attempted to design the devboard V2 myself.\nDuring this year of development, I also had written a custom USB stack and a driver to support ST\u0026rsquo;s USB implementation. And if it doesn\u0026rsquo;t sound hard, trust me, it is. ST\u0026rsquo;s USB module is the biggest pile of hardware crap I ever encountered. It had it all. Undocumented registers, hardware xprop, sudden freezes for no reasons, almost undocumented HW capabilities, pseudo-documented capabilities that aren\u0026rsquo;t actually implemented, name it and it had it. I bumped my head on the wall so much during the month and a half where I wrote that code, that the noise still resonates in the place that my now liquified brains used to occupy.\nBut that\u0026rsquo;s another story.\nAn USB stack I had, and I wanted to experiment with it. More precisely, I wanted to try using an USB-C connector instead of my old USB-B, and also wanted to give High Speed USB a try, because ST\u0026rsquo;s Full Speed USB wasn\u0026rsquo;t painfully broken enough.\nI also wanted to try and support the Ethernet, since there could be a lot more fun places where ST could fuck up, so why not visiting them all !\nI also wanted to experiment more on smaller peripherals, like I2C, UARTs et al., and I wanted dedicated connectors on the board for them because why not.\nPin multiplexing # An SoC is essentially one or more CPUs and multiple peripherals connected together via various interconnects.\nWhile CPUs are essentially processing data, peripherals sometimes use signals from the outer world. For example, an UART will have at least two signals, RX and TX, and likely have one control-flow signal per direction, which will add two more pins, CTS (Clear to Send) and RTS (Request to Send).\nThat\u0026rsquo;s already 4 signals, for a single peripheral. And that\u0026rsquo;s a rather low number of signals : memory access peripherals use data buses which use at least 8 signals (one byte), plus the various DIR/CLK signals.\nNow imagine that we have 10 UART peripherals on the chip, that\u0026rsquo;s already 40 pins used. Add the various SPI, I2C that may be present, the LCD peripherals, and so on, and you easily end up with hundreds or thousands of pins.\nA normal chip package doesn\u0026rsquo;t have that many pins : the LQFP-144, which is already a big one, only has 144 pins as the name states.\nIt simply cannot allow all peripherals to be used at the same time.\nHow is this not a problem then ? Well, simply because typical applications do not use all those peripherals.\nThanks to this, MCU manufacturers like ST have come up with a reasonable approach that we\u0026rsquo;ll call pin multiplexing : every pin of the package is internally connectable to multiple signals (but to at most one at a time). The GPIO driver has to configure which alternate function (which index of each pin\u0026rsquo;s multiplexer) is used. This is one of the first duties of any communication (USB, UART, etc\u0026hellip;) device driver of such a chip : configure the pin multiplexing so that the signals used by the driven device are actually connected to the chip\u0026rsquo;s pins. Then it can do other fun(damental) things like configuring the clocks and generating kernel panics.\nThe multiplexer can be large though. For ST\u0026rsquo;s H7 line, it is 16 entries wide, meaning that each pin can be internally connected to at most 16 signals.\nBut the fun goes the other way too : multiple pins can be connected to the same signal, which makes this connection system a good old nightmare-ish graph.\nTo see how deep out of fun-land we are, this is an excerpt from my chip\u0026rsquo;s multiplexing array.\npage 1/14 of the stm32h750vb pin multiplexer. This can be (painfully) translated into a machine readable text file that we\u0026rsquo;ll use later.\nHere is the one for my chip.\nFor the record, that took almost 3 days of part-time fun and cost me a few neurons.\nThe sytax is quite simple and doesn\u0026rsquo;t deserve a detailed explanation, it\u0026rsquo;s just a line by line description where the first non whitespace char determines the described entity.\nIt could be possible to directly get this information from the pdf using a pdf-to-text-like manner. Though I tried that for a couple of hours and quickly ran out of patience.\nThe problem # Now back to our original problem : a devboard I wanted to design. And I knew what I wanted :\n4 UARTs, among 8. 3 SPIs, among 6. 2 I2Cs, among 4. 1 CAN, among 2. 1 4 bit SD interface, among 2. 1 USB with an ULPI interface, only 1 in the chip. 1 USB with a standard FS interface, only 1 in the chip. 1 Ethernet MDC + RMII interface, only 1 in the chip. When I say \u0026lsquo;among\u0026rsquo; in the list above, I mean that there are many devices of a kind in the SoC, but that I only want a certain number of them. Those devices are, for all intents and purposes, strictly identical. Any of those can be selected.\nThe issue, as one could imagine, is that because the total number of pins is pretty low, and the number of signals is pretty high, there is a high chance of collision.\nNow, if you think about it, I could just have try-and-errored in Kicad until I found a valid solution : if I saw that one peripheral needed one pin that was used by another, I would have discarded the one with the smallest priority and repeated the process.\nAs a matter of fact, I did try that. And it gets old. Veeeeeeryyyyyy quickly.\nThat\u0026rsquo;s mostly because if you think about it for a moment, there are a LOT of possible combinations : let\u0026rsquo;s imagine that we know exactly every peripheral that we need (forget the M among N-s in the previous list), and that we have 16 of them. Every one of them has 4 signals, and each signal can be routed to 2 pins. We literally have 64 pins that each have two possible values, which makes a total of 2^64 possible configurations.\nAnd that\u0026rsquo;s a lot. More than what your computer can index.\nAnd doing that manually can be very long.\nSo after the first evening of failed attempts with the donk-ish manual way, I told myself it would be great to use the few remaining connected neurons to at least code something to accelerate the process.\nObjective # With a given problem (certain peripherals to select) there could be a lot of possible solutions.\nThe objective of the algorithm will not be to find all solutions, not because it would be a bad idea, but simply because it would potentially consume all disk space on the machine.\nIn fact, as the reader will see, the resolution method contains the procedure to find all possible solutions. It is just that in most cases, finding just one solution is enough.\nThus, the objective of the algorithm will be to find one solution. If for some reason, that solution is not what the user expects, then the algorithm will easily find the next possible solutions.\nThe expected output # Ideally, we would like to be able to iteratively test multiple solutions for the same problem, in the same design.\nTo do that, we would like our algorithm to modify the connections in our Kicad schematics.\nThis is not doable per-se, but can be accomplished by structuring our Kicad schematics like the following :\nThe microcontroller only has labels connected to its GPIO pins, like what follows.\nLabels on the stm32h750 chip. All the hardware connected to the GPIO pins (USB physical layer, ethernet PHY chip, UART connectors, etc\u0026hellip;) only has labels connected to its signal pins, like what follows.\nLabels on the board hardware. Then, the only remaining thing is for our algorithm to generate a label associative array that for each GPIO pin of the microcontroller, connects to a hardware label, or to to GPIOs if the pin is not used like follows.\nLabels associative array. Luckily, Kicad uses text-based representations for the schematic components of a design, which allows us to copy-paste them. We will use that in a hacky manner, and make our algorithm generate the text representation of a label connection array.\nHere is the one for the previous image.\nWe will then copy paste that into kicad\u0026rsquo;s schematics editor and that will be it.\nNumerical complexity # There are two factors that cause the numerical complexity of the problem to exponentially increase : signals being connected to multiple pins, and us wanting to choose \u0026ldquo;K peripherals in a set of N\u0026rdquo;.\nLet\u0026rsquo;s formally establish the number of possible combinations that we should theoretically test.\nFirst, each signal sig can be connected to multiple pins pin_nb(sig).\nEach peripheral per uses the set of signals per_sigs(per).\nThen, for each group grp (ex UART) of peripherals that we must use, we have per_nb(grp) peripherals available for that group and we must pick chosen_nb(grp) in that group.\nEach group grp has per_nb(grp) choose chosen_nb(grp) possible configurations, with n choose k = n! / (k! * (n - k)!).\nEach configuration uses the set of peripherals cfg_pers(cfg).\nThe total number of possible combinations to test can be written the following way :\nN = PROD(grp in grps){ SUM(cfg in cfgs(grp)) { PROD(per in cfg_pers(cfg)) { PROD(per_sigs(per)) {pin_nb(sig)} } } }\nTheoretically, we could just iterate over all those combinations, check if each is valid, and stop at the first valid one.\nThough, if our model has a lot of peripherals with a lot of signals, we could iterate over a lot of combinations before finding a valid one.\nIn order for that search to be quicker, there are a few optimizations that we can do. During the development of this project, I was using a template design / peripheral selection as an example, and those optimizations made the number of potential solutions go from more than 2 ^ 64 to actually 16.\nNot 2 ^ 16. I actually mean 16 combinations to try.\nAlgorithmic data structures # The first thing we must do is translate the pin multiplexing array in the microcontroller\u0026rsquo;s doc into a machine readable text file that our algorithm will process.\nThat is long and painful, but trust me, it\u0026rsquo;s worth it. It is very likely that all chips in the same family (ST\u0026rsquo;s H7 for example) share the same multiplexing layout, so you may not have to do that frequently.\nThen we can read this file, and build a graph-like in-ram data structure, where :\neach group references its possible configurations (k choose n) each configuration references its peripherals. each peripheral references its signals. each signal references its pins. Here one must note :\neach configuration is part of one and exactly one group. each peripheral can be referenced by multiple configurations of the same group. (A) each signal is part of one and exactly one peripheral. each pin can be referenced by multiple signals. (B) A and B are the two factors that were previously mentioned to make the complexity explode.\nThose are the places where we must optimize things, by reducing the exploration space. Though, we cannot afford to remove a potential solution for the sake of speed. We must ensure that we only remove invalid configurations from the exploration space.\nPeripheral types. # In the next sections, when the expression \u0026ldquo;for exploration purposes\u0026rdquo; or \u0026ldquo;FEP\u0026rdquo; is used, it means \u0026ldquo;given the current optimizations we found for the exploration\u0026rdquo;. For example, a signal S can be connectable to a pin P, that is guaranteed to be connected to another signal in every valid combination. In this case, S will be considered disconnected from P for exploration purposes.\nFirst, we can note that some peripherals are optional and some are mandatory :\nif a peripheral is part of a group where all peripherals will be chosen (FEP) (i.e. we choose N peripherals in a set of N, which is equivalent to having a group for each) then it is mandatory (FEP), in the sense that if a valid combination exists, this peripheral will be part of it. otherwise, the peripheral is optional FEP in the sense that if a valid combination exists, it may or may not be part of it. If a peripheral is mandatory, then we can start to optimize its signals.\nSignals optimizations # If a peripheral is mandatory, so are its signals. That means that if such a signal S can only be connected to one pin P, then it must be connected to it.\nOther signals of other peripherals connected to this pin can just be considered not connected to it for exploration purposes, since if there is a valid configuration, S will be connected to P.\nIf a signal is found to be connected to no pin for exploration purposes (i.e all its possible pins are known to be connected to other signals), it means that the related peripheral cannot be present in the final solution, and that we can just remove this peripheral from the graph.\nIf the peripheral is mandatory, then no solution exists.\nIf the peripheral is optional, then we can actually remove it from the graph.\nIf we find a signal to be connected to multiple pins, and one of those pins P is connected to no other signal, then we can disconnect the signal connected to P and disconnect it from all other signals.\nThis effectively reduces the number of connections on other pins, which potentially allows us to re-apply the first optimization discussed above.\nSignals optimizations # If the peripheral is optional, then we can remove it from the exploration space (i.e. from the graph). This has two consequences.\nFirst, every signal that we remove removes a connection to the pin that it was previously connected to. This potentially creates new pins with a single connection and we can re-apply the signal optimization sequence.\nThen, every peripheral that we remove is optional, and thus, is part of a group where we chose K peripherals among N, with K \u0026lt; N. Removing the peripheral reformulates the problem, by forcing us to choose K peripherals among N - 1 peripherals.\nIf K == N - 1, then all remaining peripherals become mandatory, and we can optimize them all using signal optimizations.\nIf K \u0026lt; N - 1, then all remaining peripherals are still optional.\nIn order to simplify the exploration space, we apply the signal optimizations and peripheral removal procedures repeatedly to the graph, until it leaves the graph unaffected.\nThen, we can bruteforce the graph by testing the validity of all remaining combinations.\nPathological case # There is a case where those optimisations bring no improvement.\nLet\u0026rsquo;s imagine a simple scenario where we have 2 pins on our SOC, we want to select 2 mandatory peripherals, each peripheral has 1 signal that can be connected to both pins.\nNo optimization will work here, since each pin is connectable to 2 signals, and each signal is connectable to 2 pins.\nThus, those optimizations will not always improve performance, but they will statistically improve it a lot, since this kind of pathological case is rather rare, or only involves a low number of pins.\n","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/projects/chip_pinout_generator/","section":"Personal Projects","summary":"A rather-not-smart but efficient way of connecting your PCBs.","title":"Chip pinout generator","type":"projects"},{"content":"","date":"1 January 2024","externalUrl":null,"permalink":"briztal.github.io/categories/kicad/","section":"Categories","summary":"","title":"KiCAD","type":"categories"},{"content":"","date":"9 November 2023","externalUrl":null,"permalink":"briztal.github.io/series/kasan/","section":"Series","summary":"","title":"KASAN","type":"series"},{"content":"","date":"9 November 2023","externalUrl":null,"permalink":"briztal.github.io/categories/kasan/","section":"Categories","summary":"","title":"KASAN","type":"categories"},{"content":" Introduction # Now that we have a better idea of how the kernel manages memory from a high level point of view, let\u0026rsquo;s elaborate on how the KASAN will fit in this system.\nMetadata # The KASAN will have to track the status of each byte of memory in the system.\nFor this, it will need metadata, that it will store in dedicated memory : each byte of memory managed by the memory system will have attributes, that will describe the status of this byte, or more precisely :\nif the byte is accessible (as defined in the region manager section) ? who can access the byte, either user, allocator or both ? is the byte writable ? is the byte initialized ? This can be achieved by storing 4 bits of attributes per byte of memory.\nAttributes location # Attributes must be located somewhere in memory. To place them, we will use the same strategy as the Regions allocator.\nWhen registering a memory regions in the region manager, before the region manager allocates all its metadata, the KASAN will split the region in two parts :\nkernel-accessible memory : this part will be forwarded to the region manager, that will in turn divide it in two subregions as described in the previous part : pages, and pages metadata. KASAN attributes : this part will contain 4 bits of attributes for each byte of memory in the kernel-accessible part. Attributes lifecycle # Attributes are the base information that the KASAN will use to verify a memory access. A memory access of N bytes starting at A will cause the KASAN to fetch attributes for the address range [A, A + N[, and verify that they allow the access to occur. If so, the KASAN will let the access happen. Otherwise, it will trigger an error.\nIn this context, let the access happen means emulate the access, as described in part 3. Here are some accesses that should cause an error to occur :\nreading from an uninitialized byte. writing to a non-writable byte. user accessing a byte not accessible to the user. allocator accessing a byte not accessible to the allocator. access to a non accessible byte. The following diagram describes the lifecycle of attributes, and the various events (accesses) that can occur, as well as the errors generated by the KASAN checker if any.\nTODO one does not simply find the time to draw diagrams\u0026hellip;\nCPU state # To make our attributes model work, we need an additional per-cpu variable that will report whether the said CPU is running allocator code or user code.\nThis will be used as a base to verify the accesses :\na CPU running in user mode accessing a memory block accessible to allocator only will trigger a KASAN error (use after free) a CPU running in allocator mode accessing a memory block accessible to user only will trigger a KASAN error (use after alloc) Tuning the allocators # The access checking relies on the accuracy of the KASAN attributes.\nThose attributes are not only modified by a direct access like the \u0026lsquo;initialized\u0026rsquo; field.\nIn particular, any allocation or free in both the region manager or secondary allocators must be reflected in attributes.\nThis requires the said memory managers to be modified, so that :\nwhen the region manager allocates pages, all the related bytes go from not accessible to accessible to allocator only. when the region manager frees pages, all the related bytes go from accessible to allocator only to not accessible. when secondary allocators allocate memory to the user, all the related bytes go from accessible to allocator only to accessible to user only. when secondary allocators free memory, all the related bytes go from accessible to user only to accessible to allocator only. When one of those transitions occurs, the KASAN must verify that the related bytes have the expected start state.\nAs mentioned earlier, the allocators must also be updated to report their entry / exit in the cpu state.\nCorner cases # The rules stated in the two previous sections are true in the general case. The FSM described is a high level representation of the reality, but there are some corner cases that make the actual implementation look less like an actual FSM.\nAllocators data # The main corner case is that allocators themselves (the actual allocator structs), so as CPU states, and some other main kernel components must be accessible both in allocator and user state.\nThat causes us to need a specific attribute state accessible to anyone and dedicated KASAN entrypoints to report that a specified memory block falls into this category.\nGlobal variables # Another corner case that we have to handle is the state of global variables.\nAs much as we would like to avoid them, global variables are a necessary evil. Take the memory system data structure for example : it must be initialized during the early stages of boot, when we have not yet registered any memory region, thus, when dynamic allocation is not available. In any case, memory allocation goes through it, so we literally can\u0026rsquo;t have it dynamically allocated.\nThere are a few cases like this one, where we theoretically could avoid using globals, but where there is no real benefit of doing so.\nOur kernel executable will have the two regular .data and .bss sections in which globals / static variables are located.\nThose variables are initialized as part of the very early bootstrap sequence, by copying the content of the initializer of .data (likely located in flash) directly in .data, and by zeroing-out the content of .bss.\nThose sections must be considered initialized by the KASAN, as they indeed are initialized during very early boot.\nThough, those sections are located in RAM, and will ultimately be included in a particular memory region.\nThis will add two tasks to the memory management system when registering a memory region :\nthe region manager will have to tag all pages in these sections as not allocatable, as those pages are implicitly allocated to the kernel. the KASAN will have to report any byte in such a section as accessible by anyone, and initialized, as opposed to other bytes whose initial state is accessible by no one. Attributes management # I will not provide a detailed explanation on how to actually handle attributes, as it is very implementation-specific, complex, and I have little time.\nRather, here is a link to my public code repo. Files ksn.h and ksn.c contain the platform-independent KASAN code of my kernel, whose main job is to manage attributes.\nBriztal/kr_public Code from my kernel that may be of interest to some. C 0 0 You won\u0026rsquo;t be able to compile it as it requires the rest of the kernel to work, but this will give you an example of working attributes management.\n","date":"9 November 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_5_kasanmem/","section":"Personal Projects","summary":"How KASAN manages memory.","title":"KASAN memory management.","type":"projects"},{"content":"","date":"9 November 2023","externalUrl":null,"permalink":"briztal.github.io/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" Introduction # Before elaborating on the principles of operation of the memory checker, I will give a brief description of how a kernel manages memory in a system without an MMU.\nMemory region # The term region will be used here to refer to a region of physical memory (addressable memory) and not an MPU region.\nA memory region is a portion of the address space :\nat which memory is mapped. around which memory is not mapped. In particular, if an address range R is a memory region :\nall bytes inside R are accessible and are mapped to memory. the two bytes before and after R are not mapped to memory. A microcontroller has multiple memory regions. For example, SoCs of the the STM32H7* family have the following memory regions available :\nStm32H7 family memory map. Flash for STM32H750xB : - user main memory : 0x08000000, length 0x00020000 (128Ki). - system memory 0 (RO) : 0x1ff00000, length 0x00020000 (128Ki). - system memory 1 (RO) : 0x1ff40000, length 0x00020000 (128Ki). Flash for STM32H742xI/743xI/753xI : - user main memory : 0x08000000, length 0x00200000 (2Mi). - system memory 0 (RO): 0x1ff00000, length 0x00020000 (128Ki). - system memory 1 (RO) : 0x1ff40000, length 0x00020000 (128Ki). Flash for STM32H742xG/743xG : - user main memory 0 : 0x08000000, length 0x00080000 (512Ki). - user main memory 1 : 0x08000000, length 0x00080000 (512Ki). - system memory 0 (RO) : 0x1ff00000, length 0x00020000 (128Ki). - system memory 1 (RO) : 0x1ff40000, length 0x00020000 (128Ki). RAM : - TCM Inst RAM : 0x00000000, length 0x00010000 (64Ki). - TCM Data RAM : 0x20000000, length 0x00020000 (128Ki). - D1 System AXI SRAM : 0x24000000, length 0x00080000 (512Ki). - D2 System AHB SRAM1 : 0x30000000, length 0x00020000 (128Ki). - D2 System AHB SRAM2 : 0x30020000, length 0x00020000 (128Ki). - D2 System AHB SRAM3 : 0x30040000, length 0x00008000 (32Ki). - D3 System AHB SRAM4 : 0x38000000, length 0x00010000 (64Ki). - D3 backup SRAM : 0x38800000, length 0x00001000 (4K). Region manager # One of the most fundamental jobs of the kernel is to manage memory, that is, to allow itself and its users to allocate and free memory.\nThe precise architecture, and the reason that guide the architecture of such a memory management system will not be discussed here, as it requires a dedicated chapter. I may or may not write about that in the future. Rather, I will focus on the high level blocks and features, with few justifications.\nManaging memory is not easy, particularly when dealing with small memory blocks, as it often requires metadata.\nVarious allocation techniques exist and each have their tradeoff.\nThe memory management\u0026rsquo;s fundamental block is the region manager. It has to :\nmanage memory regions, as in tracking the allocation state of every byte of every region. allow dynamic allocation of blocks of arbitrary sizes. allow dynamic free of allocated blocks. Page # To accomplish the three goals listed in the previous section in a manner that is not too catastrophic perf-wise, the region manager will define arbitrarily the notion of page, as a block of size 2 ^ PAGE_ORDER, aligned on its size, with PAGE_ORDER the order of a page.\nPAGE_ORDER may or may not be hardware-related. On systems with an MMU, it is likely a multiple of the order of the MMU translation granule. On systems without an MMU, we can choose it more freely. The region allocator will then manage memory on a per-page basis : rather than keeping track of the allocation status of each and every byte in the system, it will keep track of the status of every page in the system, and make allocations on a per-page basis.\nThe order of the page is arbitrary, but the reader can have the following orders in mind :\nmodern systems : size = 64KiB, order = 16 (or higher). older systems (and windows lol) size = 4KiB, order = 12. embedded systems : size = 1KiB, order = 10 (or lower). As said earlier, the order of a page may be tied to the hardware in a system with an MMU, as it must match the translation granule of the said MMU.\nHere, we do not have that constraint. The page order will be selected by the kernel developer depending on the use case, using the following criteria :\na larger memory order will mean less pages in the system, which will mean more fragmentation in the allocators that will use those pages, but less metadata per page, so a potential better use of the memory. May be better for specialized applications where large blocks are required. a smaller memory order will mean more pages in the system, while will mean less fragmentation in the allocators that will use those pages, but more metadata per page, so a potentially underuse of the memory. May be better for generic applications that allocate a lot of blocks of different sizes in a non-predictable manner. The way to allocate and free pages is implementation-defined and depends on the use case :\nif allocation of blocks of contiguous pages is required, a buddy allocator can be used. This method only supports allocating blocks of 2 ^ k pages (with k an integer \u0026gt;= PAGE_ORDER), which proves to be enough in practice. if the system only needs pages and not particularly blocks of pages, a simple linked list can be used. Per-page metadata # To manage the state of the pages, the region manager will need per-page metadata.\nThis per-page metadata is a net cost, in the sense that it can\u0026rsquo;t be repurposed when the page is free, or allocated. The region manager is the fundamental block of the memory allocator. Other allocators are built on top of it. A consequence to that is that it cannot rely on dynamic memory allocation to allocate its metadata.\nThink about it : when the system boots, all allocators are empty. During bootstrap, when the region manager will register its first memory region, how could it allocate metadata, since calling some form of malloc would ultimately cause its own page allocation functions to be called, yielding a failure, since no memory is yet registered. This is a classical memory chicken and egg problem.\nThe region manager solves this problem by carving out a block of static (non-reusable, not provided by another allocator) per-page metadata in the actual memory block that will contain the pages. In other words, it will use some of the (theoretical) pages to contain the metadata of the remaining pages. The memory block containing per-page-metadata will obviously not be available for allocation.\nFor the region manager, a page will be either :\nnot accessible : the page hasn\u0026rsquo;t been allocated to any user of the region manager and should never be accessed by anyone. accessible : the page has been allocated to a secondary allocator, who may access it, and provide access to portions of it to users. Secondary allocators. # As described in the previous section, the region manager\u0026rsquo;s job is to manage memory regions.\nIt can only allocate memory on a per-page basis, and cannot allocate smaller blocks.\nThough, it is kind of rare for a user software to need an actual page of memory. A rough estimation is that most allocations require around 1 to 4 cache lines of memory (64B -\u0026gt; 256B) which is way smaller than a page, even in a microcontroller environment.\nThe secondary allocators handle this use case.\nTheir behavior is simple : they act as an interface between the user and the region manager, by :\nallocating large blocks of memory (blocks of 2 ^ k pages); dividing them into smaller blocks in an implementation-defined manner; allocating these blocks to the user. There are many secondary allocators, each one having its own capabilities and tradeoffs. We can mention :\nslab : supports both allocation and free of blocks of a fixed size. Works by dividing a page into blocks of the same size. Block state is either stored in a bitmap, or directly in the data of the free blocks, to form a linked list of free blocks. slab array : supports both allocation and free of blocks of a set of sizes. Uses multiple slabs, one for each supported size. stack : supports allocation of blocks of arbitrary size and mass-free only. heap : supports allocation and free of blocks of arbitrary size, allocating blocks of different sizes in the same pages. Inefficient. Any memory block that the allocators manage is provided by the region manager. As such, any such block is always accessible.\nFrom the allocator\u0026rsquo;s perspective, a block of memory is :\nfree : the block is not accessible by any user. It can be used by the allocator to store metadata. allocated : the block is accessible to all users. It should never be accessed by the allocator. Summary # The following diagram summarizes the architecture of the memory system.\nTODO it seems that I have less time than I expected\u0026hellip; Sorry.\n","date":"3 November 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_4_kernmem/","section":"Personal Projects","summary":"How the kernel manages primary memory.","title":"Kernel memory management.","type":"projects"},{"content":"This chapter will describe the basic principles of operation of the KASAN.\nStructure # Base # The previous chapters laid the foundations on which we will build the KASAN :\nwe cannot use any transpiling-oriented method to wire our KASAN to the executable due to potential code size increase. we have an MPU that can trigger a MemManage fault whenever a specific portion of memory is accessed. the handler of this MemManage fault can modify execution context (registers) of the program that caused the memory access, and update its execution flow. the MemManage fault handler gives us the location of the fault, so as the address of the instruction that generated the fault. Memory checker execution flow # To implement our KASAN, we will first use the MPU to disable access to the whole RAM region.\nThat will make any access to any RAM to cause a MemManage fault, causing an exception and the execution of the related handler.\nThe MemManage fault handler will save the part of the context that hasn\u0026rsquo;t been saved by the HW somewhere in memory for the emulator (defined below) to read or write it later.\nThe MemManage fault handler will then read the PC of the instruction that caused the fault, read the instruction and decode it.\nIt will then check that the related instruction is a memory access. If it is not one, it will just execute the classic MemManage fault exception.\nIf it is a memory access, it will emulate the instruction and perform its checking in the meantime.\nThis will be the place where the memory checking is done. It involves :\ndecoding the instruction. verifying that every access performed by the instruction is valid. updating the KASAN memory metadata to reflect the new state caused by the memory access (ex : write to an uninitialized location causes the location to be treated as initialized in subsequent accesses to that location). emulating the instruction, by performing the underlying accesses : register reads will cause the emulator to read the context-saved values. register writes will cause the emulator to write the context-saved values. memory reads will cause the emulator to actually perform the read now that the MPU is disabled. memory writes will cause the emulator to actually perform the write now that the MPU is disabled. if the instruction emulation is successful, modifying the PC that will be restored when the exception will return, so that the instruction after the one that just was emulated is executed at that time, and not the one that we emulated again. The implementation of such an emulator will have a dedicated chapter.\nWhen this is done, it will re-enable the KASAN MPU regions, reload the SW-saved context (possibly updated by the emulator since it was saved), and return from exception.\nThen, the processor will restore the HW saved context (possibly updated by the emulator since it was saved), and give control back to the interrupted thread, but at the instruction after the one that just trapped and got emulated.\nSummary # The following diagram summarizes the high level behavior or the KASAN.\nTODO : summarize initialization sequence.\nTODO : summarize validation operation.\nDesign notes # Whitelisting the stacks # In order for our MPU-based trap system to work, we need to add two other MPU regions, with a greater priority than the RAM blacklist region, to allow access to the user and exception stack (PSP, MSP).\nIf we didn\u0026rsquo;t do this, any code making a memory access in the RAM would cause an exception.\nThe processor, in the handling of this exception, would push the context to the stack in use at that moment (either Main or Process stack), which, if those regions were blacklisted by the MPU, would cause the MemManage fault to be escalated in a hardfault. This is not what we want, as it has different privileges, and less recoverability.\nThis increases to 3 the number of MPU regions necessary to implement our KASAN.\nDisabling MPU regions in the MemManage fault handler # The KASAN memory access checker will run in the MemManage fault handler.\nThis checker will likely access variables not located in the stack.\nThis is a potential problem, as the MPU may still be active when this handler is executed.\nEven if we had the option to have the MPU automatically disabled in handler mode, this is to coarse to suit our needs, as that would prevent the KASAN to check accesses made in handler mode, under which a large part of the kernel operates (syscall, scheduling, interrupts, etc\u0026hellip;).\nTo handle this situation, we will need to reprogram the MPU when we enter and exit the MemManage fault handler.\nOn entry, we will disable the regions related to KASAN, and on exit, we will re-enable them.\nKASAN regular entry # The next chapters we will talk about how to manage the KASAN memory attributes, and in particular, how those memory attributes are updated when an allocator allocates or frees a memory block. The allocators will need to call a KASAN entrypoint to verify that the attribute change is valid, and to perform it.\nIn reality, there are some issues when trying to enter KASAN in a function-like manner.\nAs described in the memory access trap section, the basic working principle of our KASAN is to blacklist the whole RAM minus the stacks, such as any access to this region causes a MemManage fault and causes the KASAN memory access checker to be executed.\nThe issue, is that the KASAN is meant to check all accesses made by the kernel, interrupts included.\nAs such, when entering KASAN The software way, we must prevent any interrupt to occur.\nThis can be done by disabling interrupts, and re-enabling them on exit. Though, we must be careful here : interrupts may have been already disabled before entering KASAN, so we must not re-enable them in this case.\nEntering KASAN \u0026lsquo;The Software Way\u0026rsquo; also requires us to be able to disable MPU regions. To do this, we need to have read / write access to registers of the System Control Space, which requires us to run in Privileged mode.\nARMV7M ARM for CortexM3 :\nUser access prevents: - use of some instructions such as CPS to set FAULTMASK and PRIMASK - access to most registers in System Control Space (SCS). As such, if we plan to have memory management code that runs on Unprivileged mode (ex : secondary allocator), or if we plan to support user code that reports memory as read-only to debug something, we need to escalate to Privileged mode.\nWe could have a dedicated syscall in the kernel to handle those cases. But then we would have to have two paths, for code that is already privileged, and for code that is not.\nA single KASAN entrypoint # There is a more clever way.\nUnluckily for us, in microcontroller-land, address 0 is often accessible. In my board, this is the start of the TCM-D RAM region.\nWe will use another MPU region to prevent any access at address 0.\nThen, we will define a dedicated software_kasan_entry function, implemented in assembly, that receives a pointer to a function to execute, and its arguments.\nThis entry function will be located at a fixed address in the executable. This function will manually trigger a read at address 0 on purpose.\nThis will cause a memory management exception to be triggered, and will cause the execution of the generic KASAN handler, which will setup the environment for KASAN execution.\nThe generic KASAN handler will then compare the fault PC to the location where the entry function causes the read at access 0.\nIf the values are equal, then it will treat the MemManage fault as a KASAN entry, and will read the function and its arguments from the saved context, execute the function and return.\nIf the values are not equal, then it will know that the MemManage fault was not made on purpose by the entry function, and will proceed with the regular KASAN access checking.\nThe related assembly code is :\n# Enter kasan. # Receives 4 args at most : # - hdl : function to call, receives at most 4 args. # - arg0 : first arg to pass to hdl. # - arg1 : second arg to pass to hdl. # - arg2 : third arg to pass to hdl. # - arg3 : fourth arg to pass to hdl. .align 4 .global kr0_tgt_ksn_enter .type kr0_tgt_ksn_enter, %function kr0_tgt_ksn_enter: # Store arg3 in r4 for the handler to have an easier job. sub sp, #8 str r4, [sp, #0] str lr, [sp, #4] ldr r4, [sp, #8] # Store 0 in r12. mov r12, #0 # Trigger a read at address 0. # The fault handler will detect that it came from this PC and forward # the call to the kasan handler. enter_pc: str r12, [r12] # Complete. ldr r4, [sp, #0] ldr lr, [sp, #4] add sp, #8 bx lr .size kr0_tgt_ksn_enter, .-kr0_tgt_ksn_enter .global armv7m_kasan_enter_pc .set armv7m_kasan_enter_pc, enter_pc This assembly code defines a global variable whose address is equal to the address of the instruction that causes the access at PA 0.\nThe related KASAN C handler (called by the assembly MemManage fault handler after it saved the software context and disabled the KASAN MPU regions) will then compare the fault PC against this value and act accordingly.\nKasan handler C code :\n/* * Kasan entrypoint. * Called by armv7m_kasan_handler defined in armv7m.S. * @fault_pc is the location where the address of the instruction that * faulted is stored (@hw_ctx + 0x18). * @hw_ctx is the start of the hw-pushed context. * @sw_ctx is the start of the sw-pushed context. */ void armv7m_kasan_entry( u32 *fault_pc, u32 *hw_ctx, u32 *sw_ctx ) { check(hw_ctx, sw_ctx); check((void *) fault_pc == psum(hw_ctx, 0x18)); /* Only handle memfaults with a valid MMFAR. * All other faults go to kernel. */ SCS_SHCSR SHCSR; SHCSR.bits = *(volatile u32 *) SCS_SHCSR__ADDR; if ((!SHCSR.MEMFAULTACT) || (SHCSR.BUSFAULTACT) || (SHCSR.USGFAULTACT)) _flt_forward(); SCS_CFSR CFSR; CFSR.bits = *(volatile u32 *) SCS_CFSR__ADDR; if (!CFSR.MMARVALID) _flt_forward(); /* Read the MMFAR. */\tconst u32 mmfar = *(volatile u32 *) SCS_MMFAR__ADDR; /* If the fault PC is equal to @armv7m_kasan_enter_pc, this is a kasan * entry call. */ if (*fault_pc == (((u32) \u0026amp;armv7m_kasan_enter_pc) \u0026amp; ~(u32) 1)) { /* MMFAR should be 0. */ assert(mmfar == 0, \u0026#34;MMFAR expected 0 on kasan entry.\\n\u0026#34;); /* Fetch the handler and its arguments. */ void (*hdl)(uaddr, uaddr, uaddr, uaddr) = (void (*)(uaddr, uaddr, uaddr, uaddr)) hw_ctx[0]; uaddr arg0 = hw_ctx[1]; uaddr arg1 = hw_ctx[2]; uaddr arg2 = hw_ctx[3]; uaddr arg3 = sw_ctx[0]; /* Call the handler. */ (*hdl)(arg0, arg1, arg2, arg3); /* Clear the fault. */ *(volatile u32 *) SCS_CFSR__ADDR = 0xffffffff; /* Recover from the fault. */ *fault_pc += 4; return; } /* Otherwise, we must check the operation in progress. */ /* Determine the fault address. */ const u32 pc = *(volatile u32 *) fault_pc; /* Determine if we are in allocator or user context. */ const u8 from_all = !!all_ctr; /* Decode the instruction, execute it and return the * PC of the next instruction. * If an error occurs, do not return. */ const ksn_emu emu = { .emu = {\u0026amp;ksn_emut}, .hw_ctx = hw_ctx, .sw_ctx = sw_ctx, .flt_addr = mmfar, .from_all = from_all, .chk_flt = 1, .is_ici = 0 }; const u32 new_pc = _process_pc(pc, \u0026amp;emu); check(pc \u0026lt; new_pc); assert(!emu.chk_flt, \u0026#34;no memory access made by the emulator.\\n\u0026#34;); const u32 diff = new_pc - pc; check((diff == 2) || (diff == 4)); /* Clear the fault. */ *(volatile u32 *) SCS_CFSR__ADDR = 0xffffffff; /* Update the PC and resume execution. */ *fault_pc = new_pc; } ","date":"15 October 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_3_blocks/","section":"Personal Projects","summary":"KASAN in details.","title":"KASAN structure.","type":"projects"},{"content":"This article will give technical details on the hardware capabilities used to implement our KASAN.\nI\u0026rsquo;d like to point out that the KASAN implementation is not specific to this chip. The only features needed for the KASAN to theoretically work are :\nan exception handling mechanism. an MPU. The chip # Development board. Let\u0026rsquo;s introduce the development board on which I built the KASAN.\nThe devboard that my dear wife made for me has :\nSTM32H750 processor; featuring a CortexM7 CPU implementing the ARMV7M architecture; a Memory Protection Unit (MPU) conforming to the ARMV7M architecture; a Floating Point Unit; 128KiB of Flash; TODO Around 512KiB of RAM. The MPU is the main component on which our KASAN implementation will be based. It is a needed feature.\nExecution privileges # A processor implementing the ARMV7M architecture has two levels of privileges :\nunprivileged mode : code running in this privilege level has limited access to the processor system registers. privileged mode : code running in this privilege level has unlimited access to all processor features. In particular, the CPU starts in privileged mode at reset. Then, it is up to the firmware (kernel) to run threads in unprivileged mode.\nCode running in unprivileged mode can do a system call to execute a system handler. This handler will execute in privileged mode. The actual function that will be executed can\u0026rsquo;t be programmed by unprivileged software.\nStacks # A processor implementing the ARMV7M architecture supports two different stacks :\nthe Main Stack, used in handler mode. Stack pointer : MSP the Process Stack (should have been called Thread Stack), used in thread mode. Stack pointer : PSP. The MSP / PSP are aliased to SP. Accessing SP actually accesses the stack pointer active in the current mode (thread or stack).\nExceptions # A processor implementing the ARMV7M architecture supports the notion of exception : code running at a given time can be temporarily stopped, for another piece of code (a handler) to be executed instead.\nWhen the handler is done (understand : when the handler function returns), the stopped code resumes its execution transparently. This mechanism is purely hardware, and serves among other things, to handle system calls and interrupts.\nThe piece of code that is executed is called a handler; when the processor handles an exception, it is said to be in handler mode. When it doesn\u0026rsquo;t, it is said to be thread mode. Thread mode can be privileged or not. Handler mode is always privileged.\nAn exception has an ID, which is used to determine :\nif this exception is enabled (i.e. if the handler is allowed to be called at all when the exception is triggered). the priority of this exception (i.e if the handler is allowed to be called given the currently executed exception, if any). the handler to be executed : handlers addresses are listed in a table in memory called the vector table. The location of the vector table is defaulted at boot and can be changed later by configuring the VTOR (Vector Table Offset Register) register. The ARMV7M architecture supports at most 512 exceptions.\nThe ARMV7M architecture defines the 16 first exception IDs as system exception\nARMV7M exceptions. Source : ARMV7M Architecture Reference Manual. Those system exceptions are interesting for us :\nReset : function called when the processor is reset. NMI : function called when the Non Maskable Interrupt exception is triggered. HardFault : handler for certain non-recoverable faults, or for some faults occurring inside their related handler (ex : nested memfault). MemFault : handler for a MPU access fault (see after) and others. BusFault : handler for invalid transactions on the memory bus. UsageFault : handler for misuse of the assembly (ex : division by zero). SVCall : syscall handler. DebugMonitor : handler for debug events when there is no debugger. PendSV : preemption handler. Systick : system timer overflow handler. The reader may have noticed that no exception has index 0. This is on purpose, as the first 32 bit word of the exception table used during reset contains the stack pointer loaded during reset.\nThe (at most) 496 remaining exceptions are interrupts.\nWhen an exception A is triggered, the processor determines the priority of A, so as the priority of B, the exception currently handled by the processor, if any. If A is enabled and has a greater priority than B (or if the processor is not currently executing any exception), the current execution context is saved in the local stack (either Main Stack or Process Stack) by the hardware, and the handler of A executed. When the handler of A is complete (when it returns), the processor detects it, and restores the execution context saved on exception entry. Then, the execution of B (or thread code) continues transparently.\nContext # The ARMV7M context is composed of\ngeneral purpose registers : R0 -\u0026gt; R3 : Caller-saved. R4 -\u0026gt; R11 : Callee-saved. R12 : Caller saved, scratch register. R13 : Caller saved, Stack pointer, alias : SP. R14 : Caller saved, Link register, alias LR. R15 : Caller saved, Program counter, alias PC. Floating point registers (When FPU is implemented and active) : S0 -\u0026gt; S15 : Caller saved. S16 -\u0026gt; S31 : Callee saved. Status registers : xPSR : composite of all status registers, see the ARMV7M ARM for more details. Context saving and exception # Saving the execution context means atomically storing a subset of the context registers to the currently used stack (Main / Process). Restoring the execution context means atomically loading a subset of the context registers from the stack, and as such, updating the state of the processor. (The loaded subsets and the stored subsets are the same).\nIn the ARMV7M architecture, This subset of saved/restored registers is composed of all the caller-saved registers and of the program status registers. This makes the exception entry / return procedures follow the standard ARMV7M calling conventions, and allows us to implement exception handlers using C code only when required (not here lol).\nHere is the set of context saved registers when the FPU is not enabled :\nSaved context with FPU disabled. Source : ARMV7M Architecture Reference Manual Here is the set of context saved registers when the FPU enabled :\nSaved context with FPU disabled. Source : ARMV7M Architecture Reference Manual The layout may be different depending on if the CPU is programmed to preserve 8 bytes stack alignment. In particular, the Reserved words may be omitted to achieve that purpose. Please refer to the ARMV7M Architecture Reference Manual for an exact description. If after a context save, the stored value of a register is modified, the modified value will be loaded in the register during context restore. This will be the base trick on top of which we will build our emulator.\nMPU # The ARMV7M features an optional MPU, always implemented in the CortexM7.\nThe MPU for Memory Protection Unit, allows the kernel to define up to 8 or 16 (depending on the implementation) MPU regions, and to define access permissions for these MPU regions.\nThese MPU regions must be :\nof a size that is a power of 2 and that is greater or equal than 32 bytes. aligned on their size (start address is a multiple of the size). If an access is made in one of those regions, with incorrect permissions (ex : write to a location in non-writable region, executing a location in a non-executable region), the MPU will issue a permission fault, and the MemManage exception will be triggered, causing the interruption of the program that caused the access, the saving of the register context, and the execution of the MemManage handler.\nI will not elaborate a lot on the various access permissions since that would require a chapter of its own, with little added value. What is important to us is that the MPU allows us to :\nblacklist a memory block, i.e. prevent the CPU from accessing this memory block while the MPU is active. whitelist a memory block, i.e. allow the CPU from accessing this memory block while the MPU is active. The MPU regions are indexed from 0 to N (N = nb_mpu_regions - 1), and have a fixed priority : when determining the access permissions for a byte at address X, if multiple MPU regions (let\u0026rsquo;s say A and B) cover X, then the MPU region with the highest index max(A, B) will be selected to decide which permissions use to access X.\nThat allows us, for example, to blacklist a large memory block using a MPU region with low priority, and to whitelist smaller portions of this memory block using MPU regions with a higher priority : when an access to one of the smaller blocks will be made, both the whitelist and the blacklist MPU regions will be considered, but since the whitelist MPU region has a higher priority, it will be selected and access will be allowed.\nThis will be the base trick on top of which we will build the memory access checking system.\n","date":"10 October 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_2_hw/","section":"Personal Projects","summary":"What our KASAN will run on.","title":"Hardware.","type":"projects"},{"content":" Introduction # This article will state the theoretical base on memory checking required to understand how the KASAN checks its access.\nDisclaimer : I have no prior experience with the internals of existing memory checkers like valgrind. The rough explanation that I\u0026rsquo;m giving here may be partially / completely false. It just reflects my high level understanding. This should have no impact on the validity of the further sections. Let\u0026rsquo;s first define the base concepts.\nTerminology # Definitions # Memory location : location in CPU memory accessible by address. Memory access operation : read or write to a memory location. Memory area state : attributes of a memory area that define how it is supposed to be used. Ex : allocated, not allocated, writable, initialized, not initialized, accessible, not accessible. Memory management operation : software operations that change the state of a memory are. Ex : allocation, free (and friends, ex : realloc\u0026hellip;) operations. Memory operations : memory access operations, memory management operations. Memory checker : system that checks a subset of the memory operation made by a program. Checking a memory operation means intercepting that memory operation in some way before it occurs, then verifying that the said memory operation is valid, then if so, allowing the operation to complete.\nConsequences # A register is not a memory location.\nAccessing a register is not a memory access.\nSubset # The subset of memory operations that the memory checker checks can include all memory operations or an actual subset.\nExamples of hypothetical checkers that would meaningfully check a small subset of operations :\na memory checker could check only allocation and free (and related) operations, to ensure that no double free (by the user) or double alloc (by the allocator) is done. a memory checker could check only memory reads and track memory writes, to ensure that software only reads from initialized (written before) locations. Validity of an operation # Valid memory operation : an operation that philosophically makes sense, given the paradigms of the program.\nExamples :\nread to an initialized local variable. write to a writable local variable. read to a memory region allocated by the program, not freed since allocation. Invalid memory operation : an operation that philosophically doesn\u0026rsquo;t make sense, given the paradigms of the program.\nExamples :\ndouble free. read of uninitialized memory. use by the user after free. double allocation (by the allocator). use by the allocator after the allocation. write to non-writable memory. write to a memory location that philosophically doesn\u0026rsquo;t belong to the part of the program that does the access (mem corruption ?) The strategy to intercept a memory operation (the some way stated before) depends on the type of the operation.\nMemory management operations are easy to intercept.\nMemory management operations like malloc and free are regular functions with dedicated entrypoints. The memory checker just has to override these entrypoints either at compile time or at link time so that its own implementations of malloc or free are called. Then it has the opportunity to verify the validity of operations, and (potentially) call the real malloc or free if they are valid (or just manage memory itself, it doesn\u0026rsquo;t matter).\nMemory accesses are very hard to intercept.\nThose accesses are not made using a function code, they are made using actual assembly operations, and there is no easy way to just \u0026lsquo;override\u0026rsquo; those instructions.\nIntercepting memory accesses : transpiling and why we cannot # Transpiling # A potential strategy to intercept memory access operations is to transpile the program : the memory checker will disassemble the assembly code and replace all memory access operations with assembly stubs to instead call the memory checker\u0026rsquo;s entrypoints, or just do the checking in place.\nCost # this is a very tricky operation, as for example, the jump addresses / offsets have to be carefully updated. this has a significant impact on : the code size, as a single memory access assembly operation is converted to a sequence of assembly operations. the code performance, as every access will have to be checked. Applicability for microcontrollers # This strategy could theoretically work for a microcontroller code, but we have to consider the impact on code size very seriously.\nComputers have a large virtual address space and kernel-powered swapping capabilities that allows enlarging the size of the executable way beyond what we could ever need. Taking a 10x hit on the executable size won\u0026rsquo;t affect the possibility of executing the code.\nMicrocontrollers are not in this situation.\nMicrocontrollers have flash that range from a few kilobytes to a few megabytes.\nIn my case, my code was already taking 63% of the chip\u0026rsquo;s flash (128KiB large).\nEven taking a 2x hit wasn\u0026rsquo;t acceptable in my case, and the transpiling method only gives a worst case guarantee of Nx (N being the number of instructions required to emulate a single memory access instruction, N \u0026gt;= 2).\nThis strategy was not applicable in our case.\nTo implement a KASAN on such a device, we need to have a solution that is taking advantage of the processor hardware.\nThis will be the subject of two other chapters.\nMemory operation subset, and limitations # Variability in complexity # In the introduction I mentioned that the memory checker was checking a subset of the memory operations.\nIdeally, we would like our memory checker to check all memory operations.\nThough, depending on the implementation of the memory checker, some operations will be more difficult to handle than others.\nA trivial example is memory access operations vs allocation / free operations. It is very easy for a debugger to hook up to memory allocation primitives like malloc and free and to verify the validity of those operations, since those (with their realloc counterparts) support the entire lifecycle memory (you\u0026rsquo;re not supposed to free a block not acquired via free and freeing a block means that you shouldn\u0026rsquo;t free it again).\nThough, as we said earlier, checking actual memory accesses is non-trivial as involves directly working with the assembly, either directly as described in the transpiling section, or indirectly, as it will be described later for our implementation.\nStack and non-stack (heap or globals section) # Another example showing the variation in complexity between check operations can be found by thinking how we would check local variables as opposed to allocated variables.\nLet\u0026rsquo;s assume in this example that registers and compiler optimization do not exist and that when we access any type of variable, a memory access instruction is generated by the compiler.\nWhen compiling this function :\nvoid f(void) { volatile uint64_t i = 0; } the compiler will detect that a local variable i is used, and needs a location. It will place this variable in the execution stack allocated to f, and will generate :\na function prologue to reserve space on the stack. a memory access to store 0 at the location of i in the stack. a function epilogue to restore the stack to the state where it was before the execution of the prologue. In practice this hardly requires more than subtracting 8 to the stack pointer, storing 0 at the new top of stack and adding 8 to the stack pointer.\nNow, let\u0026rsquo;s note that the actual address of i (that \u0026amp;i evaluates to) is a stack address, and will become invalid as soon as the function returns. After that, it will potentially be allocated to another stack variable, or never used again, depending on the executed code.\nDue to this, if we want to check accesses to local variables (to detect a read from an uninitialized location), we also need to modify the function prologue and epilogue to report stack adjustments to the memory checker.\nNow, let\u0026rsquo;s describe what happens when the compiler processes this function :\nvoid f(void) { uint64_t *ptr = malloc(8); *ptr = 0; free(ptr); } the compiler will still allocate space on the stack for a local variable, but this time this local variable will be a pointer that will contain the address of the memory block to write 0 to.\nIn this case, it will be a lot easier for the memory checker to verify accesses to *ptr : its life starts after allocation by malloc, after which it can be considered uninitialized, and ends before free by free.\nThis life cycle will be easier to check, because as stated before, it is \u0026rsquo;trivial\u0026rsquo; for a memory checker to intercept actual calls to malloc and free, but it is way harder if not hardly possible to intercept all modifications to the stack in a meaningful manner.\nThe conclusion to this point is that it is more difficult to check accesses to local variables than it is to check accesses to dynamically allocated variables.\nLuckily for us, the compilers nowadays feature decent function-local analysers that will warn against such usages at compile time. Added to that, the main bugs that we statistically encounter in real life are due to dynamically allocated memory. Checking local variables is a nice feature to have, but not an actual requirement.\n","date":"3 October 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_1_memcheck/","section":"Personal Projects","summary":"General memory checking considerations.","title":"Memory checking.","type":"projects"},{"content":"I am Raphael Outhier, a french engineer working in Texas and this is my website.\nI graduated in 2019 from a French engineering school, worked for ARM for a year and am now working for Apple as a platform (HW) kernel engineer.\nAt work, I specialize in CPU bringup and CPU tracing (coresight).\nProgramming is one of my passions, a passion that I invest a lot of time in.\nThough, I\u0026rsquo;m legally not authorized by my employer to contribute to open source projects.\nBut that still leaves me the possibility to work on those projects, and to write about those.\nThe latter is the reason why I created this website.\nIn this website, you\u0026rsquo;ll find articles about the various projects that I\u0026rsquo;ve been working on these last years.\nThose articles will likely revolve around some of the following themes.\nRobotics I originally started studying embedded systems development with the objective of understanding how robots (CNC machines, drones etc\u0026hellip;) work.\nAnd by understanding, I mean that I wanted to build them from scratch (as much as possible), and write the code to make them work, also from scratch.\nI have currently wandered far away from this objective, mostly because of the second theme described below that took precedence these last 6 years, but still have the hope to get back to finishing my CNC control software in the future.\nKernel development Controlling robots is essentially done using microcontrollers.\nMy first robot was a quadcopter that I built myself, and that I tried to write the control software for.\nUsing ARM MBed at the time, I realized that there was a lot happening under the hood. Actually, I realized that the control code that I wrote was nothing in comparison to the kernel that was running on the processor, and that was handling 99% of the work.\nHow can one be proud of his code, when it\u0026rsquo;s running in such a helped and catered environment. I was not.\nThat was the moment where I decided to write my own microkernel from scratch using just a C compiler and the datasheet for my microcontroller.\nThis has evolved into a 6 years (so far) project targeting more modern processors than my original 8 bits AVR, in which I implemented every major kernel block, including the memory system, the scheduler, the file system, the driver abstraction layer, the resource tracking system, and many more.\nThis is essentially thanks to this project and to the many hours that I spent refining my understanding of how kernel and processor work that I was able to get hired by Apple to do the kernel bringup for their next generation of devices.\n3D CAD algorithms Before I started programming, I was doing small projects on Solidworks, a CAD tool that I really like.\nWhat I found the most interesting, and that I didn\u0026rsquo;t really understand at that time, was how the core algorithms worked : how can you, from a sequence of abstract instructions, generate an actual 3d file.\nThose kinds of algorithms are called CSG algorithms, for Constructive Solid Geometry, and quickly got my attention after I became fluent with C.\nMy dream objective was to reimplement a program \u0026ldquo;like\u0026rdquo; (i.e. with only the basic part edition features) of solidworks, that would allow me to create my robot parts myself.\nI have been working on this project for around four years, and now have a pure C library that does boolean operations on 3d polyhedrons in a highly performant and robust way, using octrees and BSP trees, so as brep-based remeshing, 2d polygon boolean operations, and many other fun stuff.\nThat makes a lot of projects and code. As stated earlier, I am not authorized to publish any project because of my current job at Apple. But maybe that will change in some years, who knows.\nIn the meantime, the best I can do is to try to make good descriptive articles about those, and hope that you will find them interesting.\n","date":"27 September 2023","externalUrl":null,"permalink":"briztal.github.io/about/","section":"","summary":"I am Raphael Outhier, a french engineer working in Texas and this is my website.","title":"About me","type":"page"},{"content":" Foreword # This series of articles will focus on how to implement a KASAN (or a memory access checker) on a microcontroller kernel.\nThe introduction is a reflection of my personal opinions and of the motivations that lead me to implement my KASAN. The reader may find it cleaving, but they should find the rest of the article more technically oriented.\nThe need for a proper debug infrastructure. # Recently my wonderful wife designed and offered me the greatest present a kernel developer could imagine : a custom development board with a builtin SWD interface.\nDevelopment board. I had been looking for a devboard to get back to microcontroller programming for a while and my only need was a proper SWD interface to attach a debugger.\nSeems like a pretty unrestrictive criterion, right ? False.\nMainstream development boards provide a dedicated bootloader chip connected to the main chip\u0026rsquo;s SWD/JTAG interface to handle the flashing for you. Though it may seem like a good idea, as it avoids the pain of buying your own debug tools, it prevents you from being able to connect your own debugger.\nUnless you are willing to trash those bootloader chips\nA debugger I needed.\nI rely a lot on debug tools to verify my code. My two most used tools are GDB to live debug, and valgrind to check for memory leaks / improper memory accesses. I consider valgrind to be the most essential correctness test, after \u0026ldquo;my code seems to do what it is supposed to do\u0026rdquo;.\nUntil recently, my strategy to debug code that aims to work on a microcontroller has been to code it in pure-C (no assembly) and to make it somehow work in a userspace process. That allowed me to debug it using gdb and valgrind.\nThough this strategy works for abstract pieces of the kernel (not dependent on the actual hardware, ex : file-system, scheduler, resource tracking, etc\u0026hellip;), this is hardly achievable for hardware-dependent ones, like drivers. One could think that we could just write an emulator of some sort and again run the driver in an emulated way. This works but will only give coverage on the behavior that the emulator supports. Failure to emulate an HW feature will lead to lack of actual coverage on that feature when run in real HW.\nThe best coverage we can get is to test the code under real operation conditions.\nTo achieve that, we need our favorite debug tools to work on embedded platforms.\nLet\u0026rsquo;s make enemies # The opinion in the embedded world seems to be\nYour code just has to be correct, and if it's not, you can just debug with printf.\nThis is not a satisfactory answer. This barely works for simple software, at the cost of bisecting the code, but becomes a nightmare when you deploy complex software that does memory allocation.\nMoreover, this relies on the fact that you have a kernel that handles your printfs correctly, to the UART that serves as log output. This implies either a booted kernel with device management initialized, or a very small kernel with embedded UART management. But what if we must debug the kernel itself ? What if the UART isn\u0026rsquo;t even available at that stage ? What if we are doing the bringup of the chip, i.e. testing boot / reset ? You won\u0026rsquo;t get any printf at that stage.\nStating that you can always debug using printf is absurd to the extent that, if you have access to a form of printf, you are potentially running your code in an environment like the Arduino framework. I can guarantee you that the people that developed this framework had some form of probe to debug the chip. When their programming and verification was done, they designed a board without this interface and sold it to you.\nIf the answer to that last statement is :\nCode on a microcontroller should only be running simple software with statically allocated memory\nThis is not a satisfactory answer either.\nModern microcontrollers (whose name should likely be reviewed) operate at the megahertz scale, and can have around one MiB of flash or RAM. This is the very definition of \u0026ldquo;potential to run complex software\u0026rdquo;.\nRather, that answer is merely a consequence of the lack of proper debugging tools widely available in the mainstream embedded world, and of the lack of knowledge around it : developers took the lack of proper debugging infrastructure as an immutable truth and their practices evolved in that direction, validating that assumption.\nBut it is not an immutable truth. To be more precise, it is completely and utterly false.\nThis series of articles aims to prove that running your code in a microcontroller environment gives you a leverage to actually perform memory checking of the same or even better quality than one available in a typical userspace process.\nDisclaimer # The KASAN implementation presented in this series is not open-source nor free, and this series of articles is not a tutorial on how to use such code. This is mostly due to two factors :\na KASAN is tightly coupled to the kernel that it is implemented in. As my kernel is closed source, such is my KASAN. I\u0026rsquo;m not a very nice guy :). Rather, it aims to give a solid base for whoever wants to add a KASAN to their kernel, so that when executing the following piece of code :\nu8 faulty_function( void ) { u32 *mem = kernel_malloc(sizeof(u32)); kernel_free(mem); kernel_free(mem); } the kernel goes from an ugly crash to :\nKr0 started. Welcome. Kr0 initialized. Kr1 started, core1 0x0 online. core1 : 0x30044900. Kr1 initialized. Kr2 started. Kr2 initialized. Initializing pre-smp kernel modules. Double free. Unexpected kasan attrs found during attrs set at address 0x20014038. Expected type : USR_ANY (exact match). NO ACCESS_ALLOCATOR | ACCESS_USER Read type : ALL_RWI (exact match). ACCESS_ALLOCATOR | NO ACCESS_USER | READ_WRITE | INITIALIZED Incompatible attributes are : ACCESS_ALLOCATOR | NO ACCESS_USER Though, to illustrate my points and give implementation details, I\u0026rsquo;ll sometimes provide code samples. For all intents and purposes, I declare this code as part of the public domain.\n","date":"27 September 2023","externalUrl":null,"permalink":"briztal.github.io/projects/kasan/kasan_0_intro/","section":"Personal Projects","summary":"Valgrind on a microcontroller.","title":"KASAN Introduction.","type":"projects"},{"content":"","date":"27 September 2023","externalUrl":null,"permalink":"briztal.github.io/projects/","section":"Personal Projects","summary":"","title":"Personal Projects","type":"projects"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/c/","section":"Tags","summary":"","title":"C","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/emscripten/","section":"Tags","summary":"","title":"Emscripten","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/gnu/","section":"Tags","summary":"","title":"Gnu","type":"tags"},{"content":"In the previous part, I introduced the base components of what I consider to be a minimalistic kernel, and spent some time defining static modules, and why their correct initialization raises some issues.\nIn this chapter, I will be going into more details on WebAssembly and describe the issues I faced when porting my kernel to this environment.\nWebAssembly C Toolchain # Though the work of porting a whole codebase to a browser environment may seem like a hellish amount of work, the Emscripten toolchain greatly eases the workload, by providing a C environment resembling (relatively speaking…) to the posix environment.\nThe C compiler used for emscripten is LLVM (abstracted by the emcc program), which helps keep the C developer in a world he is familiar with.\nBut the pleasure of enjoying compatibility stops here, due to the fact that WebAssembly chose to reimplement its own object format, instead of using a familiar standard for its executable format like ELF (Executable and Linkable Format).\nNow, I am not claiming the authority to judge whether this was a bad idea, nor question their objectives, and I am certain that they had their reasons for making such a choice. I could, for example, imagine them desiring to get rid of the executable side of the ELF format (program headers etc…) to avoid wasteful memory consumption.\nIn the end, the resulting format shares many similar features with the linkable side of the ELF format. But as we will observe in the next sections, they also chose not to support some of the ELF formats useful features that a C developer may be expecting.\nWebAssembly Object format # A quick look at the official documentation shows that the WebAssembly object format (the official documentation uses the term “module” but I will rather use the term “object” to avoid any ambiguity with the concept of module defined in the previous chapter) is similar to what a C developer expects of a linkable format, namely: the definition of sections referring to binary data. Each section having a attributes that define the format and content of this binary data :\n“ The binary encoding of modules is organized into sections. Most sections correspond to one component of a module record, except that function definitions are split into two sections, separating their type declarations in the function section from their bodies in the code section.”\nsource : https://webassembly.github.io/spec/core/binary/modules.html\nThe format defines standard section types, similarly to the ELF format, and assigns a particular meaning to them, such as code, data, import, etc…\nThey even provide support for ,what they referred to as custom sections, which may lead one to think that the feature evocated in the previous chapter (defining a custom section where a variable should be placed) may be supportable.\nBut all our hopes crumble, after observing that the compilation of the following code doesn’t result in the definition of any custom section :\n#include \u0026lt;stdio.h\u0026gt; __attribute__((section(“.xxx”))) const char *str = “MYSTRING”; int main() {printf(str);} Rather, the ‘str’ variable will be placed in the data section, meaning that the __attribute__((section(“xxx”))) is basically ignored :\nwasm-objdump -x main.wasm main.wasm: file format wasm 0x1 Section Details: Type[22]: … Import[3]: … Function[51]: … Table[1]: … Memory[1]: … Global[3]: … Export[13]: … Elem[1]: … Code[51]: … Data[3]: - segment[0] memory=0 size=565 — init i32=1024 - 0000400: 4d59 5354 5249 4e47 0000 0000 3806 0000 MYSTRING….8… - 0000410: 2d2b 2020 2030 5830 7800 286e 756c 6c29 -+ 0X0x.(null) For reference, the same file compiled with clang (C frontend for llvm) produces the expected result of placing the MYSTRING data in the custom xxx section. From this we can deduce that there isn’t any issue with the code itself but rather that the problem lies in the lack of support for this kind of operation in the emscripten toolchain.\nThis lack of support is in itself, not an issue for standard-C-compliant libraries. As the notion of sections resides outside of the C standard’s jurisdiction, and the __attribute__ keyword is an extension that just so happens to be supported by both gcc and clang. But, for any type of software that relies on the use of sections, WebAssembly will be incompatible.\nThough I may seem critical in my conclusion, I perfectly understand the reasons for this lack of support and their validity.\nIndeed the support for programmer-defined custom sections presupposes the existence of a more complex tool for defining the behaviour of the linker when merging multiple files containing custom sections.\nIn order to fill this need, standard toolchains rely on the provision by the programmer of linker scripts (.ld files), that define these behaviours.\nSupporting linker script represents a hellish workload, as such I perfectly understand why WebAssembly designers chose not to invest time on it, after all the usage of programmer-defined custom sections is actually very infrequent.\nMore details on the gnu linker scripts syntax : Command Language\nConclusion # This article served two purposes.\nFirst, introducing the requirements that may lead a developer to implementing or using a kernel, and describing the base software blocks that compose a minimalistic kernel.\nThen, showing that the concept of heterogeneous environments (bare-metal, OS-hosted, Web-browser-hosted) is not simply a concept but a reality, that implies an heterogeneity of toolchains, which force the developer aiming for portability to strictly rely on common toolchain capabilities, and that the lack of support for a toolchain feature may directly prevent particular programs to be ported to the considered environment.\nIn my example, the lack of support for user-defined per-variable custom sections forced me to rewrite parts of my kernel that depended on this functionality.\n","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/projects/kernel_to_wasm_p2/","section":"Personal Projects","summary":"WebAssembly, or why the toolchain matters.","title":"Joys of porting a kernel to WebAssembly, Part 2","type":"projects"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/kernel/","section":"Tags","summary":"","title":"Kernel","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/categories/kernel/","section":"Categories","summary":"","title":"Kernel","type":"categories"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/wasm/","section":"Tags","summary":"","title":"Wasm","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/categories/wasm/","section":"Categories","summary":"","title":"Wasm","type":"categories"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/web-development/","section":"Tags","summary":"","title":"Web Development","type":"tags"},{"content":"","date":"8 October 2021","externalUrl":null,"permalink":"briztal.github.io/tags/webassembly/","section":"Tags","summary":"","title":"Webassembly","type":"tags"},{"content":" The back story # For as long as I can remember, understanding how kernels were designed and written was something that has interested me.\nIn the beginning, a newcomer to the world of programming, I saw them as mysterious, incredibly complex and barely comprehensible software blocks, handling equally mysterious tasks in the execution of a program.\nA few years later, after having perfected my skills as a programmer, I dived into the concept with a new perspective, and discovered the real issues that required the use of kernels and the possible implementations choices that solved these problems.\nAfterwards, I did what every normal programmer would do in my situation and spent more than the next half decade implementing my own one.\nLast year, I discovered WebAssembly, an Intermediate Representation allowing the deployment of C software in a web-browser. In order to make this port possible I had to update a part of my code base to support WebAssembly, and it was within that context that I set upon myself the challenge of :\nporting the kernel to the wasm environment. WebAssembly and its related toolchain, Emscripten, have done a very respectable work in providing support for C99 with gnu extensions (ex : constructs like ({statements})), allowing regular C libraries to be ported without any issue.\nYet, I encountered my fair share of interesting issues when porting my kernel to this environment. After all, kernel’s are quite far from the simple standard-compliant C code, and as such forced me to push to the boundaries of the emscripten wasm toolchain and get familiar with it’s nooks and crannies.\nThis article’s aim is to describe these difficulties in more detail and hopes to, in the process, give you glimpses of how kernel’s on one hand and the emscripten toolchain, on the other, work under the hood.\nI will start this article by briefly introducing Webassembly, and I will then tell you more about the main needs that pushed me to implement my own kernel, and in doing so, I will give a short description of its main blocks. Following this, I will provide a more detailed look at the Emscripten toolchain, to expose a corner case that forced me to reimplement a part of the kernel’s init procedure to support WebAssembly.\nWeb-browsers and Javascript # Historically designed to render statically defined text-based pages, web-browsers have drastically evolved to the point we know, being nowadays having more in common with actual Operating Systems than simple page renderers.\nWeb-browsers getting more and more complex and varying in their implementations, and web-designers desiring to avoid as much as possible taking the execution browser brand (firefox, chrome, …) into account during development, it was central to provide a stable common API. Javascript rapidly took this place, the Javascript VM now being one of the central components of a web-browser.\nJavascript being an interpreted complex language, its execution performances rapidly showed their limits, even with JIT features enabled. In a world where Moore’s law applies, this may not seem like a real issue, as the assumption that the next generation of computer will provide enough performance increase to compensate for this slowness. Though, in our years where this law clearly started to present its validity limits, this issue had to be addressed.\nWebAssembly # Webassembly is a relatively new (2017) trending IR (Intermediate Representation) code format designed to be loaded and interpreted, translated or JITed in, among others, web-browsers.\nWasm logo An intermediate representation is, as the name states, a manner to represent a program that doesn’t aim to be directly executed, but rather :\nto serve as storage or exchange format.\nto be interpreted efficiently by a wide range of program processors (compilers, VMs etc…), to be either translated into other formats (ex : actual assembly) or directly executed.\nWebAssembly is, as a consequence, not a programming language (though the textual version may be used this way), but rather an object format that compilers may use to store the result of their compilation, and that a virtual machine may use as executable source.\nThis allows a possible wide range of programming languages to be compiled into it, for example C or Rust for the most widely known.\nThough its reputation may be tainted due to its strong use by web hackers to inject (among others) crypto-mining payloads in client side, webassembly remains a good option for porting C libraries in the Web world.\nKernels, or the need for environment abstraction layers # As any software engineer I aim to write “good” code. Naturally, pretty fast, the problem of defining what “good code” is arises. After all, given a choice between multiple possible implementations, it would be best to have some criterion.\nThough I recognize that these criterion are not universal, in my implementations at least I now consider 4 performance indicators that I have listed them below in decreasing importance :\nportability : the code must be compatible with the widest scope of environments.\nfunctionalities : the code must offer an answer to the most general problem.\nmemory consumption : the code must use the smallest amount of memory, considering the previous rules.\nexecution time : the code must run as fast as possible, considering the previous rules.\nI wish to once more remind the reader that these criterion are completely subjective, and are only the reflection of my experience and my objectives.\nIt is certain that real-time software engineers would have radically different guidelines, and would, for example, place execution time invariability, or avoidance of linked lists traversals inside critical sections, on top of their list.\nBut let us go back to my criterion.\nGenerally, I tend to place code reusability across use-cases and environments among every other requirement.\nWhen I say environment, I mean the platforms (x8664, x8632, arm32, aarch64), host OS (freestanding, linux, windows, mbedOS) and toolchain (gcc, llvm).\nThis means that the code I write must be as independent as possible of the host API its executable version will access.\nHaving this constraint implies defining standard custom APIs for the most basic host requirements of a program :\nhost memory access : ex for stdlib : malloc, free.\nhost memory mapping : ex for linux : mmap, mprotect, munmap.\nhost file access : ex : printf, for linux : open, close, read, write, seek.\nhost directory access : ex for linux : readdir, closedir, seekdir.\nthreading : ex for linux : pthread environment.\nThe need for freestanding implementation # To achieve the aforementioned code reusability objective, in a classic hosted environment defining a simple function indirections (static inline functions or macro aliases in a shared header) will do the job.\nThat being said, as an embedded software engineer, my primary environment targets are freestanding (without OS, bare-metal) systems, not hosted ones, which complicates the problem.\nThough, it is still possible to use frameworks for those freestanding systems, that would provide some semblance of a standard environment. The Arduino framework, being one example, offers a default standard for low-performance embedded systems. In this case the previous simple solution would still be viable.\nBut, after a more thorough examination and more experience with these frameworks I have started to have several objections regarding their use :\ncode quality : a simple look in an Arduino framework implementation is enough to leave one, at best doubtful, at worst frightened by the code quality and the apparent absence of debugging efforts.\ncode performance : for example, the quality of the memory management system may be weak, which could lead to memory allocation poor performances, which could impact the whole program.\nmissing features : platforms like the teensy35 (a platform nevertheless close to my heart as it was my entry point into embedded systems and my hardware of choice for a few years) is based on a cortex-m4f based on the ARMV7M architecture. As such, it features all of the architectural components required to implement a context-switch based mono-core threading API. Yet, the Arduino framework is primarily made to provide an uniform API for extremely low-power cores like AVR8 chips. As such it doesn’t provide a satisfying threading API, and so, if an implementation is required, it must be implemented manually.\nlanguage constraints : the Arduino framework is written in C++, which doesn’t ease the development of pure-C applications.\nAs a consequence, for freestanding systems, it becomes a necessity to have an available working implementation of the basic code blocks to support at least :\nmemory allocation.\nmemory mapping (if any).\nfile access.\nthreading.\nAnd those blocks happen to be the minimal blocks needed for a kernel.\n#The need for code compatibility between freestanding and hosted environments\nHosted and freestanding environments are different in many ways, but, most probably the most painful difference between the two for any programmer is the difference in debugging capabilities.\nOn the one hand, debugging on a hosted environment (ex : linux process) is very easy and fast. Tools like valgrind are extremely versatile, easy to use and can allow the developer to have an exhaustive list of all invalid accesses, memory leaks, uninitialized accesses, without requiring any additional instrumentation. Program faults are handled by the host and faults’ side-effects are reduced, as the host will close any used file, resource, etc…\nOn the other hand debugging on a freestanding environment is much more of a headache and may require a particularly expensive instrumentation (the most well-known toolkit being the Segger’s J-Link) to set up a gdb server, in the best case.\nAn invaluable tool like valgrind being a simulation-based debug tool, it only supports its host’s instruction set, and as such can’t be adapted to this environment. As such our debugging capabilities are greatly reduced.\nMemory leaks will be harder to detect, uninitialized accessed will be untraceable.\nProgram faults may be fatal, if not handled properly, as they may put the program in an unsafe state. Worst, if the related program is a controller for physical objects, may lead to erratic behaviours with repercussions in the physical world, which may be inconvenient if not harmful.\nAs a consequence, we have two complementary environments, one that depends on a safe and verified code but can’t provide ways to verify it, and another that provides such ways, but for which the consequences of not having a dependable and verified code are much lower.\nAs such we can make up for the shortcoming of the freestanding environment by making the code as much platform-independent as possible. This allows for a rigorous stress-testing and debugging of shared code via standardised, powerful and robust tools in the hosted environment, and safe deployment of this verified code on freestanding platforms.\nA simple user space kernel # Though the definition of what is and what is not a kernel may be a subject of debate, I believe the best definition to start with is the following :\na kernel manages shared resources in a system.\nThose resources may be :\ncores.\nTasks (and threads and processes).\nExecutables, the kernel executable at first, then possible dynamic libraries and modules.\nMemory.\nPeripherals.\nFiles.\nMore generally, a kernel aims to manage any shared resources accessible by multiple programs in a system. The role of the kernel is to supervise the access to those resources, by providing a consistent API to use them.\nContrary to what is most common for a kernel, my kernel will run in user-space.\nNormally kernels do not do this and isolate its threads from itself via the virtualisation / protection mechanisms offered by MMUs or MPUs. My kernel doesn’t include this feature as protection mechanisms from the MMUs and MPUs may be absent on the targeted systems. This can occur when the kernel is run in the process space on a hosted system (no MMU, may be emulated via linux-KVM if relevant but that’s another story), or on a cortex-m4f (implementation-defined MPU capabilities but no MMU).\nThat aside, the kernel implements most of the features that you would expect to encounter :\nBSP : environment dependent code, provides a standardised API to interact with cores, host memory managers, host file manager, etc… Memory manager : responsible for managing primary memory and to provide secondary memory APIs to cores. See my detailed article on this topic. Scheduler : decentralised system responsible for scheduling tasks on each core, and to coordinate the transfer of tasks between cores depending on each core’s load. Introduces the concept of process, i.e. threads accessing the same set of resources. Loader : provides static module initialization capabilities and dynamic modules / libraries loading / unloading capabilities. File system : tracks resources processes can use, manages their access. Resource tracker : tracks resources used by each process, and frees them automatically when the process terminates. Modules # The previous section introduced a particular block of the kernel, the loader, that was in the centre of one of the issues I encountered, the Loader.\nIt is not my intention to go into details regarding the internals of the loader, as this would be long enough for an entire article. Yet, as having a clear comprehension of the concept of modules is essential for understanding the following sections, we will introduce it.\nWe can divide the software that will be executed at runtime in two categories :\nkernel : contains all blocks previously mentioned, i.e. blocks required to manage resources, and to run user code. Kernel code must be linked statically to form the kernel executable. modules : code that is not strictly required in the kernel executable, and so, that can be dynamically loaded. For example, the file system is a component of the kernel, but the code to support a particular type of file manager (ext2, ext4, ntfs) can be provided in a module.\nNow that this nuance is introduced, we next need to introduce the concept of static modules and introduce their use case.\nOn certain platforms, the RAM memory sections are not executable, this implies that Writable memory can’t be executed, and so, such dynamic loading is impossible, as dynamically loading a module means writing it to memory from an external source, and modifying it (ex : applying relocations) such that it can access kernel symbols (functions and variables).\nIn these platforms, dynamic loading will be impossible, and all required modules should be linked statically to the kernel executable.\nOn other environments, like WebAssembly, the executable format is atypical, and dynamic loading may be difficult. Even if a loader implementation for such a format may be possible, it will require a lot more development effort than what I was willing to invest. In such a situation statically linked modules there again seem like a valid option, even if only as a temporary solution.\nAnother benefit of linking modules statically to the kernel executable is time, dynamic loading being a complex problem with complex and time-consuming solutions. Doing the linking work at compile time avoids the need to do it at each execution.\nA module being an implementation-defined executable aiming to have a user-defined impact on the kernel (for example, to start processes or to provide support for file system types), it must comprise a function accomplishing the aforementioned tasks, the module initialization function, that the loader can find and execute .\nThis execution will be done at different times, depending on the module type :\nFor static modules, all init functions of static modules must be retrieved at once and executed sequentially, possibly in a particular order. For dynamic modules, the init function must be executed as soon as the module is loaded in the kernel. Again, what will be of interest here will be the case of static modules.\nAt a high level the build procedure of static modules in our kernel works as the following :\nSource files of the same module are compiled independently. Source files of the same module are merged to form a single relocatable executable, the module executable. Module executables are modified, by possibly forcing symbols locality to prevent multiple symbols definitions. Module executables are merged to form a single relocatable executable, the static modules executable. During this process multiple static modules executables will be generated, each one having its own initialization function. Later, they will be merged into a single relocatable executable, and the position of these functions in the resulting executable may not be known.\nNot knowing the position in the executable of each module’s initialization function will be an issue as, at runtime, the kernel will have to execute each one of these functions, but the kernel being independent of modules, it has no knowledge a priori of the location of these functions.\nThere are two strategies one can use to help overcome this hurdle.\nThe first, and most common solution involved using the concept of sections, supported by the elf format. Practically this involves defining for each module a static function pointer that contains the location of the module’s initialization function, and placing this variable in a custom section of the efl. We shall be calling this custom elf section the .modinit.\nLater in the build process, during the merging of multiple static modules executables, the linker (ld) will merge the content of sections with the same name (default behaviour, but can be configured with the use of a linker script), in our case this means merging into one section the multiple static modules .modinit content. This will cause all our pointers to be located in the same output section, which in the resulting executable, will cause them to be located contiguously in memory.\nA wisely crafted linker script provided to the linker will define two global variables, .modinit_start and .modinit_end, used to help the kernel locate the start and end of this section, which will allow the kernel to retrieve addresses of all modules initialization functions needed to execute them.\nThis aforementioned process is the one that regular executable processes use to trigger the\ninitialization and de-initialization of static variables whose expressions need to be computed at run time.\nFor more details : ELF: From The Programmer\u0026rsquo;s Perspective\nThe second solution, which is in my humble opinion much less elegant and relies on knowing at compile-time the full list of static modules that need to be compiled and linked, consists in providing a unique name to each initialization function (derived from the module name, modules names must be unique), and to assign a global visibility to this function, then to define a single function calling each initialization function by name. The kernel then only needs to execute this function to initialize all modules.\nThough being un-clever, this procedure can be done automatically using a bash script.\nIn my pre-WebAssembly implementation, I used the first solution. This was only possible because I had easy access to the standard elf format as I was developing for the linux environment. But, as we will now observe, WebAssembly’s constraints on the elf format forced me to review my choice and opt for the second solution.\nIn-process kernels, WebAssembly and their difficulties to cohabitate.\nThe second solution, which is in my humble opinion much less elegant and relies on knowing at compile-time the full list of static modules that need to be compiled and linked, consists in providing a unique name to each initialization function (derived from the module name, modules names must be unique), and to assign a global visibility to this function, then to define a single function calling each initialization function by name. The kernel then only needs to execute this function to initialize all modules.\nThough being un-clever, this procedure can be done automatically using a bash script.\nIn my pre-WebAssembly implementation, I used the first solution. This was only possible because I had easy access to the standard elf format as I was developing for the linux environment. But, as we will now observe, WebAssembly’s constraints on the elf format forced me to review my choice and opt for the second solution.\nIn-process kernels, WebAssembly and their difficulties to cohabitate.\n","date":"7 October 2021","externalUrl":null,"permalink":"briztal.github.io/projects/kernel_to_wasm_p1/","section":"Personal Projects","summary":"In-process kernels, WebAssembly and their difficulties to cohabitate.","title":"Joys of porting a kernel to WebAssembly, Part 1","type":"projects"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"briztal.github.io/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"}]